output,source
"As a company that develops AI software, if your clients use your software in high-risk applications, your company would be considered a ""Provider"" under the AI Act. As a Provider, you have several obligations to ensure compliance with the regulation:

1. Implement risk management processes as per Article 9.
2. Use high-quality training, validation, and testing data as outlined in Article 10.
3. Establish and maintain the necessary documentation as defined by Annex IV and Article 11.
4. Implement automatic logging to record the operations of the AI system, in accordance with Article 12.
5. Ensure an appropriate level of transparency with Deployers, as required by Article 13.
6. Ensure human oversight measures are in place, as specified in Article 14.
7. Ensure the robustness, accuracy, and cybersecurity of the AI system, as per Article 15.
8. Set up a Quality Management System, which could be based on ISO 42001, as mentioned in Article 17.
9. Keep documentation for 10 years at the disposal of the national competent authorities, as required by Article 18.
10. Store the automatically generated logs for at least 6 months, as stated in Article 20.
11. Take corrective actions and fulfill the Duty of information when the system is not in conformity, as outlined in Article 21.
12. Provide the necessary information to competent authorities, as per Article 23.
13. Register the high-risk AI system in the EU Database, according to Article 51.

Additionally, since your AI software is used in high-risk applications, it must go through a third-party conformity assessment to obtain a CE mark before being placed on the EU market. This is a crucial step to ensure that your AI system complies with the necessary requirements and standards set forth by the AI Act for high-risk AI systems.","The AI Act takes a “risk-based” approach, scaling the stringency of
requirements based on the inherent risk of an AI system. You can learn more
about how this approach differs from other frameworks and standards in our
[previous blog post on the state of AI
regulation.](https://www.citadel.co.jp/en/blog/2023/03/20/an-overview-of-ai-
standardization-and-regulation-in-2023/)

As explained below, AI systems categorized as __ high-risk will feel the most
impact from this regulation. Developers of such high-risk AI systems must go
through a **third-party conformity assessment** in order to **obtain a CE mark
and access the EU market**.

## Do the requirements differ for developers and users of AI systems?

The most recent version of the AI Act [from Feb
2](https://artificialintelligenceact.eu/wp-content/uploads/2024/01/AI-Act-
FullText.pdf), approved by the Committee of Permanent Representatives,
provides additional information on the
[roles](https://artificialintelligenceact.eu/article/3) and [their
responsibilities](https://artificialintelligenceact.eu/article/28/) across the
AI Value Chain. The law defines six key roles:

  1.  **Provider**. An entity (e.g. company) that develops an AI system and “places them on the market or puts the system into service under its own name or trademark, whether for payment or free of charge”.
  2.  **Deployer**. An entity that uses an AI system under its authority, except in personal non-professional activities. Note that this does not refer to the affected end users.
  3.  **Importer**. An EU entity that places an non-EU AI system on the EU market.
  4.  **Distributor**. An entity in the supply chain, other than the provider or the importer, that makes an AI system available in the EU market.
  5.  **Authorized Representatives**. An EU entity that carries out obligations on behalf of a Provider.
  6.  **Operator.** An entity that falls into any of the five categories above.

> Operators could act in more than one role at the same time and should
> therefore fulfill cumulatively all relevant obligations associated with
> those roles. For example, an operator could act as a distributor and an
> importer at the same time.
>
> [Recital 56a](https://artificialintelligenceact.eu/recital/56a)

 **Providers** bear the majority of responsibilities under the AI Act.
Providers of high-risk AI systems will need to comply with [Article
16](https://artificialintelligenceact.eu/article/16/), and by extension:

  * [Article 9](https://artificialintelligenceact.eu/article/9). Implementing risk management processes.
  * [Article 10](https://artificialintelligenceact.eu/article/10). Data and Data Governance. Using high-quality training, validation and testing data.
  * [Article 11](https://artificialintelligenceact.eu/article/11). Establishing documentation as defined by [Annex IV](https://artificialintelligenceact.eu/annex/4).
  * [Article 12](https://artificialintelligenceact.eu/article/12). Implementing automatic logging.
  * [Article 13](https://artificialintelligenceact.eu/article/13). Ensuring an appropriate level of transparency with Deployers.
  * [Article 14](https://artificialintelligenceact.eu/article/14). Ensuring human oversight measures.
  * [Article 15](https://artificialintelligenceact.eu/article/15). Ensuring robustness, accuracy and cybersecurity.
  * [Article 17](https://artificialintelligenceact.eu/article/17). Setting up a Quality Management System (e.g. [based on ISO 42001](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-you/))
  * [Article 18](https://artificialintelligenceact.eu/article/18). Keeping documentation for 10 years at the disposal of the national competent authorities.
  * [Article 20](https://artificialintelligenceact.eu/article/20/). Storing the automatically generated logs for at least 6 months.
  * [Article 21](https://artificialintelligenceact.eu/article/21/). Corrective actions and Duty of information when the system is not in conformity.
  * [Article 23](https://artificialintelligenceact.eu/article/23). Providing the necessary information to competent authorities.
  * [Article 51](https://artificialintelligenceact.eu/article/51).
Setting up a Quality Management System (e.g. [based on ISO 42001](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-you/))
  * [Article 18](https://artificialintelligenceact.eu/article/18). Keeping documentation for 10 years at the disposal of the national competent authorities.
  * [Article 20](https://artificialintelligenceact.eu/article/20/). Storing the automatically generated logs for at least 6 months.
  * [Article 21](https://artificialintelligenceact.eu/article/21/). Corrective actions and Duty of information when the system is not in conformity.
  * [Article 23](https://artificialintelligenceact.eu/article/23). Providing the necessary information to competent authorities.
  * [Article 51](https://artificialintelligenceact.eu/article/51). Registering the high-risk model in the EU Database.

 **Deployers** of high-risk AI systems have lighter obligations, and will need
to comply with [Article 29](https://artificialintelligenceact.eu/article/29).
This includes exercising due diligence in using the system; monitoring and
logging the system in use; and, in specific conditions, carrying out [a
fundamental rights impact
assessment](https://artificialintelligenceact.eu/article/29a).

However, a Deployer, Importer, or Distributor is considered a Provider of a
high-risk AI system if they:

  * Change the name or a trademark of the AI system
  * Make a substantial modification to the AI system
  * Modify the intended purpose of a non-high-risk AI system already on the market, in such a manner that makes the AI system high-risk

If the initial Provider was overridden in the cases above, the initial
Provider is no longer subject to the usual obligations, but must provide “the
reasonably expected technical access” to the new Provider.

 **Importers** and **Distributors** are subject to the obligations in [Article
26](https://artificialintelligenceact.eu/article/26) and [Article
27](https://artificialintelligenceact.eu/article/27), respectively. They
include verifying that the provider has complied with their obligations, and
that the system has gone through the relevant conformity assessment and bears
the CE mark.

 **Authorized Representatives** are subject to the obligations in [Article
25](https://artificialintelligenceact.eu/article/25), which outline the
cooperation with relevant authorities and submission of relevant documents.

The following diagram can be useful to understand your entity’s (“operator’s”)
position in the AI system supply chain as defined by the AI Act. Note that
this is a simplified version that makes some assumptions; for example, that
the AI system’s output is located or used within the EU. We also recommend
using the interactive [AI Act Compliance
Checker](https://artificialintelligenceact.eu/assessment/eu-ai-act-compliance-
checker/) tool developed by the Future of Life Institute.

![](https://sp-
ao.shortpixel.ai/client/to_auto,q_glossy,ret_img,w_2090,h_3309/https://www.citadel.co.jp/en/wp-
content/uploads/sites/6/2024/02/AIA-flowchart-2.png)

## What are the requirements for different types of AI systems?

The Act defines several categories of AI systems with different obligations.
In principle, these categories are not mutually exclusive, and the same AI
system may fall under a mix of obligations. For example, a foundation model
that’s used in a medical device and directly interacts with end users might
need to fulfill High-Risk, GPAI, and Transparency obligations all at once.

### Prohibited AI

According to [Article 5](https://artificialintelligenceact.eu/article/5),
these use cases are considered unacceptable and will be prohibited, with some
exceptions for law enforcement activities. Examples: real-time biometric
identification, social scoring, manipulative techniques.

### High-Risk AI

These use cases are considered highly harmful if misused, and will need to go
through a third-party conformity assessment before being placed on the market.
According to [Article 6](https://artificialintelligenceact.eu/article/6), an
AI system is considered high-risk under either of the following conditions:

  1. It’s used as a safety component of a product, or is itself a product that’s already regulated by existing EU laws listed in Annex II, and is required to undergo a third-party conformity assessment under those Annex II laws; or,
  2. It falls under Annex III, unless it’s proved to not possess significant risk of harm to health, safety, or fundamental rights

We will now cover the two points above in more detail."
"The provided context information does not include specific details about provisions in the AI Act for AI systems that could be used for social scoring by governments. To obtain information on this topic, one would need to refer to the full text of the AI Act or other relevant documents that address this specific use case.","The AI Act takes a “risk-based” approach, scaling the stringency of
requirements based on the inherent risk of an AI system. You can learn more
about how this approach differs from other frameworks and standards in our
[previous blog post on the state of AI
regulation.](https://www.citadel.co.jp/en/blog/2023/03/20/an-overview-of-ai-
standardization-and-regulation-in-2023/)

As explained below, AI systems categorized as __ high-risk will feel the most
impact from this regulation. Developers of such high-risk AI systems must go
through a **third-party conformity assessment** in order to **obtain a CE mark
and access the EU market**.

## Do the requirements differ for developers and users of AI systems?

The most recent version of the AI Act [from Feb
2](https://artificialintelligenceact.eu/wp-content/uploads/2024/01/AI-Act-
FullText.pdf), approved by the Committee of Permanent Representatives,
provides additional information on the
[roles](https://artificialintelligenceact.eu/article/3) and [their
responsibilities](https://artificialintelligenceact.eu/article/28/) across the
AI Value Chain. The law defines six key roles:

  1.  **Provider**. An entity (e.g. company) that develops an AI system and “places them on the market or puts the system into service under its own name or trademark, whether for payment or free of charge”.
  2.  **Deployer**. An entity that uses an AI system under its authority, except in personal non-professional activities. Note that this does not refer to the affected end users.
  3.  **Importer**. An EU entity that places an non-EU AI system on the EU market.
  4.  **Distributor**. An entity in the supply chain, other than the provider or the importer, that makes an AI system available in the EU market.
  5.  **Authorized Representatives**. An EU entity that carries out obligations on behalf of a Provider.
  6.  **Operator.** An entity that falls into any of the five categories above.

> Operators could act in more than one role at the same time and should
> therefore fulfill cumulatively all relevant obligations associated with
> those roles. For example, an operator could act as a distributor and an
> importer at the same time.
>
> [Recital 56a](https://artificialintelligenceact.eu/recital/56a)

 **Providers** bear the majority of responsibilities under the AI Act.
Providers of high-risk AI systems will need to comply with [Article
16](https://artificialintelligenceact.eu/article/16/), and by extension:

  * [Article 9](https://artificialintelligenceact.eu/article/9). Implementing risk management processes.
  * [Article 10](https://artificialintelligenceact.eu/article/10). Data and Data Governance. Using high-quality training, validation and testing data.
  * [Article 11](https://artificialintelligenceact.eu/article/11). Establishing documentation as defined by [Annex IV](https://artificialintelligenceact.eu/annex/4).
  * [Article 12](https://artificialintelligenceact.eu/article/12). Implementing automatic logging.
  * [Article 13](https://artificialintelligenceact.eu/article/13). Ensuring an appropriate level of transparency with Deployers.
  * [Article 14](https://artificialintelligenceact.eu/article/14). Ensuring human oversight measures.
  * [Article 15](https://artificialintelligenceact.eu/article/15). Ensuring robustness, accuracy and cybersecurity.
  * [Article 17](https://artificialintelligenceact.eu/article/17). Setting up a Quality Management System (e.g. [based on ISO 42001](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-you/))
  * [Article 18](https://artificialintelligenceact.eu/article/18). Keeping documentation for 10 years at the disposal of the national competent authorities.
  * [Article 20](https://artificialintelligenceact.eu/article/20/). Storing the automatically generated logs for at least 6 months.
  * [Article 21](https://artificialintelligenceact.eu/article/21/). Corrective actions and Duty of information when the system is not in conformity.
  * [Article 23](https://artificialintelligenceact.eu/article/23). Providing the necessary information to competent authorities.
  * [Article 51](https://artificialintelligenceact.eu/article/51).
According to [Article 52c](https://artificialintelligenceact.eu/article/52c),
Providers of GPAI models must:

  * Draw up technical documentation, including training and testing process and evaluation results.
  * Enable other Providers that intend to integrate the GPAI model into their own AI system with sufficient documentation and information.
  * Put in place a policy to respect the EU copyright law.
  * Based on a template provided by the AI Office, publish a sufficiently detailed summary about the content used for training.
  * Develop and adhere to a code of practice

One of the reasons for the recent delays in the AI Act’s development was a
debate on the two-tiered categorization for GPAI models. Currently, models
that were trained using a total computing power of more than [10^25
FLOPs](https://artificialintelligenceact.eu/article/52a) are considered to
carry “ **systemic risk** ”. This places most models on the market, excluding
**GPT-4 and potentially Gemini Ultra** , in the non-systemic risk subcategory.
However, the threshold may be updated in future by the European Commission and
the AI Office as the technology progresses.

Providers of GPAI models with systemic risk are subject [to additional
obligations](https://artificialintelligenceact.eu/article/52d) to:

  * Perform model evaluations, including adversarial testing
  * Assess and mitigate possible systemic risks
  * Document and report issues to the AI office and relevant authorities

* * *

However, there are also major **exemptions** for Providers of AI systems for
research and academic use, as well as for AI models (including GPAI) that are
free and open-source. These models are not subject to any obligations until
they are placed on the market by a Provider as part of an AI system that has
obligations under the AI Act. Military and law enforcement uses are excluded
from the majority of obligations, but have some special considerations that we
will not cover.

You can learn more details about each risk category in the [official
FAQ](https://ec.europa.eu/commission/presscorner/detail/en/QANDA_21_1683)
published by the European Commission.

## What are the consequences of not complying with the EU AI Act?

As defined in [Article 71](https://artificialintelligenceact.eu/article/71/),
administrative fines act as the main penalty. The actual fine amount will be
decided on a case-by-case basis depending on the severity of the infringement
(based on rules to be further developed by individual EU Member states), but
the AI Act does outline maximum penalties for different types of non-
compliance:

  *  _Prohibited practices or non-compliance related to requirements on data:_ up to €35m or 7% of the total worldwide annual turnover
  *  _Non-compliance with any of the other requirements:_ up to €15m or 3% of the total worldwide annual turnover
  *  _Incorrect, incomplete or misleading information to notified bodies and national competent authorities:_ up to €7.5m or 1.5% of the total worldwide annual turnover

For each category of infringement, the threshold would be the lower of the two
amounts for small and medium-sized enterprises (including startups) and the
higher for other companies.

## What is the current legislative status and timeline?

The EU AI Act is not yet law, but it’s on a fast track to be published in the
Official Journal in early-mid 2024, and most of its requirements will
gradually be enforced in the following 24 months.

The[ timeline for
enforcement](https://artificialintelligenceact.eu/article/85) is as follows:

  1. After 6 months: Prohibition of AI applications of unacceptable risk becomes applicable.
  2. After 9 months: [Codes of practice](https://artificialintelligenceact.eu/recital/60t/) for GPAI providers must be prepared.
  3. After 12 months (approximately 2025 Q2~): Provisions on GPAI become applicable.
  4. After 24 months (approximately 2026 Q2~): Obligations for high-risk AI systems under Annex III become applicable. Each EU Member State is required to establish or participate in at least one regulatory sandbox.
  5. After 36 months: Obligations for high-risk AI systems under Annex II become applicable.

In practical terms, this means that the next two years will see a flurry of
activity in the field of AI auditing, both in government and in the private
sector. Relevant offices and systems will need to be set up in EU member
states; standards-making and certification bodies will be under extra pressure
to fill the gap and provide clear assessment frameworks."
"AI systems that can generate synthetic content, such as deepfakes or AI-generated content, are subject to regulations under the EU AI Act that require providers of these systems to embed technical solutions to mark the content in a machine-readable format. This is to ensure that it can be detected when the output has been generated or manipulated by an AI system rather than a human. The technical solutions should be reliable, interoperable, effective, and robust, and may include techniques such as watermarks, metadata identifications, cryptographic methods for proving provenance and authenticity, logging methods, fingerprints, or other appropriate techniques. Providers must consider the specificities and limitations of different types of content and the state-of-the-art in technology and market developments when implementing these obligations. The marking obligation is designed to be proportionate and does not cover AI systems that primarily assist with standard editing or do not substantially alter the input data or its semantics.","Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

This Regulation regulates AI systems and models by imposing certain
requirements and obligations for relevant market actors that are placing them
on the market, putting into service or use in the Union, thereby complementing
obligations for providers of intermediary services that embed such systems or
models into their services regulated by Regulation (EU) 2022/2065. To the
extent that such systems or models are embedded into designated very large
online platforms or very large online search engines, they are subject to the
risk management framework provided for in Regulation (EU) 2022/2065.
Consequently, the corresponding obligations of the AI Act should be presumed
to be fulfilled, unless significant systemic risks not covered by Regulation
(EU) 2022/2065 emerge and are identified in such models. Within this
framework, providers of very large online platforms and very large search
engines are obliged to assess potential systemic risks stemming from the
design, functioning and use of their services, including how the design of
algorithmic systems used in the service may contribute to such risks, as well
as systemic risks stemming from potential misuses. Those providers are also
obliged to take appropriate mitigating measures in observance of fundamental
rights.

[ <- Previous ](https://artificialintelligenceact.eu/recital/117/) [ Next ->
](https://artificialintelligenceact.eu/recital/119/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

A variety of AI systems can generate large quantities of synthetic content
that becomes increasingly hard for humans to distinguish from human-generated
and authentic content. The wide availability and increasing capabilities of
those systems have a significant impact on the integrity and trust in the
information ecosystem, raising new risks of misinformation and manipulation at
scale, fraud, impersonation and consumer deception. In the light of those
impacts, the fast technological pace and the need for new methods and
techniques to trace origin of information, it is appropriate to require
providers of those systems to embed technical solutions that enable marking in
a machine readable format and detection that the output has been generated or
manipulated by an AI system and not a human. Such techniques and methods
should be sufficiently reliable, interoperable, effective and robust as far as
this is technically feasible, taking into account available techniques or a
combination of such techniques, such as watermarks, metadata identifications,
cryptographic methods for proving provenance and authenticity of content,
logging methods, fingerprints or other techniques, as may be appropriate. When
implementing this obligation, providers should also take into account the
specificities and the limitations of the different types of content and the
relevant technological and market developments in the field, as reflected in
the generally acknowledged state-of-the-art. Such techniques and methods can
be implemented at the level of the system or at the level of the model,
including general-purpose AI models generating content, thereby facilitating
fulfilment of this obligation by the downstream provider of the AI system. To
remain proportionate, it is appropriate to envisage that this marking
obligation should not cover AI systems performing primarily an assistive
function for standard editing or AI systems not substantially altering the
input data provided by the deployer or the semantics thereof.

[ <- Previous ](https://artificialintelligenceact.eu/recital/132/) [ Next ->
](https://artificialintelligenceact.eu/recital/134/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"AI applications are considered high-risk under the EU AI Act if they meet certain criteria. These include AI systems used as safety components of products, or as products themselves, that are regulated by existing EU laws listed in Annex II and require a third-party conformity assessment under those laws. Additionally, AI systems that fall under the categories listed in Annex III are also considered high-risk unless proven otherwise. Examples of high-risk AI applications include:

- AI systems integrated into machinery, toys, recreational craft, lifts, equipment for explosive atmospheres, radio equipment, pressure equipment, cableway installations, personal protective equipment, appliances burning gaseous fuels, medical devices, and in vitro diagnostic medical devices.
- AI systems related to civil aviation security, two- or three-wheel vehicles, agricultural and forestry vehicles, marine equipment, rail system interoperability, motor vehicles, and civil aviation.
- AI applications used for biometric identification, critical infrastructure, education and vocational training, employment and workers management, and access to essential public and private services.

These high-risk AI systems are subject to stringent regulatory requirements and must undergo a conformity assessment before being placed on the market.","Examples: real-time biometric
identification, social scoring, manipulative techniques.

### High-Risk AI

These use cases are considered highly harmful if misused, and will need to go
through a third-party conformity assessment before being placed on the market.
According to [Article 6](https://artificialintelligenceact.eu/article/6), an
AI system is considered high-risk under either of the following conditions:

  1. It’s used as a safety component of a product, or is itself a product that’s already regulated by existing EU laws listed in Annex II, and is required to undergo a third-party conformity assessment under those Annex II laws; or,
  2. It falls under Annex III, unless it’s proved to not possess significant risk of harm to health, safety, or fundamental rights

We will now cover the two points above in more detail.

* * *

One of the major points of deliberation during the AI Act’s development was
alignment with existing EU harmonisation legislations that regulate existing
product categories. This is covered by Annex II.

 **[Annex II](https://artificialintelligenceact.eu/annex/2), Section A: List
of Union Harmonisation Legislation Based on the New Legislative Framework**

Includes AI systems, or the products for which the AI system is a safety
component, that fall under the following categories. As per [Article 28
2(a)](https://artificialintelligenceact.eu/article/28/), the manufacturer of
such products is considered the Provider of the included high-risk AI system
if it’s placed on the market under the manufacturer’s name or trademark:

  * Machinery
  * Toys
  * Recreational craft and personal watercraft
  * Lifts
  * Equipment and protective systems intended for use in potentially explosive atmospheres
  * Radio equipment
  * Pressure equipment
  * Cableway installations
  * Personal protective equipment
  * Appliances burning gaseous fuels
  * Medical devices
  * In vitro diagnostic medical devices

According to [Article 6(1)](https://artificialintelligenceact.eu/article/6),
if an AI system is covered by the above categories, and already needs to go
through third-party assessment under the relevant legislation, it’s considered
high-risk – with all the relevant obligations applicable to the Provider.

However, the exact interplay between the AI Act and existing regulations, e.g.
the Medical Devices Regulation (MDR), is [not yet
finalized](https://www.emergobyul.com/news/effect-europes-artificial-
intelligence-act-medical-device-industry) and remains a [point of
discussion](https://haiweb.org/wp-content/uploads/2023/03/MDR-
AIAct_OnePager_FINAL.pdf). Obligations for AI systems in Annex II will be
enforced a year later than other high-risk systems to provide sufficient time
to make the requirements compatible.

[ **Annex II**](https://artificialintelligenceact.eu/annex/2) **, Section B:
Other devices that fall under the Union Harmonisation Legislation**

Includes AI systems, or the product for which the AI system is a safety
component, that fall under the following categories:

  * Civil aviation security
  * Two- or three-wheel vehicles and quadricycles
  * Agricultural and forestry vehicles
  * Marine equipment
  * Interoperability of the rail system
  * Motor vehicles and their trailers
  * Civil aviation

According to [Article 2(2)](https://artificialintelligenceact.eu/article/2),
if a high-risk AI system is covered by the above categories, and already needs
to go through third-party assessment under the relevant legislation, it’s
categorized as high-risk. However, unlike for use cases under Annex II Section
A above, **only** **[Article
84](https://artificialintelligenceact.eu/article/84) shall apply.** The
Provider is only obligated to comply with the existing harmonized legislation,
and is exempt from the regular high-risk AI system rules.

* * *

[ **Annex III**](https://artificialintelligenceact.eu/annex/3) provides the
list of other high-risk use cases that are not covered by other existing
harmonization legislation:

  * Biometrics (ones permitted under relevant Union or national law) 
  * Critical infrastructure (e.g. safety component in water, gas, heating or electricity infrastructure)
  * Education and vocational training (e.g. determining admission to training institutions)
  * Employment, workers management and access to self-employment (e.g. monitoring and evaluating performance; targeted job ads)
  * Access to and enjoyment of essential public and private services (e.g.
It is uncertain what share of AI systems will be in the high-risk category, as
both the field of AI and the law are still evolving. The European Commission
estimated in an [impact study](https://digital-
strategy.ec.europa.eu/en/library/impact-assessment-regulation-artificial-
intelligence) that only 5-15% of applications would be subject to stricter
rules. A [study](https://aai.frb.io/assets/files/AI-Act-Risk-Classification-
Study-appliedAI-March-2023.pdf) by appliedAI of 106 enterprise AI systems
found that 18% were high-risk, 42% low-risk, and 40% unclear if high or low
risk. The system with unclear risk classification in this study were mainly
within the areas of critical infrastructure, employment, law enforcement, and
product safety. In a related [survey](https://aai.frb.io/assets/files/AI-Act-
Impact-Survey_Report_Dec12.2022.pdf) of 113 EU AI startups, 33% of the
startups surveyed believed that their AI systems would be classified as high-
risk, compared to the 5-15% assessed by the European Commission.



#### What are the penalties of the EU AI Act?

Based on the European Parliament’s adopted position, using prohibited AI
practices outlined in Article 5 can result in fines of up to €40 million, or
7% of worldwide annual turnover – whichever is higher. Not complying with the
data governance requirements (under Article 10) and the requirements for
transparency and provision of information (under Articles 13) can lead to
fines up to €20 million, or 4% of worldwide turnover if that is higher. Non-
compliance with any other requirements or obligations can result in fines of
up to €10 million or 2% of worldwide turnover – again, whichever is higher.
Fines are tiered depending on the provision violated, with prohibited
practices receiving the highest penalties and other violations receiving lower
maximum fines.



#### What measures does the EU AI Act implement for SMEs specifically?

The EU AI Act aims to provide support for SMEs and startups as specified in
Article 55. Specific measures include granting priority access for SMEs and
EU-based startups to regulatory sandboxes if eligibility criteria are met.
Additionally, tailored awareness raising and digital skills development
activities will be organised to address the needs of smaller organisations.
Moreover, dedicated communication channels will be set up to offer guidance
and respond to queries from SMEs and startups. Participation of SMEs and other
stakeholders in the standards development process will also be encouraged. To
reduce the financial burden of compliance, conformity assessment fees will be
lowered for SMEs and startups based on factors like development stage, size,
and market demand. The Commission will regularly review certification and
compliance costs for SMEs/startups (with input from transparent consultations)
and work with Member States to reduce these costs where possible.



#### Can I voluntarily comply with the EU AI Act even if my system is not in
scope?

Yes! The Commission, the AI Office, and/or Member States will encourage
voluntary codes of conduct for requirements under Title III, Chapter 2 (e.g.
risk management, data governance, and human oversight) for AI systems not
deemed to be high-risk. These codes of conduct will provide technical
solutions for how an AI system can meet the requirements, according to the
system’s intended purpose. Other objectives like environmental sustainability,
accessibility, stakeholder participation, and diversity of development teams
will be considered. Small businesses and startups will be taken into account
when encouraging codes of conduct. See Article 69 on Codes of Conduct for
Voluntary Application of Specific Requirements.



## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"The provided context does not specify the frequency at which the NIST AI RMF (National Institute of Standards and Technology Artificial Intelligence Risk Management Framework) recommends organizations to review and update their AI risk assessments. Instead, it discusses the importance of companies re-evaluating and updating their understanding of AI systems and their impacts, as well as conducting regular analyses of conditions and risks, but does not mention a specific frequency as recommended by NIST AI RMF.","49 6. Re-analy sis of conditions and risks  
(1) Re-implementing Action Targets 1 -1 to 1 -3 in a timely manner  
Action Target 6 -1: Companies that develop and operate AI systems should, under the leadership 
of top management, conduct re -evaluations, update their understanding, obtain new points of 
views, or take other relevant actions with respect to Action Targets 1 -1 through 1.3, in a timely 
manner. When implementing Action Target 5 -2, they should also consider obtaining opinions not 
only for the current AI management system and the operation of such system, but also conducive 
to review of entire AI governance, incl uding analyses of conditions and risks.  
 
[Practical Example 1]  
Because, as of the present time, a standard perception of AI systems has yet to form in our societies, 
we should re -evaluate the positive and negative impacts that AI systems can have, societies’ 
acceptance of AI system development and operation, and our o wn AI proficiency as described in 
Action Targets 1 -1 through 1 -3, as well as update our understanding and acquire new perspectives in 
a timely manner. We regularly perform analyses of conditions and risks, and report to 
management even during normal times where we do not see any “near misses,” or a 
significant increase in public attention to specific incidents, or changes in the regulatory 
environment . While there is an active ongoing debate over how to properly develop and operate AI 
systems, we place emph asis on avoiding governance fatigue by haphazard agile re -analyses and 
using agile re -analyses to understand major trends. Opportunities to report to management are good 
opportunities for directing our attention to major trends.  
 
[Practical Example 2]  
We r egularly analyze conditions and risks as described in Practical Example 1, but because 
verifications of AI governance and AI management systems have overlapping elements, we include 
the positive and negative impacts of AI systems and social acceptance of A I system 
development and operation in the agenda of the AI Ethics Committee meetings that we hold 
regularly with invited external experts to gain their insights into the major trends associated 
with these points .
7\. The testing of the high-risk AI systems shall be performed, as
appropriate, at any point in time throughout the development process, and, in
any event, prior to the placing on the market or the putting into service.
Testing shall be made against prior defined metrics and probabilistic
thresholds that are appropriate to the intended purpose of the high-risk AI
system.

8\. When implementing the risk management system described in paragraphs 1 to
6, providers shall give consideration to whether in view of its intended
purpose the high-risk AI system is likely to adversely impact persons under
the age of 18 and, as appropriate, other vulnerable groups of people.

9\. For providers of high-risk AI systems that are subject to requirements
regarding internal risk management processes under relevant sectorial Union
law, the aspects described in paragraphs 1 to 8 may be part of or combined
with the risk management procedures established pursuant to that law.

[ <- Previous ](https://artificialintelligenceact.eu/article/8/) [ Next ->
](https://artificialintelligenceact.eu/article/10/)

#### Suitable Recitals

[

Recital 47

](https://artificialintelligenceact.eu/recital/47/) [

Recital 64

](https://artificialintelligenceact.eu/recital/64/) [

Recital 66

](https://artificialintelligenceact.eu/recital/66/) [

Recital 67

](https://artificialintelligenceact.eu/recital/67/) [

Recital 69

](https://artificialintelligenceact.eu/recital/69/) [

Recital 71

](https://artificialintelligenceact.eu/recital/71/) [

Recital 81

](https://artificialintelligenceact.eu/recital/81/) [

Recital 138

](https://artificialintelligenceact.eu/recital/138/) [

Recital 155

](https://artificialintelligenceact.eu/recital/155/) [

Recital 158

](https://artificialintelligenceact.eu/recital/158/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"An AI system is considered high-risk under the EU AI Act if it meets either of the following conditions:

1. The AI system is intended to be used as a safety component of a product, or the AI system is itself a product, that is covered by the Union harmonisation legislation listed in Annex II. Additionally, the product or the AI system itself must be required to undergo a third-party conformity assessment as per the Union harmonisation legislation listed in Annex II.

2. The AI system falls under the categories listed in Annex III. However, there is a derogation that allows for AI systems not to be considered high-risk if they do not pose a significant risk of harm to the health, safety, or fundamental rights of natural persons. This can be the case if the AI system meets one or more of the following criteria:
   - It is intended to perform a narrow procedural task.
   - It is intended to improve the result of a previously completed human activity.
   - It is intended to detect decision-making patterns or deviations from prior decision-making patterns and is not meant to replace or influence the previously completed human assessment, without proper human review.
   - It is intended to perform a preparatory task to an assessment relevant for the purpose of the use cases listed in Annex III.

An AI system will always be considered high-risk if it performs profiling of natural persons. Providers who consider their AI systems referred to in Annex III as not high-risk must document their assessment before placing the system on the market or putting it into service and may be subject to registration obligations. The Commission will provide guidelines and a comprehensive list of practical examples to clarify the practical implementation of these criteria.","Examples: real-time biometric
identification, social scoring, manipulative techniques.

### High-Risk AI

These use cases are considered highly harmful if misused, and will need to go
through a third-party conformity assessment before being placed on the market.
According to [Article 6](https://artificialintelligenceact.eu/article/6), an
AI system is considered high-risk under either of the following conditions:

  1. It’s used as a safety component of a product, or is itself a product that’s already regulated by existing EU laws listed in Annex II, and is required to undergo a third-party conformity assessment under those Annex II laws; or,
  2. It falls under Annex III, unless it’s proved to not possess significant risk of harm to health, safety, or fundamental rights

We will now cover the two points above in more detail.

* * *

One of the major points of deliberation during the AI Act’s development was
alignment with existing EU harmonisation legislations that regulate existing
product categories. This is covered by Annex II.

 **[Annex II](https://artificialintelligenceact.eu/annex/2), Section A: List
of Union Harmonisation Legislation Based on the New Legislative Framework**

Includes AI systems, or the products for which the AI system is a safety
component, that fall under the following categories. As per [Article 28
2(a)](https://artificialintelligenceact.eu/article/28/), the manufacturer of
such products is considered the Provider of the included high-risk AI system
if it’s placed on the market under the manufacturer’s name or trademark:

  * Machinery
  * Toys
  * Recreational craft and personal watercraft
  * Lifts
  * Equipment and protective systems intended for use in potentially explosive atmospheres
  * Radio equipment
  * Pressure equipment
  * Cableway installations
  * Personal protective equipment
  * Appliances burning gaseous fuels
  * Medical devices
  * In vitro diagnostic medical devices

According to [Article 6(1)](https://artificialintelligenceact.eu/article/6),
if an AI system is covered by the above categories, and already needs to go
through third-party assessment under the relevant legislation, it’s considered
high-risk – with all the relevant obligations applicable to the Provider.

However, the exact interplay between the AI Act and existing regulations, e.g.
the Medical Devices Regulation (MDR), is [not yet
finalized](https://www.emergobyul.com/news/effect-europes-artificial-
intelligence-act-medical-device-industry) and remains a [point of
discussion](https://haiweb.org/wp-content/uploads/2023/03/MDR-
AIAct_OnePager_FINAL.pdf). Obligations for AI systems in Annex II will be
enforced a year later than other high-risk systems to provide sufficient time
to make the requirements compatible.

[ **Annex II**](https://artificialintelligenceact.eu/annex/2) **, Section B:
Other devices that fall under the Union Harmonisation Legislation**

Includes AI systems, or the product for which the AI system is a safety
component, that fall under the following categories:

  * Civil aviation security
  * Two- or three-wheel vehicles and quadricycles
  * Agricultural and forestry vehicles
  * Marine equipment
  * Interoperability of the rail system
  * Motor vehicles and their trailers
  * Civil aviation

According to [Article 2(2)](https://artificialintelligenceact.eu/article/2),
if a high-risk AI system is covered by the above categories, and already needs
to go through third-party assessment under the relevant legislation, it’s
categorized as high-risk. However, unlike for use cases under Annex II Section
A above, **only** **[Article
84](https://artificialintelligenceact.eu/article/84) shall apply.** The
Provider is only obligated to comply with the existing harmonized legislation,
and is exempt from the regular high-risk AI system rules.

* * *

[ **Annex III**](https://artificialintelligenceact.eu/annex/3) provides the
list of other high-risk use cases that are not covered by other existing
harmonization legislation:

  * Biometrics (ones permitted under relevant Union or national law) 
  * Critical infrastructure (e.g. safety component in water, gas, heating or electricity infrastructure)
  * Education and vocational training (e.g. determining admission to training institutions)
  * Employment, workers management and access to self-employment (e.g. monitoring and evaluating performance; targeted job ads)
  * Access to and enjoyment of essential public and private services (e.g.
eu/annex/12/) [

Annex XIII: Criteria for the designation of general purpose AI models with
systemic risk referred to in Article 51

](https://artificialintelligenceact.eu/annex/13/)

## Search within the Act

Search for:

⚠️ The AI Act Explorer is currently being updated to reflect the texts adopted
by the European Parliament on 13 March 2024. Internal links to other parts of
the Act may not work as expected.

#### article

Part of [Section 1: Classification of AI Systems as High-
Risk](https://artificialintelligenceact.eu/chapter/3-1/)

# Article 6: Classification Rules for High-Risk AI Systems

**Feedback** – We are working to improve this tool. Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

1\. Irrespective of whether an AI system is placed on the market or put into
service independently from the products referred to in points (a) and (b),
that AI system shall be considered high-risk where both of the following
conditions are fulfilled:

(a) the AI system is intended to be used as a safety component of a product,
or the AI system is itself a product, covered by the Union harmonisation
legislation listed in [Annex
I](https://artificialintelligenceact.eu/annex/1/);

(b) the product whose safety component pursuant to point (a) is the AI system,
or the AI system itself as a product, is required to undergo a third party
conformity assessment, with a view to the placing on the market or putting
into service of that product pursuant to the Union harmonisation legislation
listed in [Annex I](https://artificialintelligenceact.eu/annex/1/).

2\. In addition to the high-risk AI systems referred to in paragraph 1, AI
systems referred to in [Annex
III](https://artificialintelligenceact.eu/annex/3/) shall also be considered
high-risk.

2a. By derogation from paragraph 2 AI systems shall not be considered as high
risk if they do not pose a significant risk of harm, to the health, safety or
fundamental rights of natural persons, including by not materially influencing
the outcome of decision making. This shall be the case if one or more of the
following criteria are fulfilled:

(a) the AI system is intended to perform a narrow procedural task;

(b) the AI system is intended to improve the result of a previously completed
human activity;

(c) the AI system is intended to detect decision-making patterns or deviations
from prior decision-making patterns and is not meant to replace or influence
the previously completed human assessment, without proper human review; or

(d) the AI system is intended to perform a preparatory task to an assessment
relevant for the purpose of the use cases listed in [Annex
III](https://artificialintelligenceact.eu/annex/3/). Notwithstanding first
subparagraph of this paragraph, an AI system shall always be considered high-
risk if the AI system performs profiling of natural persons.

2b. A provider who considers that an AI system referred to in [Annex
III](https://artificialintelligenceact.eu/annex/3/) is not high-risk shall
document its assessment before that system is placed on the market or put into
service. Such provider shall be subject to the registration obligation set out
in [Article 49](https://artificialintelligenceact.eu/article/49/)(1a). Upon
request of national competent authorities, the provider shall provide the
documentation of the assessment.

2c. The Commission shall, after consulting the AI Board, and no later than [18
months] after the entry into force of this Regulation, provide guidelines
specifying the practical implementation of this article completed by a
comprehensive list of practical examples of high risk and non-high risk use
cases on AI systems pursuant to [Article
82b](https://artificialintelligenceact.eu/article/82b/).

2d. The Commission is empowered to adopt delegated acts in accordance with
[Article 97](https://artificialintelligenceact.eu/article/97/) to amend the
criteria laid down in points a) to d) of the first subparagraph of paragraph
2a."
"The context provided does not mention the NIST AI RMF (Risk Management Framework) or any specific security requirements under it. Instead, it discusses the compliance with cybersecurity requirements for high-risk AI systems under Regulation 2022/0272, which is separate from NIST standards. The context outlines that high-risk AI systems can demonstrate compliance with cybersecurity requirements by fulfilling the essential cybersecurity requirements set out in Article 10 and Annex I of Regulation 2022/0272. It also mentions that the assessment of cybersecurity risks should consider AI-specific vulnerabilities and that the European Commission should cooperate with ENISA on issues related to cybersecurity of AI systems. However, there is no mention of the NIST AI RMF or its specific security requirements in the provided context.","Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

Without prejudice to the requirements related to robustness and accuracy set
out in this Regulation, high-risk AI systems which fall within the scope of
the Regulation 2022/0272, in accordance with Article 8 of the the Regulation
2022/0272 may demonstrate compliance with the cybersecurity requirement of
this Regulation by fulfilling the essential cybersecurity requirements set out
in Article 10 and Annex I of the Regulation 2022/0272.When high-risk AI
systems fulfil the essential requirements of Regulation 2022/0272, they should
be deemed compliant with the cybersecurity requirements set out in this
Regulation in so far as the achievement of those requirements is demonstrated
in the EU declaration of conformity or parts thereof issued under Regulation
2022/0272. For this purpose, the assessment of the cybersecurity risks,
associated to a product with digital elements classified as high-risk AI
system according to this Regulation, carried out under Regulation 2022/0272,
should consider risks to the cyber resilience of an AI system as regards
attempts by unauthorised third parties to alter its use, behaviour or
performance, including AI specific vulnerabilities such as data poisoning or
adversarial attacks, as well as, as relevant, risks to fundamental rights as
required by this Regulation. The conformity assessment procedure provided by
this Regulation should apply in relation to the essential cybersecurity
requirements of a product with digital elements covered by Regulation
2022/0272 and classified as a high-risk AI system under this Regulation.
However, this rule should not result in reducing the necessary level of
assurance for critical products with digital elements covered by Regulation
2022/0272. Therefore, by way of derogation from this rule, high-risk AI
systems that fall within the scope of this Regulation and are also qualified
as important and critical products with digital elements pursuant to
Regulation 2022/0272 and to which the conformity assessment procedure based on
internal control referred to in [Annex
VI](https://artificialintelligenceact.eu/annex/6/) of this Regulation applies,
are subject to the conformity assessment provisions of Regulation 2022/0272
insofar as the essential cybersecurity requirements of Regulation 2022/0272
are concerned. In this case, for all the other aspects covered by this
Regulation the respective provisions on conformity assessment based on
internal control set out in [Annex
VI](https://artificialintelligenceact.eu/annex/6/) of this Regulation should
apply. Building on the knowledge and expertise of ENISA on the cybersecurity
policy and tasks assigned to ENISA under the Regulation 2019/1020 the European
Commission should cooperate with ENISA on issues related to cybersecurity of
AI systems.

[ <- Previous ](https://artificialintelligenceact.eu/recital/76/) [ Next ->
](https://artificialintelligenceact.eu/recital/78/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
It requires developers to implement a standardized evaluation protocol for AI models and datasets.
  * B.6.2.8 AI system recording of event logs. It requires developers to have a logging system to record production input data, output predictions, and potential outliers.
  * B.7.4 Quality of data for AI systems. It requires developers to define and measure training and production data quality, as well as consider impact of data bias on performance.

(If you’re interested in a solution to automate the requirements above,
consider using [Citadel Lens](https://www.citadel.co.jp/en/products/lens/) and
[Citadel Radar](https://www.citadel.co.jp/en/products/radar/).)

###### Organization-level Requirements

These requirements apply to the whole organization and its processes:

  * B.2.2 AI policy
  * B.2.3 Alignment with other organizational policies
  * B.2.4 Review of the AI policy
  * B.3.2 AI roles and responsibilities
  * B.3.3 Reporting of concerns
  * B.5.2 AI system impact assessment process
  * B.6.1.2 Objectives for responsible development of AI system
  * B.6.1.3 Processes for responsible design and development of AI systems
  * B.7.2 Data for development and enhancement of AI system
  * B.7.3 Acquisition of data
  * B.8.3 External reporting
  * B.8.4 Communication of incidents
  * B.8.5 Information for interested parties
  * B.9.2 Processes for responsible use of AI
  * B.9.3 Objectives for responsible use of AI system
  * B.10.2 Allocating responsibilities
  * B.10.3 Suppliers
  * B.10.4 Customers

###### Project-level Requirements

These requirements apply to specific projects within the organization:

  * B.4.2 Resource documentation
  * B.4.3 Data resources
  * B.4.4 Tooling resources
  * B.4.5 System and computing resources
  * B.4.6 Human resources
  * B.5.3 Documentation of AI system impact assessments
  * B.5.4 Assessing AI system impact on individuals and groups of individuals
  * B.5.5 Assessing societal impacts of AI systems
  * B.6.2.2 AI system requirements and specification
  * B.6.2.3 Documentation of AI system design and development
  * B.6.2.4 AI system verification and validation
  * B.6.2.5 AI system deployment
  * B.6.2.6 AI system operation and monitoring
  * B.6.2.7 AI system technical documentation
  * B.6.2.8 AI system recording of event logs
  * B.7.4 Quality of data for AI systems
  * B.7.5 Data provenance
  * B.7.6 Data preparation
  * B.8.2 System documentation and information for users
  * B.9.3 Objectives for responsible use of AI system
  * B.9.4 Intended use of the AI system

### Does ISO 42001 cover all technical aspects of various AI processes?

ISO 42001 sits at the center of the responsible AI standards ecosystem, which
includes other standards that go deeper into specific aspects of AI
management, use, and development. The goal of ISO 42001 is to maintain a
balance between being prescriptive in terms of the requirements for an AIMS,
but flexible in terms of how AIMS is implemented for a particular
organization.

Since ISO 42001 itself **does not address the details of specific AI
applications** , it outsources the more technical details to more narrowly
focused standards and other ""generally accepted frameworks"".  Some examples of
other standards referenced in ISO 42001 are:

  * [ISO 5259](https://www.iso.org/standard/81860.html) series on Data Quality
  * [ISO 23894](https://www.iso.org/standard/77304.html) AI Risk Management
  * [ISO 24029-1](https://www.iso.org/standard/77609.html) Assessment of the robustness of neural networks

### What is the process for ISO 42001 certification?

To obtain an ISO 42001 certification, an organization must successfully pass
an audit conducted by a certification body. However, the [ISO
42006](https://www.iso.org/standard/44546.html) standard defining the
requirements for such bodies is currently under development, which means that
**as of January 2024, no certifications can be granted yet**."
"The provided context information does not mention the NIST AI RMF (AI Risk Management Framework) or its specific focus areas. Therefore, based on the context provided, it is not possible to determine whether the NIST AI RMF includes guidelines for AI deployment or if it is focused only on development and management.","Machine Learning Quality Management Guideline,   National Institute of  
3rd English edition    Advanced Industrial Science and Technology  
 
191 
  
Table 11: Security considerations 
Chapter and 
section  Guideline description  Security response plan Remarks  
1.3.2  Lifetime -long requirements 
for risk assessment  At the time of security risk assessment, the result s of the functional 
safety risk assessment  and the items to be treated with priority are 
included in the targets of assumed attacks. Include items such as privacy and fairness 
that related  to the target business for which 
the system will be used. 
1.3.3  when a machine leaning based  
system is developed by 
sharing works  When referring to terms  and guidelines related to AI and machine 
learning, include compliance with referenced standards and guidelines 
for the entire supply chain in the contract. Machine Learning Quality Management 
Guidelines, ISO/IEC regulations, etc.  
 GDPR, China Cyber Security Law, etc. 
depending on the business to be designed 
and developed.  
 Also refer to laws related to outsourcing, 
such as  the Dispatched Worker Law and the 
Subcontracting Law.  
1.3.3  Security risk of contamination 
of learning  results due to 
intentional inclusion of 
improper  data  ● Collect information on attacks specializing in AI and machine 
learning, such as evasion atta cks, data poisoning attacks, and model 
poisoning attacks exemplified in Chapter 9 of the Machine Learning Quality Management Guidelines, analyze attack scenarios,  and identify 
threats and vulnerabilities. implement measures for  
● Collect information on attack and defense methods and update 
defense measures periodically . The cycle of review of overall defense 
measures should be within one year. 
 Defense measures should be prioritized 
and implemented from the most feasible 
items.
31 Management Agency have created , based on these Guidelines, a format for recording details of the 
implementation of reliability assessments in the area of plant security. In addition, we also learned 
that model cards are proposed with the belief that AI models should have labelling abou t their 
performance just as ingredient labelling on food products help consumers make responsible choices20. 
At the moment, although there is no standard documenting procedure for sharing, performance and 
quality information of trained machine learning mod els, etc . between plural companies , we intend to 
refer to various other efforts instead of considering our own standards from scratch in 
establishing our in -house system . 
 
[Practical Example 2]  
We belong to an organization engaged in AI ethics and quality,  and actively exchange views 
with other affiliated companies on best practices for providing information on AI system’s 
performance and other attributes . AI system users should be provided with sufficient information 
about AI systems, but because not all u sers, whether they are consumers or other users, are 
knowledgeable about details of characteristics and limitations of AI, it is not appropriate to think that 
it is enough merely to give AI system users  information that is difficult for anyone other than e xperts 
to understand or huge amounts of detailed information unilaterally . In order to consider appropriate 
ways of providing information, it is important for companies to not only have direct user experience of 
their own, but also indirect contact with la rge numbers of users through exchanges of views with other 
companies.  
Information that AI system developers should give to AI system operators includes, for example, 
information about the data used to develop their AI systems. For example, this may include  
information on the source of data (which may sometimes be open data), the amount of data and their 
distribution, and overviews for each of the categories covered in the data. It is also important to 
describe the algorithms chosen (or not chosen) during de velopment, provide an overview of the 
generated model, and in particular, to explain the conditions under which tests were performed and 
how much accuracy was attained.  
While these perspectives are not new to companies with rich experience in developing an d operating 
AI systems, we believe that “how we communicate ” is important;  that is, types of information we 
should provide and how in -depth our explanations should be.  Understanding the current state 
of information sharing among plural companies is important in considering the overall design 
of AI governance , and herein lies the meaningfulness of participating in organizations that focus on 
AI ethics and quality.  
 
  
 
20 Google, “The value of a shared understanding of AI models,” https://modelcards.withgoogle.com/about ."
"AI systems for military, defense, or national security purposes are excluded from the scope of the EU AI Act. However, if such an AI system is used for other purposes, such as civilian or law enforcement, it would fall within the scope of the regulation and would need to comply with its requirements. Conversely, an AI system placed on the market for civilian or law enforcement purposes that is later used for military, defense, or national security purposes would not fall within the scope of the regulation.","eu/annex/13/)

## Search within the Act

Search for:

⚠️ The AI Act Explorer is currently being updated to reflect the texts adopted
by the European Parliament on 13 March 2024. Internal links to other parts of
the Act may not work as expected.

#### recital

# Recital 24

**Feedback** – We are working to improve this tool. Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

If and insofar AI systems are placed on the market, put into service, or used
with or without modification of such systems for military, defence or national
security purposes, those should be excluded from the scope of this Regulation
regardless of which type of entity is carrying out those activities, such as
whether it is a public or private entity. As regards military and defence
purposes, such exclusion is justified both by Article 4(2) TEU and by the
specifities of the Member States’ and the common Union defence policy covered
by Chapter 2 of Title V of the Treaty on European Union (TEU) that are subject
to public international law, which is therefore the more appropriate legal
framework for the regulation of AI systems in the context of the use of lethal
force and other AI systems in the context of military and defence activities.
As regards national security purposes, the exclusion is justified both by the
fact that national security remains the sole responsibility of Member States
in accordance with Article 4(2) TEU and by the specific nature and operational
needs of national security activities and specific national rules applicable
to those activities. Nonetheless, if an AI system developed, placed on the
market, put into service or used for military, defence or national security
purposes is used outside those temporarily or permanently for other purposes
(for example, civilian or humanitarian purposes, law enforcement or public
security purposes), such a system would fall within the scope of this
Regulation. In that case, the entity using the system for other than military,
defence or national security purposes should ensure compliance of the system
with this Regulation, unless the system is already compliant with this
Regulation. AI systems placed on the market or put into service for an
excluded (i.e. military, defence or national security) and one or more non
excluded purposes (e.g. civilian purposes, law enforcement, etc.), fall within
the scope of this Regulation and providers of those systems should ensure
compliance with this Regulation. In those cases, the fact that an AI system
may fall within the scope of this Regulation should not affect the possibility
of entities carrying out national security, defence and military activities,
regardless of the type of entity carrying out those activities, to use AI
systems for national security, military and defence purposes, the use of which
is excluded from the scope of this Regulation. An AI system placed on the
market for civilian or law enforcement purposes which is used with or without
modification for military, defence or national security purposes should not
fall within the scope of this Regulation, regardless of the type of entity
carrying out those activities.

[ <- Previous ](https://artificialintelligenceact.eu/recital/23/) [ Next ->
](https://artificialintelligenceact.eu/recital/25/)

#### This Recital relates to

[Article 2: Scope](https://artificialintelligenceact.eu/article/2/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
Depending on the
type of AI system, the use of the system may affect persons other than the
deployer.

[ <- Previous ](https://artificialintelligenceact.eu/recital/12/) [ Next ->
](https://artificialintelligenceact.eu/recital/14/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"Yes, the EU AI Act applies to AI systems developed outside of the EU if the output produced by those systems is intended to be used within the EU. Providers and deployers of AI systems that are established in a third country are subject to the regulation to the extent that their system's output is used in the Union. This is to ensure effective protection of natural persons located in the Union and to prevent circumvention of the Regulation.","Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

In light of their digital nature, certain AI systems should fall within the
scope of this Regulation even when they are neither placed on the market, nor
put into service, nor used in the Union. This is the case for example of an
operator established in the Union that contracts certain services to an
operator established outside the Union in relation to an activity to be
performed by an AI system that would qualify as high-risk. In those
circumstances, the AI system used by the operator outside the Union could
process data lawfully collected in and transferred from the Union, and provide
to the contracting operator in the Union the output of that AI system
resulting from that processing, without that AI system being placed on the
market, put into service or used in the Union. To prevent the circumvention of
this Regulation and to ensure an effective protection of natural persons
located in the Union, this Regulation should also apply to providers and
deployers of AI systems that are established in a third country, to the extent
the output produced by those systems is intended to be used in the Union.
Nonetheless, to take into account existing arrangements and special needs for
future cooperation with foreign partners with whom information and evidence is
exchanged, this Regulation should not apply to public authorities of a third
country and international organisations when acting in the framework of
cooperation or international agreements concluded at national or European
level for law enforcement and judicial cooperation with the Union or with its
Member States, under the condition that this third country or international
organisations provide adequate safeguards with respect to the protection of
fundamental rights and freedoms of individuals. Where relevant, this may also
cover activities of entities entrusted by the third countries to carry out
specific tasks in support of such law enforcement and judicial cooperation.
Such framework for cooperation or agreements have been established bilaterally
between Member States and third countries or between the European Union,
Europol and other EU agencies and third countries and international
organisations. The authorities competent for supervision of the law
enforcement and judicial authorities under the AI Act should assess whether
these frameworks for cooperation or international agreements include adequate
safeguards with respect to the protection of fundamental rights and freedoms
of individuals. Recipient Member States authorities and Union institutions,
offices and bodies making use of such outputs in the Union remain accountable
to ensure their use complies with Union law. When those international
agreements are revised or new ones are concluded in the future, the
contracting parties should undertake the utmost effort to align those
agreements with the requirements of this Regulation.

[ <- Previous ](https://artificialintelligenceact.eu/recital/21/) [ Next ->
](https://artificialintelligenceact.eu/recital/23/)

#### This Recital relates to

[Article 2: Scope](https://artificialintelligenceact.eu/article/2/)

[Article 50: Transparency Obligations for Providers and Users of Certain AI
Systems and GPAI Models](https://artificialintelligenceact.eu/article/50/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
The
EU’s AI regulation is already making waves internationally. In late September
2021, Brazil’s Congress [passed](https://politico.us8.list-
manage.com/track/click?u=e26c1a1c392386a968d02fdbc&id=edf7623d97&e=e4d8507e94)
a bill that creates a legal framework for artificial intelligence. It still
needs to pass the country's Senate.

![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/robot.webp)

## How can organisations apply it?

We have developed a new tool at FLI to help European SMEs and startups better
understand whether they might have any legal obligations under the EU AI Act
or whether they may implement the Act solely to make their business stand out
as more trustworthy. Please note that the Act is still in negotiations, and
our tool is a simplification. There are three positions with different
proposals for the Act currently available and we were selective to make the
tool more user-friendly. This tool can help give an indication about what
obligations your system might face. This tool is still a work in progress.
Please use the tool [here](https://artificialintelligenceact.eu/assessment/eu-
ai-act-compliance-checker/).

![](https://artificialintelligenceact.eu/wp-content/uploads/2021/08/document-
scaled.jpeg)

## Articles on the AI Act

####  [The AI Office is hiring](https://artificialintelligenceact.eu/the-ai-
office-is-hiring/)

Mar 22, 2024

The European Commission is recruiting contract agents who are AI technology
specialists to govern the most cutting-edge AI models.  Deadline to apply is
12:00 CET on 27 March (application form).  Role This is an opportunity to work
in a team within the...

####  [The AI Office: What is it, and how does it
work?](https://artificialintelligenceact.eu/the-ai-office-summary/)

Mar 21, 2024

In this overview, we offer a summary of the key elements of the AI Office
relevant for those interested in AI governance. We’ve highlighted the
responsibilities of the AI Office, its role within the European Commission,
its relationship with the AI Board, its national...

####  [AI Act Implementation: Timelines & Next
steps](https://artificialintelligenceact.eu/ai-act-implementation-next-steps/)

Feb 28, 2024

In this article we provide an outline of the key dates relevant to the
implementation of the AI Act. We also list some secondary legislation that the
Commission might add to supplement the AI Act, and some guidelines it may
publish to support compliance efforts. The...

####  [High-level summary of the AI
Act](https://artificialintelligenceact.eu/high-level-summary/)

Feb 27, 2024

Everything you need to know about the AI Act in ten minutes.

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"No, small businesses that use low-risk AI applications cannot completely ignore the requirements of the EU AI Act. While the Act does provide certain measures to support SMEs, such as lowering conformity assessment fees and offering guidance, all AI systems, including those classified as low-risk, must comply with the general requirements set out in the Act. Non-compliance with these requirements can result in fines, although the fines for non-compliance with lower-risk AI systems are less severe than for high-risk AI systems.","It is uncertain what share of AI systems will be in the high-risk category, as
both the field of AI and the law are still evolving. The European Commission
estimated in an [impact study](https://digital-
strategy.ec.europa.eu/en/library/impact-assessment-regulation-artificial-
intelligence) that only 5-15% of applications would be subject to stricter
rules. A [study](https://aai.frb.io/assets/files/AI-Act-Risk-Classification-
Study-appliedAI-March-2023.pdf) by appliedAI of 106 enterprise AI systems
found that 18% were high-risk, 42% low-risk, and 40% unclear if high or low
risk. The system with unclear risk classification in this study were mainly
within the areas of critical infrastructure, employment, law enforcement, and
product safety. In a related [survey](https://aai.frb.io/assets/files/AI-Act-
Impact-Survey_Report_Dec12.2022.pdf) of 113 EU AI startups, 33% of the
startups surveyed believed that their AI systems would be classified as high-
risk, compared to the 5-15% assessed by the European Commission.



#### What are the penalties of the EU AI Act?

Based on the European Parliament’s adopted position, using prohibited AI
practices outlined in Article 5 can result in fines of up to €40 million, or
7% of worldwide annual turnover – whichever is higher. Not complying with the
data governance requirements (under Article 10) and the requirements for
transparency and provision of information (under Articles 13) can lead to
fines up to €20 million, or 4% of worldwide turnover if that is higher. Non-
compliance with any other requirements or obligations can result in fines of
up to €10 million or 2% of worldwide turnover – again, whichever is higher.
Fines are tiered depending on the provision violated, with prohibited
practices receiving the highest penalties and other violations receiving lower
maximum fines.



#### What measures does the EU AI Act implement for SMEs specifically?

The EU AI Act aims to provide support for SMEs and startups as specified in
Article 55. Specific measures include granting priority access for SMEs and
EU-based startups to regulatory sandboxes if eligibility criteria are met.
Additionally, tailored awareness raising and digital skills development
activities will be organised to address the needs of smaller organisations.
Moreover, dedicated communication channels will be set up to offer guidance
and respond to queries from SMEs and startups. Participation of SMEs and other
stakeholders in the standards development process will also be encouraged. To
reduce the financial burden of compliance, conformity assessment fees will be
lowered for SMEs and startups based on factors like development stage, size,
and market demand. The Commission will regularly review certification and
compliance costs for SMEs/startups (with input from transparent consultations)
and work with Member States to reduce these costs where possible.



#### Can I voluntarily comply with the EU AI Act even if my system is not in
scope?

Yes! The Commission, the AI Office, and/or Member States will encourage
voluntary codes of conduct for requirements under Title III, Chapter 2 (e.g.
risk management, data governance, and human oversight) for AI systems not
deemed to be high-risk. These codes of conduct will provide technical
solutions for how an AI system can meet the requirements, according to the
system’s intended purpose. Other objectives like environmental sustainability,
accessibility, stakeholder participation, and diversity of development teams
will be considered. Small businesses and startups will be taken into account
when encouraging codes of conduct. See Article 69 on Codes of Conduct for
Voluntary Application of Specific Requirements.



## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
It worries that
the law, in its current form, would fail to achieve the objective of
protecting fundamental rights. Specifically, it does not think that the
proposal goes far enough to protect fundamental rights in relation to
biometric applications such as emotion recognition and AI polygraphs. The
current draft of the AI Act advocates for transparency obligations for these
applications, but Access Now recommends stronger measures to reduce all
associated risks, such as bans. Read its concrete suggestions
[here](https://ec.europa.eu/info/law/better-regulation/have-your-
say/initiatives/12527-Artificial-intelligence-ethical-and-legal-
requirements/F2665462_en).

![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/09/AccessNowNewSiteAnnouncement.jpeg)

## Michael Veale & Frederik Zuiderveen Borgesius

Michael Veale, Assistant Professor at University College London in Digital
Rights and Regulation, and Frederik Zuiderveen Borgesius, Professor of ICT and
Private Law at the Dutch Radboud University, provide a thorough analysis of
some of the most sensitive parts of the EU AI Act. One of their article's many
surprising insights is that compliance with the law would almost completely
depend on self-assessment. Self-assessment means that there is no enforcement
to comply with the law. Once standardisation bodies such as CEN and CENELEC
have published their standards, third-party verification with the law will no
longer be required. The full article can be found
[here](https://osf.io/preprints/socarxiv/38p5f).

![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/michael_veale_800x500.png)

## The Future Society

The Future Society, a nonprofit organisation registered in Estonia, which
advocates for the responsible adoption of AI for the benefit of humanity,
submitted its feedback to the European Commission on the EU AI Act. One of its
suggestions is to ensure governance remains responsive to technological
trends. This could be achieved by improving the flow of information between
national and European institutions and systematically compiling and analysing
incident reports from member states. Read the full feedback
[here](https://ec.europa.eu/info/law/better-regulation/have-your-
say/initiatives/12527-Artificial-intelligence-ethical-and-legal-
requirements/F2665611_en).

![](https://artificialintelligenceact.eu/wp-content/uploads/2021/08/FS.png)

## Nathalie A. Smuha and colleagues

Nathalie A. Smuha, Researcher at the Faculty of Law at KU Leuven, Emma Ahmed-
Rengers, PhD Researcher in Law and Computer Science at the University of
Birmingham, and colleagues argue that the EU AI Act does not always accurately
recognise the wrongs and harms associated with different kinds of AI systems
nor allocate responsibility for them adequately. They also state that the
proposal does not provide an effective framework for the enforcement of legal
rights and duties. The proposal neglects to ensure meaningful transparency,
accountability, and rights of public participation. Read the full article
[here](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3899991).

![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/smuha.jpeg)

## The European DIGITAL SME Alliance

The European DIGITAL SME Alliance, a network of ICT small and medium
enterprises (SME) in Europe, welcomes a harmonised AI regulation and focus on
ethical AI in the EU but suggests many improvements to avoid overburdening
SMEs. For instance, it contends that wherever conformity assessments will be
based on standards, SMEs should actively participate in the development of
those standards. Otherwise, standards may be written in a way that is
impractical for SMEs. Many other recommendations can be read
[here](https://ec.europa.eu/info/law/better-regulation/have-your-
say/initiatives/12527-Artificial-intelligence-ethical-and-legal-
requirements/F2665574_en).

![](https://artificialintelligenceact.eu/wp-content/uploads/2021/09/DIGITAL-
SME-Logo.png)

## The cost of the EU AI Act

Center for Data Innovation, a nonprofit focused on data-driven innovation,
published a [report](https://www2.datainnovation.org/2021-aia-costs.pdf)
claiming that the EU AI Act will cost €31 billion over the next five years and
reduce AI investments by almost 20%."
"The EU AI Act does not explicitly categorize AI applications as ""minimal-risk"" within the provided context. The Act focuses on identifying high-risk AI applications, which are subject to stricter regulations and requirements, including third-party conformity assessments before being placed on the market. AI applications that are not identified as high-risk may be considered lower risk, but the provided context does not detail a specific ""minimal-risk"" category.","It is uncertain what share of AI systems will be in the high-risk category, as
both the field of AI and the law are still evolving. The European Commission
estimated in an [impact study](https://digital-
strategy.ec.europa.eu/en/library/impact-assessment-regulation-artificial-
intelligence) that only 5-15% of applications would be subject to stricter
rules. A [study](https://aai.frb.io/assets/files/AI-Act-Risk-Classification-
Study-appliedAI-March-2023.pdf) by appliedAI of 106 enterprise AI systems
found that 18% were high-risk, 42% low-risk, and 40% unclear if high or low
risk. The system with unclear risk classification in this study were mainly
within the areas of critical infrastructure, employment, law enforcement, and
product safety. In a related [survey](https://aai.frb.io/assets/files/AI-Act-
Impact-Survey_Report_Dec12.2022.pdf) of 113 EU AI startups, 33% of the
startups surveyed believed that their AI systems would be classified as high-
risk, compared to the 5-15% assessed by the European Commission.



#### What are the penalties of the EU AI Act?

Based on the European Parliament’s adopted position, using prohibited AI
practices outlined in Article 5 can result in fines of up to €40 million, or
7% of worldwide annual turnover – whichever is higher. Not complying with the
data governance requirements (under Article 10) and the requirements for
transparency and provision of information (under Articles 13) can lead to
fines up to €20 million, or 4% of worldwide turnover if that is higher. Non-
compliance with any other requirements or obligations can result in fines of
up to €10 million or 2% of worldwide turnover – again, whichever is higher.
Fines are tiered depending on the provision violated, with prohibited
practices receiving the highest penalties and other violations receiving lower
maximum fines.



#### What measures does the EU AI Act implement for SMEs specifically?

The EU AI Act aims to provide support for SMEs and startups as specified in
Article 55. Specific measures include granting priority access for SMEs and
EU-based startups to regulatory sandboxes if eligibility criteria are met.
Additionally, tailored awareness raising and digital skills development
activities will be organised to address the needs of smaller organisations.
Moreover, dedicated communication channels will be set up to offer guidance
and respond to queries from SMEs and startups. Participation of SMEs and other
stakeholders in the standards development process will also be encouraged. To
reduce the financial burden of compliance, conformity assessment fees will be
lowered for SMEs and startups based on factors like development stage, size,
and market demand. The Commission will regularly review certification and
compliance costs for SMEs/startups (with input from transparent consultations)
and work with Member States to reduce these costs where possible.



#### Can I voluntarily comply with the EU AI Act even if my system is not in
scope?

Yes! The Commission, the AI Office, and/or Member States will encourage
voluntary codes of conduct for requirements under Title III, Chapter 2 (e.g.
risk management, data governance, and human oversight) for AI systems not
deemed to be high-risk. These codes of conduct will provide technical
solutions for how an AI system can meet the requirements, according to the
system’s intended purpose. Other objectives like environmental sustainability,
accessibility, stakeholder participation, and diversity of development teams
will be considered. Small businesses and startups will be taken into account
when encouraging codes of conduct. See Article 69 on Codes of Conduct for
Voluntary Application of Specific Requirements.



## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
Examples: real-time biometric
identification, social scoring, manipulative techniques.

### High-Risk AI

These use cases are considered highly harmful if misused, and will need to go
through a third-party conformity assessment before being placed on the market.
According to [Article 6](https://artificialintelligenceact.eu/article/6), an
AI system is considered high-risk under either of the following conditions:

  1. It’s used as a safety component of a product, or is itself a product that’s already regulated by existing EU laws listed in Annex II, and is required to undergo a third-party conformity assessment under those Annex II laws; or,
  2. It falls under Annex III, unless it’s proved to not possess significant risk of harm to health, safety, or fundamental rights

We will now cover the two points above in more detail.

* * *

One of the major points of deliberation during the AI Act’s development was
alignment with existing EU harmonisation legislations that regulate existing
product categories. This is covered by Annex II.

 **[Annex II](https://artificialintelligenceact.eu/annex/2), Section A: List
of Union Harmonisation Legislation Based on the New Legislative Framework**

Includes AI systems, or the products for which the AI system is a safety
component, that fall under the following categories. As per [Article 28
2(a)](https://artificialintelligenceact.eu/article/28/), the manufacturer of
such products is considered the Provider of the included high-risk AI system
if it’s placed on the market under the manufacturer’s name or trademark:

  * Machinery
  * Toys
  * Recreational craft and personal watercraft
  * Lifts
  * Equipment and protective systems intended for use in potentially explosive atmospheres
  * Radio equipment
  * Pressure equipment
  * Cableway installations
  * Personal protective equipment
  * Appliances burning gaseous fuels
  * Medical devices
  * In vitro diagnostic medical devices

According to [Article 6(1)](https://artificialintelligenceact.eu/article/6),
if an AI system is covered by the above categories, and already needs to go
through third-party assessment under the relevant legislation, it’s considered
high-risk – with all the relevant obligations applicable to the Provider.

However, the exact interplay between the AI Act and existing regulations, e.g.
the Medical Devices Regulation (MDR), is [not yet
finalized](https://www.emergobyul.com/news/effect-europes-artificial-
intelligence-act-medical-device-industry) and remains a [point of
discussion](https://haiweb.org/wp-content/uploads/2023/03/MDR-
AIAct_OnePager_FINAL.pdf). Obligations for AI systems in Annex II will be
enforced a year later than other high-risk systems to provide sufficient time
to make the requirements compatible.

[ **Annex II**](https://artificialintelligenceact.eu/annex/2) **, Section B:
Other devices that fall under the Union Harmonisation Legislation**

Includes AI systems, or the product for which the AI system is a safety
component, that fall under the following categories:

  * Civil aviation security
  * Two- or three-wheel vehicles and quadricycles
  * Agricultural and forestry vehicles
  * Marine equipment
  * Interoperability of the rail system
  * Motor vehicles and their trailers
  * Civil aviation

According to [Article 2(2)](https://artificialintelligenceact.eu/article/2),
if a high-risk AI system is covered by the above categories, and already needs
to go through third-party assessment under the relevant legislation, it’s
categorized as high-risk. However, unlike for use cases under Annex II Section
A above, **only** **[Article
84](https://artificialintelligenceact.eu/article/84) shall apply.** The
Provider is only obligated to comply with the existing harmonized legislation,
and is exempt from the regular high-risk AI system rules.

* * *

[ **Annex III**](https://artificialintelligenceact.eu/annex/3) provides the
list of other high-risk use cases that are not covered by other existing
harmonization legislation:

  * Biometrics (ones permitted under relevant Union or national law) 
  * Critical infrastructure (e.g. safety component in water, gas, heating or electricity infrastructure)
  * Education and vocational training (e.g. determining admission to training institutions)
  * Employment, workers management and access to self-employment (e.g. monitoring and evaluating performance; targeted job ads)
  * Access to and enjoyment of essential public and private services (e.g."
"The EU AI Act does categorize AI systems based on their level of risk, with high-risk AI systems subject to more stringent requirements. However, simply adhering to a list of high-risk applications does not guarantee compliance. Providers of high-risk AI systems must comply with a range of obligations, including risk management processes, data governance, documentation, automatic logging, transparency, human oversight, robustness, accuracy, cybersecurity, and maintaining a Quality Management System. They must also keep documentation and logs for specified periods and take corrective actions if non-conformities are found. Compliance involves fulfilling all these requirements and obligations, not just identifying an AI system as high-risk.","The AI Act takes a “risk-based” approach, scaling the stringency of
requirements based on the inherent risk of an AI system. You can learn more
about how this approach differs from other frameworks and standards in our
[previous blog post on the state of AI
regulation.](https://www.citadel.co.jp/en/blog/2023/03/20/an-overview-of-ai-
standardization-and-regulation-in-2023/)

As explained below, AI systems categorized as __ high-risk will feel the most
impact from this regulation. Developers of such high-risk AI systems must go
through a **third-party conformity assessment** in order to **obtain a CE mark
and access the EU market**.

## Do the requirements differ for developers and users of AI systems?

The most recent version of the AI Act [from Feb
2](https://artificialintelligenceact.eu/wp-content/uploads/2024/01/AI-Act-
FullText.pdf), approved by the Committee of Permanent Representatives,
provides additional information on the
[roles](https://artificialintelligenceact.eu/article/3) and [their
responsibilities](https://artificialintelligenceact.eu/article/28/) across the
AI Value Chain. The law defines six key roles:

  1.  **Provider**. An entity (e.g. company) that develops an AI system and “places them on the market or puts the system into service under its own name or trademark, whether for payment or free of charge”.
  2.  **Deployer**. An entity that uses an AI system under its authority, except in personal non-professional activities. Note that this does not refer to the affected end users.
  3.  **Importer**. An EU entity that places an non-EU AI system on the EU market.
  4.  **Distributor**. An entity in the supply chain, other than the provider or the importer, that makes an AI system available in the EU market.
  5.  **Authorized Representatives**. An EU entity that carries out obligations on behalf of a Provider.
  6.  **Operator.** An entity that falls into any of the five categories above.

> Operators could act in more than one role at the same time and should
> therefore fulfill cumulatively all relevant obligations associated with
> those roles. For example, an operator could act as a distributor and an
> importer at the same time.
>
> [Recital 56a](https://artificialintelligenceact.eu/recital/56a)

 **Providers** bear the majority of responsibilities under the AI Act.
Providers of high-risk AI systems will need to comply with [Article
16](https://artificialintelligenceact.eu/article/16/), and by extension:

  * [Article 9](https://artificialintelligenceact.eu/article/9). Implementing risk management processes.
  * [Article 10](https://artificialintelligenceact.eu/article/10). Data and Data Governance. Using high-quality training, validation and testing data.
  * [Article 11](https://artificialintelligenceact.eu/article/11). Establishing documentation as defined by [Annex IV](https://artificialintelligenceact.eu/annex/4).
  * [Article 12](https://artificialintelligenceact.eu/article/12). Implementing automatic logging.
  * [Article 13](https://artificialintelligenceact.eu/article/13). Ensuring an appropriate level of transparency with Deployers.
  * [Article 14](https://artificialintelligenceact.eu/article/14). Ensuring human oversight measures.
  * [Article 15](https://artificialintelligenceact.eu/article/15). Ensuring robustness, accuracy and cybersecurity.
  * [Article 17](https://artificialintelligenceact.eu/article/17). Setting up a Quality Management System (e.g. [based on ISO 42001](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-you/))
  * [Article 18](https://artificialintelligenceact.eu/article/18). Keeping documentation for 10 years at the disposal of the national competent authorities.
  * [Article 20](https://artificialintelligenceact.eu/article/20/). Storing the automatically generated logs for at least 6 months.
  * [Article 21](https://artificialintelligenceact.eu/article/21/). Corrective actions and Duty of information when the system is not in conformity.
  * [Article 23](https://artificialintelligenceact.eu/article/23). Providing the necessary information to competent authorities.
  * [Article 51](https://artificialintelligenceact.eu/article/51).
It is uncertain what share of AI systems will be in the high-risk category, as
both the field of AI and the law are still evolving. The European Commission
estimated in an [impact study](https://digital-
strategy.ec.europa.eu/en/library/impact-assessment-regulation-artificial-
intelligence) that only 5-15% of applications would be subject to stricter
rules. A [study](https://aai.frb.io/assets/files/AI-Act-Risk-Classification-
Study-appliedAI-March-2023.pdf) by appliedAI of 106 enterprise AI systems
found that 18% were high-risk, 42% low-risk, and 40% unclear if high or low
risk. The system with unclear risk classification in this study were mainly
within the areas of critical infrastructure, employment, law enforcement, and
product safety. In a related [survey](https://aai.frb.io/assets/files/AI-Act-
Impact-Survey_Report_Dec12.2022.pdf) of 113 EU AI startups, 33% of the
startups surveyed believed that their AI systems would be classified as high-
risk, compared to the 5-15% assessed by the European Commission.



#### What are the penalties of the EU AI Act?

Based on the European Parliament’s adopted position, using prohibited AI
practices outlined in Article 5 can result in fines of up to €40 million, or
7% of worldwide annual turnover – whichever is higher. Not complying with the
data governance requirements (under Article 10) and the requirements for
transparency and provision of information (under Articles 13) can lead to
fines up to €20 million, or 4% of worldwide turnover if that is higher. Non-
compliance with any other requirements or obligations can result in fines of
up to €10 million or 2% of worldwide turnover – again, whichever is higher.
Fines are tiered depending on the provision violated, with prohibited
practices receiving the highest penalties and other violations receiving lower
maximum fines.



#### What measures does the EU AI Act implement for SMEs specifically?

The EU AI Act aims to provide support for SMEs and startups as specified in
Article 55. Specific measures include granting priority access for SMEs and
EU-based startups to regulatory sandboxes if eligibility criteria are met.
Additionally, tailored awareness raising and digital skills development
activities will be organised to address the needs of smaller organisations.
Moreover, dedicated communication channels will be set up to offer guidance
and respond to queries from SMEs and startups. Participation of SMEs and other
stakeholders in the standards development process will also be encouraged. To
reduce the financial burden of compliance, conformity assessment fees will be
lowered for SMEs and startups based on factors like development stage, size,
and market demand. The Commission will regularly review certification and
compliance costs for SMEs/startups (with input from transparent consultations)
and work with Member States to reduce these costs where possible.



#### Can I voluntarily comply with the EU AI Act even if my system is not in
scope?

Yes! The Commission, the AI Office, and/or Member States will encourage
voluntary codes of conduct for requirements under Title III, Chapter 2 (e.g.
risk management, data governance, and human oversight) for AI systems not
deemed to be high-risk. These codes of conduct will provide technical
solutions for how an AI system can meet the requirements, according to the
system’s intended purpose. Other objectives like environmental sustainability,
accessibility, stakeholder participation, and diversity of development teams
will be considered. Small businesses and startups will be taken into account
when encouraging codes of conduct. See Article 69 on Codes of Conduct for
Voluntary Application of Specific Requirements.



## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"The exact number of AI incidents that must be reported before an AI system is re-evaluated under the EU AI Act is not specified in the provided context information. The context indicates that for high-risk AI systems, the notification of serious incidents should be made to the national competent authority where the incident occurred, but it does not state a specific number of incidents that triggers a re-evaluation.","3a. For high-risk AI systems which are safety components of devices, or are
themselves devices, covered by Regulation (EU) 2017/745 and Regulation (EU)
2017/746 the notification of serious incidents shall be limited to those
referred to in [Article
3](https://artificialintelligenceact.eu/article/3/)(44)(c) and be made to the
national competent authority chosen for this purpose by the Member States
where that incident occurred.

3a. National competent authorities shall immediately notify the Commission of
any serious incident, whether or not it has taken action on it, in accordance
with Article 20 of Regulation 2019/1020.

[ <- Previous ](https://artificialintelligenceact.eu/article/72/) [ Next ->
](https://artificialintelligenceact.eu/article/74/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
It is uncertain what share of AI systems will be in the high-risk category, as
both the field of AI and the law are still evolving. The European Commission
estimated in an [impact study](https://digital-
strategy.ec.europa.eu/en/library/impact-assessment-regulation-artificial-
intelligence) that only 5-15% of applications would be subject to stricter
rules. A [study](https://aai.frb.io/assets/files/AI-Act-Risk-Classification-
Study-appliedAI-March-2023.pdf) by appliedAI of 106 enterprise AI systems
found that 18% were high-risk, 42% low-risk, and 40% unclear if high or low
risk. The system with unclear risk classification in this study were mainly
within the areas of critical infrastructure, employment, law enforcement, and
product safety. In a related [survey](https://aai.frb.io/assets/files/AI-Act-
Impact-Survey_Report_Dec12.2022.pdf) of 113 EU AI startups, 33% of the
startups surveyed believed that their AI systems would be classified as high-
risk, compared to the 5-15% assessed by the European Commission.



#### What are the penalties of the EU AI Act?

Based on the European Parliament’s adopted position, using prohibited AI
practices outlined in Article 5 can result in fines of up to €40 million, or
7% of worldwide annual turnover – whichever is higher. Not complying with the
data governance requirements (under Article 10) and the requirements for
transparency and provision of information (under Articles 13) can lead to
fines up to €20 million, or 4% of worldwide turnover if that is higher. Non-
compliance with any other requirements or obligations can result in fines of
up to €10 million or 2% of worldwide turnover – again, whichever is higher.
Fines are tiered depending on the provision violated, with prohibited
practices receiving the highest penalties and other violations receiving lower
maximum fines.



#### What measures does the EU AI Act implement for SMEs specifically?

The EU AI Act aims to provide support for SMEs and startups as specified in
Article 55. Specific measures include granting priority access for SMEs and
EU-based startups to regulatory sandboxes if eligibility criteria are met.
Additionally, tailored awareness raising and digital skills development
activities will be organised to address the needs of smaller organisations.
Moreover, dedicated communication channels will be set up to offer guidance
and respond to queries from SMEs and startups. Participation of SMEs and other
stakeholders in the standards development process will also be encouraged. To
reduce the financial burden of compliance, conformity assessment fees will be
lowered for SMEs and startups based on factors like development stage, size,
and market demand. The Commission will regularly review certification and
compliance costs for SMEs/startups (with input from transparent consultations)
and work with Member States to reduce these costs where possible.



#### Can I voluntarily comply with the EU AI Act even if my system is not in
scope?

Yes! The Commission, the AI Office, and/or Member States will encourage
voluntary codes of conduct for requirements under Title III, Chapter 2 (e.g.
risk management, data governance, and human oversight) for AI systems not
deemed to be high-risk. These codes of conduct will provide technical
solutions for how an AI system can meet the requirements, according to the
system’s intended purpose. Other objectives like environmental sustainability,
accessibility, stakeholder participation, and diversity of development teams
will be considered. Small businesses and startups will be taken into account
when encouraging codes of conduct. See Article 69 on Codes of Conduct for
Voluntary Application of Specific Requirements.



## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"The provided context does not specify quantitative thresholds for data bias that require action under the EU AI Act. Instead, it outlines conditions under which special categories of personal data may be processed for the purpose of bias detection and correction in high-risk AI systems. These conditions include the necessity of processing such data for effective bias correction when it cannot be achieved by other means, implementing state-of-the-art security and privacy measures, and ensuring data is deleted after the correction of bias or at the end of its retention period. The context also mentions the need for records of processing activities to include justification for processing special categories of personal data for bias detection and correction.","4\. Datasets shall take into account, to the extent required by the intended
purpose, the characteristics or elements that are particular to the specific
geographical, contextual, behavioural or functional setting within which the
high-risk AI system is intended to be used.

5\. To the extent that it is strictly necessary for the purposes of ensuring
bias detection and correction in relation to the high-risk AI systems in
accordance with the second paragraph, point f and fa, the providers of such
systems may exceptionally process special categories of personal data referred
to in Article 9(1) of Regulation (EU) 2016/679, Article 10 of Directive (EU)
2016/680 and Article 10(1) of Regulation (EU) 2018/1725, subject to
appropriate safeguards for the fundamental rights and freedoms of natural
persons. In addition to provisions set out in the Regulation (EU) 2016/679,
Directive (EU) 2016/680 and Regulation (EU) 2018/1725, all the following
conditions shall apply in order for such processing to occur:

(a) the bias detection and correction cannot be effectively fulfilled by
processing other data, including synthetic or anonymised data;

(b) the special categories of personal data processed for the purpose of this
paragraph are subject to technical limitations on the re-use of the personal
data and state of the art security and privacy-preserving measures, including
pseudonymisation;

(c) the special categories of personal data processed for the purpose of this
paragraph are subject to measures to ensure that the personal data processed
are secured, protected, subject to suitable safeguards, including strict
controls and documentation of the access, to avoid misuse and ensure only
authorised persons have access to those personal data with appropriate
confidentiality obligations;

(d) the special categories of personal data processed for the purpose of this
paragraph are not to be transmitted, transferred or otherwise accessed by
other parties;

(e) the special categories of personal data processed for the purpose of this
paragraph are deleted once the bias has been corrected or the personal data
has reached the end of its retention period, whatever comes first;

(f) the records of processing activities pursuant to Regulation (EU) 2016/679,
Directive (EU) 2016/680 and Regulation (EU) 2018/1725 includes justification
why the processing of special categories of personal data was strictly
necessary to detect and correct biases and this objective could not be
achieved by processing other data.

6\. For the development of high-risk AI systems not using techniques involving
the training of models, paragraphs 2 to 5 shall apply only to the testing data
sets.

[ <- Previous ](https://artificialintelligenceact.eu/article/9/) [ Next ->
](https://artificialintelligenceact.eu/article/11/)

#### Suitable Recitals

[

Recital 66

](https://artificialintelligenceact.eu/recital/66/) [

Recital 67

](https://artificialintelligenceact.eu/recital/67/) [

Recital 69

](https://artificialintelligenceact.eu/recital/69/) [

Recital 71

](https://artificialintelligenceact.eu/recital/71/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
According to [Article 52c](https://artificialintelligenceact.eu/article/52c),
Providers of GPAI models must:

  * Draw up technical documentation, including training and testing process and evaluation results.
  * Enable other Providers that intend to integrate the GPAI model into their own AI system with sufficient documentation and information.
  * Put in place a policy to respect the EU copyright law.
  * Based on a template provided by the AI Office, publish a sufficiently detailed summary about the content used for training.
  * Develop and adhere to a code of practice

One of the reasons for the recent delays in the AI Act’s development was a
debate on the two-tiered categorization for GPAI models. Currently, models
that were trained using a total computing power of more than [10^25
FLOPs](https://artificialintelligenceact.eu/article/52a) are considered to
carry “ **systemic risk** ”. This places most models on the market, excluding
**GPT-4 and potentially Gemini Ultra** , in the non-systemic risk subcategory.
However, the threshold may be updated in future by the European Commission and
the AI Office as the technology progresses.

Providers of GPAI models with systemic risk are subject [to additional
obligations](https://artificialintelligenceact.eu/article/52d) to:

  * Perform model evaluations, including adversarial testing
  * Assess and mitigate possible systemic risks
  * Document and report issues to the AI office and relevant authorities

* * *

However, there are also major **exemptions** for Providers of AI systems for
research and academic use, as well as for AI models (including GPAI) that are
free and open-source. These models are not subject to any obligations until
they are placed on the market by a Provider as part of an AI system that has
obligations under the AI Act. Military and law enforcement uses are excluded
from the majority of obligations, but have some special considerations that we
will not cover.

You can learn more details about each risk category in the [official
FAQ](https://ec.europa.eu/commission/presscorner/detail/en/QANDA_21_1683)
published by the European Commission.

## What are the consequences of not complying with the EU AI Act?

As defined in [Article 71](https://artificialintelligenceact.eu/article/71/),
administrative fines act as the main penalty. The actual fine amount will be
decided on a case-by-case basis depending on the severity of the infringement
(based on rules to be further developed by individual EU Member states), but
the AI Act does outline maximum penalties for different types of non-
compliance:

  *  _Prohibited practices or non-compliance related to requirements on data:_ up to €35m or 7% of the total worldwide annual turnover
  *  _Non-compliance with any of the other requirements:_ up to €15m or 3% of the total worldwide annual turnover
  *  _Incorrect, incomplete or misleading information to notified bodies and national competent authorities:_ up to €7.5m or 1.5% of the total worldwide annual turnover

For each category of infringement, the threshold would be the lower of the two
amounts for small and medium-sized enterprises (including startups) and the
higher for other companies.

## What is the current legislative status and timeline?

The EU AI Act is not yet law, but it’s on a fast track to be published in the
Official Journal in early-mid 2024, and most of its requirements will
gradually be enforced in the following 24 months.

The[ timeline for
enforcement](https://artificialintelligenceact.eu/article/85) is as follows:

  1. After 6 months: Prohibition of AI applications of unacceptable risk becomes applicable.
  2. After 9 months: [Codes of practice](https://artificialintelligenceact.eu/recital/60t/) for GPAI providers must be prepared.
  3. After 12 months (approximately 2025 Q2~): Provisions on GPAI become applicable.
  4. After 24 months (approximately 2026 Q2~): Obligations for high-risk AI systems under Annex III become applicable. Each EU Member State is required to establish or participate in at least one regulatory sandbox.
  5. After 36 months: Obligations for high-risk AI systems under Annex II become applicable.

In practical terms, this means that the next two years will see a flurry of
activity in the field of AI auditing, both in government and in the private
sector. Relevant offices and systems will need to be set up in EU member
states; standards-making and certification bodies will be under extra pressure
to fill the gap and provide clear assessment frameworks."
"The context information provided does not contain an explicit mention or definition of the acronym ""RMF"" in relation to NIST AI. Therefore, based on the information given, it is not possible to determine what ""RMF"" stands for in the context of NIST AI RMF.","40 
 外部への説明という目的のため、 米国国立標準技術研究所 （ NIST ） の文書23を参考にし、
可能な限り正確かつ相手に理解できる 記録とし、説明の限界を意識できるよう努めてい
る。 
 
【実践例２】  
 当社は AIシステムを開発する小規模企業である。 技術担当役員 は全てのプロジェクト
を把握しており、自らプログラミングしたり論文を読解したりするなど AIに大変詳し
く、 AI倫理の問題についても強い関心を持っている。そのため当社では、部門間の専門
性のギャップが問題 になることはないと考えている。他方で、 プロジェクトに関わる人
たちの専門性が高いために、 いちいち確認しなくても 、行動目標ができているであろう
と思い込みがち である。そのため、 プロジェクトの進捗報告のレポートに 乖離評価 チェ
ックリストを添付し、 技術担当役員が必要に応じて聞き取りできるように工夫 している。  
また、当社は専門性が 高い集団であることから、世間の認識 とのずれが生じやすい傾
向があると分析している。 そのため、 運用状況を確認しつつ、 行動目標３－３－２にした
がって 日常的な情報収集や意見交換から得られた状況を定期的に共有する ことで、社会
的受容に意識を向けるようにしている。  
 
  
 
23 P J. Phillips, Amanda C. Hahn, Peter C. Fontana, David A. Broniatowski, Mark A. Przybocki, “Four 
Principles of Explainable Artificial Intelligence (Draft), NIST Interagency/Internal Report (NISTIR) - 8312 -
draft” (August 19, 2020)
Machine Learning Quality Management Guideline   National Institute of  
3rd English edition   Advanced Industrial Science and Technology  
 
227 
 via Randomized Smoothing. The 36th International Confer ence on Machine Learning 
(ICML 2019) , 2019.  
[81] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, 
Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. RobustBench: A 
Standardized Adversarial Ro bustness Benchmark. 2021. 
http://arxiv.org/abs/2010.09670, https://robustbench.github.io  
[82] George Deckert , NASA Hazard Analysis Process. Johnson Space Center, National 
Aeronautics and Space Administration, 2010. 
https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/201000406 78.pdf . 
[83] Ann- Kathrin Dombrowski, Maximilian Alber, Christopher J. Anders, Marcel Ackermann, 
Klaus -Robert Mu ̈ ller, and Pan Kessel. Explanations can be manipulated and geometry is 
to blame. In Proceedings of the Annual Conference on Neural Information Processing 
Systems 2019 (NeurIPS’19) , pp. 13567 –13578, 2019. 
https://arxiv.org/pdf/1906.07983.pdf  
[84] Yizhen Dong, Peixin Zhang, Jingyi Wang, Shuang Liu, Jun Sun, Jianye Hao, Xinyu Wang, Li Wang, Jin Song Dong, and Dai Ting. There is Limited Correlation between Coverage and Robustness for Deep Neural Networks. arXiv:1911.05904 [cs.LG]. 
https://arxiv.org/abs/1911.05904
 
[85] Cynthia Dwork, Differential Privacy, In Proc. ICALP’06 , pp.1 -12, 2006.  
[86] Cynthia Dwork and Aaron Roth, The Algorithmic Foundations of Differential Privacy, Foundations and Trends in Theoretical Computer Science , 9(3 -4), pp.211 -407, 2014.  
[87] S. Elbaum and D. S. Rosenblum. Known Unknowns – Testing in the Presence of 
Uncertainty, In P roceedings of the 22nd ACM SIGSOFT International Symposium on 
Foundations of Software Engineering  (FSE 20 14), pp. 833 –836, 2014.  
[88] Arisa Ema (ed.). Small Featured Articles “From AI principles to implementation: 
introduction of international activities” . Artificial Intelligence 36(2), The Japanese 
Society for Artificial Intelligence, March 2021.. In Japanese  
[89] Ulfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova, RAPPOR: Randomized 
Aggregatable Privacy -Preserving Ordinal Response, arXiv:1407.6981v2, 2014.  
[90] Itay P . Fainmesser, Andrea Galeotti, and Ruslan Momot, Digital Privacy, HEC Paris Research Paper, 2021.  
[91] E. R. Faria, J. Gama, and A. C. Carvalho. Novelty detection algorithm for data streams 
multi class problems, In Procedings of the 28th annual ACM symposium on applied 
computing , pp. 795 –800, 2013.  
[92] Y. Feng, Q. Shi, X. Gao, J. Wan, C. Fang, and Z. Chen, DeepGini: Prioritizing Massive Tests 
to Enhance the Robustness of Deep Neural Networks, Proc. 29th ISSTA, pp. 177 -188, 
and arXiv:1903.00661v2, 2020.  
[93] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model Inversion Attacks that"
"The primary goal of the NIST AI RMF (Risk Management Framework) is to ensure that high-risk AI systems are designed and developed with an appropriate level of accuracy, robustness, and cybersecurity, and that they maintain these qualities throughout their lifecycle. This includes making AI systems resilient to errors, faults, inconsistencies, and unauthorized attempts to alter their use or performance. The framework also emphasizes the need for AI systems to be developed with measures to mitigate risks such as biased outputs and feedback loops, and to address AI-specific vulnerabilities like data poisoning, model poisoning, adversarial examples, and confidentiality attacks. Additionally, the framework calls for regular review and updating of the risk management system to ensure its effectiveness and to identify and mitigate risks to health, safety, and fundamental rights.","Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

1\. High-risk AI systems shall be designed and developed in such a way that
they achieve an appropriate level of accuracy, robustness, and cybersecurity,
and perform consistently in those respects throughout their lifecycle.

1a. To address the technical aspects of how to measure the appropriate levels
of accuracy and robustness set out in paragraph 1 of this Article and any
other relevant performance metrics, the Commission shall, in cooperation with
relevant stakeholder and organisations such as metrology and benchmarking
authorities, encourage as appropriate, the development of benchmarks and
measurement methodologies.

2\. The levels of accuracy and the relevant accuracy metrics of high-risk AI
systems shall be declared in the accompanying instructions of use.

3\. High-risk AI systems shall be as resilient as possible regarding errors,
faults or inconsistencies that may occur within the system or the environment
in which the system operates, in particular due to their interaction with
natural persons or other systems. Technical and organisational measures shall
be taken towards this regard. The robustness of high-risk AI systems may be
achieved through technical redundancy solutions, which may include backup or
fail-safe plans. High-risk AI systems that continue to learn after being
placed on the market or put into service shall be developed in such a way to
eliminate or reduce as far as possible the risk of possibly biased outputs
influencing input for future operations (‘feedback loops’) are duly addressed
with appropriate mitigation measures.

4\. High-risk AI systems shall be resilient as regards to attempts by
unauthorised third parties to alter their use, outputs or performance by
exploiting the system vulnerabilities. The technical solutions aimed at
ensuring the cybersecurity of high-risk AI systems shall be appropriate to the
relevant circumstances and the risks. The technical solutions to address AI
specific vulnerabilities shall include, where appropriate, measures to
prevent, detect, respond to, resolve and control for attacks trying to
manipulate the training dataset (‘data poisoning’), or pre-trained components
used in training (‘model poisoning’) , inputs designed to cause the model to
make a mistake (‘adversarial examples’ or ‘model evasion’), confidentiality
attacks or model flaws.

[ <- Previous ](https://artificialintelligenceact.eu/article/14/) [ Next ->
](https://artificialintelligenceact.eu/article/16/)

#### Suitable Recitals

[

Recital 66

](https://artificialintelligenceact.eu/recital/66/) [

Recital 74

](https://artificialintelligenceact.eu/recital/74/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
This process should be aimed at identifying and mitigating the
relevant risks of artificial intelligence systems on health, safety and
fundamental rights. The risk management system should be regularly reviewed
and updated to ensure its continuing effectiveness, as well as justification
and documentation of any significant decisions and actions taken subject to
this Regulation. This process should ensure that the provider identifies risks
or adverse impacts and implements mitigation measures for the known and
reasonably foreseeable risks of artificial intelligence systems to the health,
safety and fundamental rights in light of its intended purpose and reasonably
foreseeable misuse, including the possible risks arising from the interaction
between the AI system and the environment within which it operates. The risk
management system should adopt the most appropriate risk management measures
in the light of the state of the art in AI. When identifyingthe most
appropriate risk management measures, the provider should document and explain
the choices made and, when relevant, involve experts and external
stakeholders. In identifying reasonably foreseeable misuse of high risk AI
systems the provider should cover uses of the AI systems which, while not
directly covered by the intended purpose and provided for in the instruction
for use may nevertheless be reasonably expected to result from readily
predictable human behaviour in the context of the specific characteristics and
use of the particular AI system. Any known or foreseeable circumstances,
related to the use of the high-risk AI system in accordance with its intended
purpose or under conditions of reasonably foreseeable misuse, which may lead
to risks to the health and safety or fundamental rights should be included in
the instructions for use provided by the provider. This is to ensure that the
deployer is aware and takes them into account when using the high-risk AI
system. Identifying and implementing risk mitigation measures for foreseeable
misuse under this Regulation should not require specific additional training
measures for the high-risk AI system by the provider to address them. The
providers however are encouraged to consider such additional training measures
to mitigate reasonable foreseeable misuses as necessary and appropriate.

[ <- Previous ](https://artificialintelligenceact.eu/recital/64/) [ Next ->
](https://artificialintelligenceact.eu/recital/66/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"The provided context information does not explicitly mention a tiered approach to managing AI risks in relation to the NIST AI RMF. It discusses the commitment of organizations to assess and adopt mitigation measures for various risks associated with advanced AI systems, including systemic risks, and to advance research and investment in several areas such as security, safety, and transparency. It also outlines the need for organizations to identify and mitigate vulnerabilities, report on AI systems' capabilities and limitations, share information responsibly, and develop AI governance and risk management policies based on a risk-based approach. However, it does not detail a tiered approach.","3 
measures, organizations commit to devote attention to the following risks as appropriate:  
 
> Chemical, biological, radio logical, and nuclear risks, such as the ways in which advanced AI systems 
can lower barriers to entry, including for non -state actors, for weapons development, design 
acquisition, or use.  
 > Offensive cyber capabilities, such as the ways in which systems c an enable vulnerability discovery, 
exploitation, or operational use, bearing in mind that such capabilities could also have useful defensive applications and might be appropriate to include in a system.  
 > Risks to health and/or Safety, including the effec ts of system interaction and tool use, including for 
example the capacity to control physical systems and interfere with critical infrastructure.  
 >  Risks from models of making copies of themselves or “ self-replicating ” or training other models.  
 >  Socie tal risks, as well as risks to individuals and communities such as the ways in which advanced 
AI systems or models can give rise to  harmful bias and discrimination or lead to violation of 
applicable legal frameworks, including on privacy and data protectio n. 
 >  Threats to democratic values and human rights, including the facilitation of disinformation or 
harming privacy.  
 > Risk that a particular event could lead to a chain reaction with considerable negative effects that 
could affect up to an entire city,  an entire domain activity or an entire community.  
 Organizations commit to work in collaboration with relevant actors across sectors, to assess and adopt mitigation measures to address these risks, in particular systemic risks.  
 Organizations making these commitments should also endeavor to advance research and investment on the security, safety, bias and disinformation, fairness, explainability and interpretability, and transparency of advanced AI systems and on increasing robust ness and 
trustworthiness of advanced AI systems against misuse.  
  
2 Identify and mitigate vulnerabilities, and, where appropriate, incidents and patterns of 
misuse, after deployment including placement on the market.  
 Organizations should use, as and when appropriate commensurate to the level of risk, AI systems as intended and monitor for vulnerabilities, incidents, emerging risks and misuse after deployment,
* Risk that a particular event could lead to a chain reaction with considerable negative effects that could affect up to an entire city, an entire domain activity or an entire community.

Organizations commit to work in collaboration with relevant actors across
sectors, to assess and adopt mitigation measures to address these risks, in
particular systemic risks.

Organizations making these commitments should also endeavor to advance
research and investment on the security, safety, bias and disinformation,
fairness, explainability and interpretability, and transparency of advanced AI
systems and on increasing robustness and trustworthiness of advanced AI
systems against misuse.

2 Identify and mitigate vulnerabilities, and, where appropriate, incidents and
patterns of misuse, after deployment including placement on the market.

Organizations should use, as and when appropriate commensurate to the level of
risk, AI systems as intended and monitor for vulnerabilities, incidents,
emerging risks and misuse after deployment, and take appropriate action to
address these. Organizations are encouraged to consider, for example,
facilitating third-party and user discovery and reporting of issues and
vulnerabilities after deployment such as through bounty systems, contests, or
prizes to incentivize the responsible disclosure of weaknesses. Organizations
are further encouraged to maintain appropriate documentation of reported
incidents and to mitigate the identified risks and vulnerabilities, in
collaboration with other stakeholders. Mechanisms to report vulnerabilities,
where appropriate, should be accessible to a diverse set of stakeholders.

3 Publicly report advanced AI systemsâ capabilities, limitations and domains
of appropriate and inappropriate use, to support ensuring sufficient
transparency, thereby contributing to increase accountability.

This should include publishing transparency reports containing meaningful
information for all new significant releases of advanced AI systems.  
These reports, instruction for use and relevant technical documentation, as
appropriate as, should be kept up-to-date and should include, for example;

  * Details of the evaluations conducted for potential safety, security, and societal risks, as well as risks to human rights,
  * Capacities of a model/system and significant limitations in performance that have implications for the domains of appropriate use,
  * Discussion and assessment of the modelâs or systemâs effects and risks to safety and society such as harmful bias, discrimination, threats to protection of privacy or personal data, and effects on fairness, and 
  * The results of red-teaming conducted to evaluate the modelâs/systemâs fitness for moving beyond the development stage.

Organizations should make the information in the transparency reports
sufficiently clear and understandable to enable deployers and users as
appropriate and relevant to interpret the model/systemâs output and to
enable users to use it appropriately; and that transparency reporting should
be supported and informed by robust documentation processes such as technical
documentation and instructions for use.

4 Work towards responsible information sharing and reporting of incidents
among organizations developing advanced AI systems including with industry,
governments, civil society, and academia

This includes responsibly sharing information, as appropriate, including, but
not limited to evaluation reports, information on security and safety risks,
dangerous intended or unintended capabilities, and attempts by AI actors to
circumvent safeguards across the AI lifecycle.

Organizations should establish or join mechanisms to develop, advance, and
adopt, where appropriate, shared standards, tools, mechanisms, and best
practices for ensuring the safety, security, and trustworthiness of advanced
AI systems.

This should also include ensuring appropriate and relevant documentation and
transparency across the AI lifecycle in particular for advanced AI systems
that cause significant risks to safety and society.

Organizations should collaborate with other organizations across the AI
lifecycle to share and report relevant information to the public with a view
to advancing safety, security and trustworthiness of advanced AI systems.
Organizations should also collaborate and share the aforementioned information
with relevant public authorities, as appropriate.

Such reporting should safeguard intellectual property rights.

5 Develop, implement and disclose AI governance and risk management policies,
grounded in a risk-based approach â including privacy policies, and
mitigation measures.

Organizations should put in place appropriate organizational mechanisms to
develop, disclose and implement risk management and governance policies,
including for example accountability and governance processes to identify,
assess, prevent, and address risks, where feasible throughout the AI
lifecycle.

This includes disclosing where appropriate privacy policies, including for
personal data, user prompts and advanced AI system outputs. Organizations are
expected to establish and disclose their AI governance policies and
organizational mechanisms to implement these policies in accordance with a
risk-based approach. This should include accountability and governance
processes to evaluate and mitigate risks, where feasible throughout the AI
lifecycle.

The risk management policies should be developed in accordance with a risk-
based approach and apply a risk management framework across the AI lifecycle
as appropriate and relevant, to address the range of risks associated with AI
systems, and policies should also be regularly updated."
"The provided context information does not mention the NIST AI RMF (National Institute of Standards and Technology Artificial Intelligence Risk Management Framework) or any obligations for private sector companies to follow it when contracting with the federal government. The context pertains to the European Union's regulatory framework concerning AI, including the development of voluntary model contractual terms for high-risk AI systems and the protection of fundamental rights in relation to the use of such systems. Therefore, based on the information given, there is no indication of a requirement for private sector companies to follow the NIST AI RMF when contracting with the federal government.","This obligation shall not apply to third parties
making accessible to the public tools, services, processes, or AI components
other than general-purpose AI models under a free and open licence. The AI
Office may develop and recommend voluntary model contractual terms between
providers of high-risk AI systems and third parties that supply tools,
services, components or processes that are used or integrated in high-risk AI
systems. When developing voluntary model contractual terms, the AI Office
shall take into account possible contractual requirements applicable in
specific sectors or business cases. The model contractual terms shall be
published and be available free of charge in an easily usable electronic
format.

2b. Paragraphs 2 and 2a are without prejudice to the need to respect and
protect intellectual property rights and confidential business information or
trade secrets in accordance with Union and national law.

[ <- Previous ](https://artificialintelligenceact.eu/article/24/) [ Next ->
](https://artificialintelligenceact.eu/article/26/)

#### Suitable Recitals

[

Recital 87

](https://artificialintelligenceact.eu/recital/87/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

3\. National public authorities or bodies which supervise or enforce the
respect of obligations under Union law protecting fundamental rights,
including the right to non-discrimination, in relation to the use of high-risk
AI systems referred to in [Annex
III](https://artificialintelligenceact.eu/annex/3/) shall have the power to
request and access any documentation created or maintained under this
Regulation in accesible language and format when access to that documentation
is necessary for effectively fulfilling their mandate within the limits of
their jurisdiction. The relevant public authority or body shall inform the
market surveillance authority of the Member State concerned of any such
request.

4\. By three months after the entering into force of this Regulation, each
Member State shall identify the public authorities or bodies referred to in
paragraph 3 and make a list publicly available. Member States shall notify the
list to the Commission and all other Member States and keep the list up to
date.

5\. Where the documentation referred to in paragraph 3 is insufficient to
ascertain whether a breach of obligations under Union law intended to protect
fundamental rights has occurred, the public authority or body referred to in
paragraph 3 may make a reasoned request to the market surveillance authority,
to organise testing of the high-risk AI system through technical means. The
market surveillance authority shall organise the testing with the close
involvement of the requesting public authority or body within reasonable time
following the request.

6\. Any information and documentation obtained by the national public
authorities or bodies referred to in paragraph 3 pursuant to the provisions of
this Article shall be treated in compliance with the confidentiality
obligations set out in [Article
78](https://artificialintelligenceact.eu/article/78/).

[ <- Previous ](https://artificialintelligenceact.eu/article/76/) [ Next ->
](https://artificialintelligenceact.eu/article/79/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"The provided context does not explicitly mention the NIST AI RMF (National Institute of Standards and Technology Artificial Intelligence Risk Management Framework). However, it does outline a commitment by organizations to identify and categorize AI risks, including chemical, biological, radiological, and nuclear risks, offensive cyber capabilities, risks to health and safety, risks from self-replicating models, societal risks, threats to democratic values and human rights, and systemic risks that could lead to chain reactions with considerable negative effects. It also mentions the establishment of a methodology for classifying general-purpose AI models with systemic risks, which includes evaluating high-impact capabilities and the impact on the internal market. The context suggests a systematic approach to risk assessment and mitigation, including after deployment, and indicates that organizations should work collaboratively to address these risks. While this approach seems structured, it also implies a degree of flexibility in how risks are identified and mitigated, as it allows for the use of appropriate technical tools and methodologies that can be adjusted over time to reflect technological and industrial changes.","3 
measures, organizations commit to devote attention to the following risks as appropriate:  
 
> Chemical, biological, radio logical, and nuclear risks, such as the ways in which advanced AI systems 
can lower barriers to entry, including for non -state actors, for weapons development, design 
acquisition, or use.  
 > Offensive cyber capabilities, such as the ways in which systems c an enable vulnerability discovery, 
exploitation, or operational use, bearing in mind that such capabilities could also have useful defensive applications and might be appropriate to include in a system.  
 > Risks to health and/or Safety, including the effec ts of system interaction and tool use, including for 
example the capacity to control physical systems and interfere with critical infrastructure.  
 >  Risks from models of making copies of themselves or “ self-replicating ” or training other models.  
 >  Socie tal risks, as well as risks to individuals and communities such as the ways in which advanced 
AI systems or models can give rise to  harmful bias and discrimination or lead to violation of 
applicable legal frameworks, including on privacy and data protectio n. 
 >  Threats to democratic values and human rights, including the facilitation of disinformation or 
harming privacy.  
 > Risk that a particular event could lead to a chain reaction with considerable negative effects that 
could affect up to an entire city,  an entire domain activity or an entire community.  
 Organizations commit to work in collaboration with relevant actors across sectors, to assess and adopt mitigation measures to address these risks, in particular systemic risks.  
 Organizations making these commitments should also endeavor to advance research and investment on the security, safety, bias and disinformation, fairness, explainability and interpretability, and transparency of advanced AI systems and on increasing robust ness and 
trustworthiness of advanced AI systems against misuse.  
  
2 Identify and mitigate vulnerabilities, and, where appropriate, incidents and patterns of 
misuse, after deployment including placement on the market.  
 Organizations should use, as and when appropriate commensurate to the level of risk, AI systems as intended and monitor for vulnerabilities, incidents, emerging risks and misuse after deployment,
#### recital

# Recital 111

**Feedback** – We are working to improve this tool. Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

It is appropriate to establish a methodology for the classification of
general-purpose AI models as general-purpose AI model with systemic risks.
Since systemic risks result from particularly high capabilities, a general-
purpose AI models should be considered to present systemic risks if it has
high-impact capabilities, evaluated on the basis of appropriate technical
tools and methodologies, or significant impact on the internal market due to
its reach. High-impact capabilities in general purpose AI models means
capabilities that match or exceed the capabilities recorded in the most
advanced general-purpose AI models. The full range of capabilities in a model
could be better understood after its release on the market or when users
interact with the model. According to the state of the art at the time of
entry into force of this Regulation, the cumulative amount of compute used for
the training of the general-purpose AI model measured in floating point
operations (FLOPs) is one of the relevant approximations for model
capabilities. The amount of compute used for training cumulates the compute
used across the activities and methods that are intended to enhance the
capabilities of the model prior to deployment, such as pre-training, synthetic
data generation and fine-tuning. Therefore, an initial threshold of FLOPs
should be set, which, if met by a general-purpose AI model, leads to a
presumption that the model is a general-purpose AI model with systemic risks.
This threshold should be adjusted over time to reflect technological and
industrial changes, such as algorithmic improvements or increased hardware
efficiency, and should be supplemented with benchmarks and indicators for
model capability. To inform this, the AI Office should engage with the
scientific community, industry, civil society and other experts. Thresholds,
as well as tools and benchmarks for the assessment of high-impact
capabilities, should be strong predictors of generality, its capabilities and
associated systemic risk of general-purpose AI models, and could take into
taking into account the way the model will be placed on the market or the
number of users it may affect. To complement this system, there should be a
possibility for the Commission to take individual decisions designating a
general-purpose AI model as a general-purpose AI model with systemic risk if
it is found that such model has capabilities or impact equivalent to those
captured by the set threshold. This decision should be taken on the basis of
an overall assessment of the criteria set out in [Annex
XIII](https://artificialintelligenceact.eu/annex/13/), such as quality or size
of the training data set, number of business and end users, its input and
output modalities, its degree of autonomy and scalability, or the tools it has
access to. Upon a reasoned request of a provider whose model has been
designated as a general-purpose AI model with systemic risk, the Commission
should take the request into account and may decide to reassess whether the
general-purpose AI model can still be considered to present systemic risks.

[ <- Previous ](https://artificialintelligenceact.eu/recital/110/) [ Next ->
](https://artificialintelligenceact.eu/recital/112/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
The exact number of AI risk categories defined by the NIST AI RMF is not provided in the context information.,"It is uncertain what share of AI systems will be in the high-risk category, as
both the field of AI and the law are still evolving. The European Commission
estimated in an [impact study](https://digital-
strategy.ec.europa.eu/en/library/impact-assessment-regulation-artificial-
intelligence) that only 5-15% of applications would be subject to stricter
rules. A [study](https://aai.frb.io/assets/files/AI-Act-Risk-Classification-
Study-appliedAI-March-2023.pdf) by appliedAI of 106 enterprise AI systems
found that 18% were high-risk, 42% low-risk, and 40% unclear if high or low
risk. The system with unclear risk classification in this study were mainly
within the areas of critical infrastructure, employment, law enforcement, and
product safety. In a related [survey](https://aai.frb.io/assets/files/AI-Act-
Impact-Survey_Report_Dec12.2022.pdf) of 113 EU AI startups, 33% of the
startups surveyed believed that their AI systems would be classified as high-
risk, compared to the 5-15% assessed by the European Commission.



#### What are the penalties of the EU AI Act?

Based on the European Parliament’s adopted position, using prohibited AI
practices outlined in Article 5 can result in fines of up to €40 million, or
7% of worldwide annual turnover – whichever is higher. Not complying with the
data governance requirements (under Article 10) and the requirements for
transparency and provision of information (under Articles 13) can lead to
fines up to €20 million, or 4% of worldwide turnover if that is higher. Non-
compliance with any other requirements or obligations can result in fines of
up to €10 million or 2% of worldwide turnover – again, whichever is higher.
Fines are tiered depending on the provision violated, with prohibited
practices receiving the highest penalties and other violations receiving lower
maximum fines.



#### What measures does the EU AI Act implement for SMEs specifically?

The EU AI Act aims to provide support for SMEs and startups as specified in
Article 55. Specific measures include granting priority access for SMEs and
EU-based startups to regulatory sandboxes if eligibility criteria are met.
Additionally, tailored awareness raising and digital skills development
activities will be organised to address the needs of smaller organisations.
Moreover, dedicated communication channels will be set up to offer guidance
and respond to queries from SMEs and startups. Participation of SMEs and other
stakeholders in the standards development process will also be encouraged. To
reduce the financial burden of compliance, conformity assessment fees will be
lowered for SMEs and startups based on factors like development stage, size,
and market demand. The Commission will regularly review certification and
compliance costs for SMEs/startups (with input from transparent consultations)
and work with Member States to reduce these costs where possible.



#### Can I voluntarily comply with the EU AI Act even if my system is not in
scope?

Yes! The Commission, the AI Office, and/or Member States will encourage
voluntary codes of conduct for requirements under Title III, Chapter 2 (e.g.
risk management, data governance, and human oversight) for AI systems not
deemed to be high-risk. These codes of conduct will provide technical
solutions for how an AI system can meet the requirements, according to the
system’s intended purpose. Other objectives like environmental sustainability,
accessibility, stakeholder participation, and diversity of development teams
will be considered. Small businesses and startups will be taken into account
when encouraging codes of conduct. See Article 69 on Codes of Conduct for
Voluntary Application of Specific Requirements.



## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

1\. High-risk AI systems shall be designed and developed in such a way that
they achieve an appropriate level of accuracy, robustness, and cybersecurity,
and perform consistently in those respects throughout their lifecycle.

1a. To address the technical aspects of how to measure the appropriate levels
of accuracy and robustness set out in paragraph 1 of this Article and any
other relevant performance metrics, the Commission shall, in cooperation with
relevant stakeholder and organisations such as metrology and benchmarking
authorities, encourage as appropriate, the development of benchmarks and
measurement methodologies.

2\. The levels of accuracy and the relevant accuracy metrics of high-risk AI
systems shall be declared in the accompanying instructions of use.

3\. High-risk AI systems shall be as resilient as possible regarding errors,
faults or inconsistencies that may occur within the system or the environment
in which the system operates, in particular due to their interaction with
natural persons or other systems. Technical and organisational measures shall
be taken towards this regard. The robustness of high-risk AI systems may be
achieved through technical redundancy solutions, which may include backup or
fail-safe plans. High-risk AI systems that continue to learn after being
placed on the market or put into service shall be developed in such a way to
eliminate or reduce as far as possible the risk of possibly biased outputs
influencing input for future operations (‘feedback loops’) are duly addressed
with appropriate mitigation measures.

4\. High-risk AI systems shall be resilient as regards to attempts by
unauthorised third parties to alter their use, outputs or performance by
exploiting the system vulnerabilities. The technical solutions aimed at
ensuring the cybersecurity of high-risk AI systems shall be appropriate to the
relevant circumstances and the risks. The technical solutions to address AI
specific vulnerabilities shall include, where appropriate, measures to
prevent, detect, respond to, resolve and control for attacks trying to
manipulate the training dataset (‘data poisoning’), or pre-trained components
used in training (‘model poisoning’) , inputs designed to cause the model to
make a mistake (‘adversarial examples’ or ‘model evasion’), confidentiality
attacks or model flaws.

[ <- Previous ](https://artificialintelligenceact.eu/article/14/) [ Next ->
](https://artificialintelligenceact.eu/article/16/)

#### Suitable Recitals

[

Recital 66

](https://artificialintelligenceact.eu/recital/66/) [

Recital 74

](https://artificialintelligenceact.eu/recital/74/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"The provided context does not mention specific metrics used by NIST (National Institute of Standards and Technology) to measure compliance effectiveness within the AI RMF (Artificial Intelligence Risk Management Framework). The context instead discusses the Machine Learning Quality Management Guideline by the National Institute of Advanced Industrial Science and Technology, which outlines actions and fairness metrics goals related to machine learning model development and dataset preparation. It does not provide information about NIST's AI RMF compliance metrics.","Machine Learning Quality Management Guideline   National Institute of  
3rd English edition   Advanced Industrial Science and Technology  
 
131 
  
Table 5: Summary of work scope and actions  by AIFL  
 When the training dataset 
can be modified  When the design model 
and learning algorithm 
can be modified  When only post -
processing of trained models is possible  
AIFL 2  Define and record fairness 
metrics goals  for the 
training dataset, with multiple pre -processing 
techniques, if necessary,  
to improve and ensure they are met as much as possible . Add fairness metrics for 
model output to the learning objectives and balance them with other metrics such as AI perfor mance,  and to 
achieve the target using in-processing techniques.  N/A  
(Consider in advance  
about the measures that 
can be taken to adjust the trained model. If the essential conditions are unlikely to be met, the work scope needs to be reviewed.)  
AIFL 1  Define and measure 
fairness metrics goals  for 
the training dataset.  If 
there are deviations, pre-processing techniques are used to improve and recorded with the result.  Fairness metrics for model 
output are added to the learning goals, and if the results of  training with AI 
performance priority deviate from the goals, improve it by  in-
processing techniques and recorded with the result.  Define fairness metrics for 
model output and measure them during testing.  If there is a 
deviation from the target, try to improve it by adjusting the trained model, or any other adjustment in the operation.  
AIFL 0  Define, measure and 
record fairness metrics for training datasets.  Define fairness metrics for 
model outputs and record their measurement during 
training.  Define fairness metrics for 
model outputs and record 
the measurements during 
testing.  
 
The following sections describe the details of the techniques, mainly in line with the internal 
quality set forth in Section 1.7 and Chapter  6. 
8.5.2.2.  A-1: Sufficiency of p roblem domain analysis ( preliminary preparation)  
By following the guide presented in the preliminary  preparation ( Section 8.5.1 ), the following 
aspects can be investigated.  
– Dependency analysis between attributes  
– Clarification of requirements for training data set distribution  
– Metrics to be checked (fairness metrics)
Machine Learning Quality Management Guideline   National Institute of  
3rd English edition   Advanced Industrial Science and Technology  
 
132 
 In addition, considering the nature of the overall system to be developed, the requirements of 
the following perspectives should be sufficiently confirmed.  
– Consistency with legal and social requirements  
Since the development team may not have sufficient knowledge of legal and social requirements, it is advisable to consult experts in the field.  
 
The required actions for each  target AIFL  level are as follows  in the preliminary preparation 
phase.  
・ AIFL 1  
 Define the fairness requirements and record them, including the history.  
 Based on this requirement, requirements for data preparation such as fairness 
metrics for training datasets shall be  defined.  
 Based on this requirement, fairness metrics for model outputs shall be defined.  
・ AIFL 2 
 In addition to those listed in AIFL  1, the following actions shall be  taken 
 To model dependencies and causal relationships between data attributes.  
 The results of modeling should be reflected in the fairness metrics of the training dataset . 
 
The fairness metrics  for the training dataset are basically from the same perspective as those 
for the model output. For example, if the fairness requirement is n o difference in pass  or fail 
judgments based on gender  for the output, the training dataset should also be free of bias from 
that perspective.  Examples of metrics are described in Section 8.5.2.4.2.1  
8.5.2.3.  B-1 to 3: Coverage/ uniformity/validity of datasets (data preparation) 
In the data preparation stage, pre-processing  techniques are taken to ensure required fairness, 
mainly from the internal quality perspective of coverage , uniformity , and validity of datasets . 
What is commonly referred to as Data Fairness is achieved at this stage, and there are two types 
to address it; one for data collection process and the other for the data collected."
The provided context information does not include data on the percentage of federal AI projects that have fully adopted the NIST AI RMF to date.,"40 
 外部への説明という目的のため、 米国国立標準技術研究所 （ NIST ） の文書23を参考にし、
可能な限り正確かつ相手に理解できる 記録とし、説明の限界を意識できるよう努めてい
る。 
 
【実践例２】  
 当社は AIシステムを開発する小規模企業である。 技術担当役員 は全てのプロジェクト
を把握しており、自らプログラミングしたり論文を読解したりするなど AIに大変詳し
く、 AI倫理の問題についても強い関心を持っている。そのため当社では、部門間の専門
性のギャップが問題 になることはないと考えている。他方で、 プロジェクトに関わる人
たちの専門性が高いために、 いちいち確認しなくても 、行動目標ができているであろう
と思い込みがち である。そのため、 プロジェクトの進捗報告のレポートに 乖離評価 チェ
ックリストを添付し、 技術担当役員が必要に応じて聞き取りできるように工夫 している。  
また、当社は専門性が 高い集団であることから、世間の認識 とのずれが生じやすい傾
向があると分析している。 そのため、 運用状況を確認しつつ、 行動目標３－３－２にした
がって 日常的な情報収集や意見交換から得られた状況を定期的に共有する ことで、社会
的受容に意識を向けるようにしている。  
 
  
 
23 P J. Phillips, Amanda C. Hahn, Peter C. Fontana, David A. Broniatowski, Mark A. Przybocki, “Four 
Principles of Explainable Artificial Intelligence (Draft), NIST Interagency/Internal Report (NISTIR) - 8312 -
draft” (August 19, 2020)
**11 March 2024**

  * Updated the Compliance Checker to reflect the 'Final draft' of the AI Act.

**Data Privacy:** We are required to store your inputs from this form,
including your email address, in order to email your results to you. If you do
not use the email function, none of your data will be stored. We do not store
any other data that could be used to identify you or your device. If you wish
to remain anonymous, please use an email address that does not reveal your
identity. We do not share any of your data with any other parties. If you
would like your data to be deleted from our servers, please contact
[taylor@futureoflife.org](mailto:taylor@futureoflife.org)

## Frequently Asked Questions

#### Who developed this tool, and why?

This tool is developed by the team at the Future of Life Institute. We are in
no way affiliated with the European Union. We have developed this tool
voluntarily in order to aid the effective implementation of the AI Act,
because we believe the AI Act supports our mission to ensure that AI
technology remains beneficial for life, and avoids extreme large-scale risks.

#### Can I integrate this tool in my own website?

Our tool is built in a way that makes it difficult to support implementation
on other sites. We do not have an API available. Therefore, for almost all
cases, we suggest that you make this tool available to users of your website
by linking to this webpage – you are welcome to use these [mockup
image](https://workdrive.zohopublic.eu/external/0dde01ca37ea2f3e20d3af51fb016f0360b2570aa2f0fe555fc5321e3281e1d2?layout=list)
to feature the tool on your site.

#### When does the EU AI Act come into effect?

The European Commission is now supporting the Council of the European Union
and the European Parliament in concluding inter-institutional negotiations
(trilogue) – the last phase of negotiations before the EU AI Act is passed.
This is expected to be finished by the end of 2023 or early 2024. Once the
regulation is officially passed, there will be an implementation period of two
to three years depending on how the negotiations between the EU institutions
unfold. This means that the AI Act will likely be enforced in 2026 or later.
In addition, during the implementation period, the European standards bodies
are expected to develop standards for the AI Act.

#### What are the categories of risk defined by the EU AI Act?

The Act’s regulatory framework defines four levels of risk for AI systems:
unacceptable, high, limited, and minimal or no risk. Systems posing
unacceptable risks, such as threatening people’s safety, livelihood, and
rights – from social scoring by governments to toys using voice assistance –
will be prohibited. High-risk systems, such as those used in critical
infrastructure or law enforcement, will face strict requirements, including
around risk assessment, data quality, documentation, transparency, human
oversight, and accuracy. Systems posing limited risks, like chatbots, must
adhere to transparency obligations so users know they are not interacting with
humans. Minimal risk systems like games and spam filters can be freely used.

![](https://futureoflife.org/wp-
content/uploads/2023/08/pyramid_7F5843E5-9386-8052-931F5C4E98C6E5F2_75757.jpg)

#### What share of AI systems will fall into the high-risk category?

It is uncertain what share of AI systems will be in the high-risk category, as
both the field of AI and the law are still evolving. The European Commission
estimated in an [impact study](https://digital-
strategy.ec.europa.eu/en/library/impact-assessment-regulation-artificial-
intelligence) that only 5-15% of applications would be subject to stricter
rules. A [study](https://aai.frb.io/assets/files/AI-Act-Risk-Classification-
Study-appliedAI-March-2023.pdf) by appliedAI of 106 enterprise AI systems
found that 18% were high-risk, 42% low-risk, and 40% unclear if high or low
risk. The system with unclear risk classification in this study were mainly
within the areas of critical infrastructure, employment, law enforcement, and
product safety."
ISO 42001 was officially published in 2024.,"Some examples of
other standards referenced in ISO 42001 are:

  * [ISO 5259](https://www.iso.org/standard/81860.html) series on Data Quality
  * [ISO 23894](https://www.iso.org/standard/77304.html) AI Risk Management
  * [ISO 24029-1](https://www.iso.org/standard/77609.html) Assessment of the robustness of neural networks

### What is the process for ISO 42001 certification?

To obtain an ISO 42001 certification, an organization must successfully pass
an audit conducted by a certification body. However, the [ISO
42006](https://www.iso.org/standard/44546.html) standard defining the
requirements for such bodies is currently under development, which means that
**as of January 2024, no certifications can be granted yet**.

However, the standard can still be freely used for voluntary assessments, both
internally and by third parties. An **early gap analysis** based on the
published standard can significantly accelerate preparations for an official
certification when it becomes available.

Based on known timelines for other management system standards, the audit can
be expected to take **from several months to up to a year** , depending on the
number of people involved in the AI lifecycle processes; the organization’s
role as an AI provider, developer, or user; and the complexity of the AIMS.

According to the current draft of ISO 42006, the audit process will likely
align with the established sequence of an internal audit (initial assessment
of nonconformities with a window for improvement), followed by a two-stage
external audit (an accredited body reviewing relevant documentation and
evidence), with annual recertification.

## How Citadel AI tools help with ISO 42001 preparation

Citadel AI’s technology helps organizations streamline their AI testing and
governance processes, and automate compliance with AI standards and
guidelines. Our products, Lens and Radar, can:

  1. Automatically fulfill some of the most technically demanding requirements of ISO 42001
  2. Help engineering teams validate their models and datasets against international standards
  3. Provide easy reporting and guidance to get on the certification track as quickly as possible

Citadel AI is trusted by world-leading organizations such as [the British
Standards Institution](https://www.citadel.co.jp/en/news/2023/04/11/bsi-and-
citadel-ai-partner-to-solve-the-ai-challenges-of-tomorrow/). At this critical
period of time where AI standards and regulation are maturing, we believe that
we can help you streamline compliance, improve AI reliability, and navigate
this evolving landscape.

Contact us if you’re interested in learning more.

## Get in Touch

Interested in a product demo or discussing how Citadel AI can improve your AI
quality? Please reach out to us here or [by email](mailto:info@citadel.co.jp).

Full Name

Company Email

Message

I agree to Citadel AI’s [privacy policy](/en/privacy-policy/).

Submit

## Related Articles

[ ![](https://sp-
ao.shortpixel.ai/client/to_auto,q_glossy,ret_img,w_1536,h_806/https://www.citadel.co.jp/en/wp-
content/uploads/sites/6/2024/02/AIA-Header-1536x806.png)
](https://www.citadel.co.jp/en/blog/2024/02/14/eu-ai-act-what-it-means-for-
you/)

  * 14 Feb 2024 

###### [EU AI Act: what it means for
you](https://www.citadel.co.jp/en/blog/2024/02/14/eu-ai-act-what-it-means-for-
you/)

  * [Blog](https://www.citadel.co.jp/en/./blog/)

[ ![](https://sp-
ao.shortpixel.ai/client/to_auto,q_glossy,ret_img,w_1536,h_806/https://www.citadel.co.jp/en/wp-
content/uploads/sites/6/2024/01/ISO-42001-1536x806.png)
](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-
you/)

  * 26 Jan 2024 

###### [ISO 42001 AI Management System: what it means for
you](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-
you/)

  * [Blog](https://www.citadel.co.jp/en/./blog/)

[ !
そうした製品レベルの規格に比べ、ISO
42001は、AIに対する組織レベルのアプローチに焦点を当てており、適切な予防策や改善策を講じることで、AI関連のリスクを管理する実践的な方法を、マネジメントシステム規格として提示しています。マネジメントシステムを構築し、ISO
42001に適合するためには、以下対応が必要となります。

  1. AIの開発・利用時における、組織内のさまざまなユースケース、プロセスならびにポリシーに関わる全社レベルでの再調査
  2. 現在組織内で利用されている実践方法や、ポリシー、システム、ツール、ドキュメンテーションの評価ならびに更新
  3. 製品化やサービス提供段階における、既存のあるいは見直したプロセスが実際に実践されていることのエビデンスの提示

ただし、組織レベルのマネジメントシステム原則が、実際に現場で適用されていることを確認するには、製品レベルあるいはプロジェクトレベルの具体的かつ相当量のエビデンスを提示することも求められます。このため、組織内部で利用するツール、モニタリング、その他の技術的な仕組みの構築に関わるプロジェクトレベルでの評価や、ドキュメンテーションが必要となります（詳細は後述）。

### ISO 42001はどのような構成になっているのか？

ISO42001は、これまで発行されてきたセクター別のマネジメントシステム(MSS)群の中に新たに加わることとなります。よく知られているMSS規格としては以下のようなものがあります。

  * ISO 27001: 情報セキュリティマネジメントシステム
  * ISO 13485: 医療機器品質マネジメントシステム
  * ISO 37001: 贈収賄防止マネジメントシステム
  * ISO 14001: 環境マネジメントシステム

全てのマネジメントシステム(MSS)は[極めて似た構成](https://www.iso.org/management-system-
standards.html)となっており、さまざまな企業や組織に対して可能な限り柔軟に適用・検証することができるようになっています。ISO
42001もその例外ではありません。

  * Clause 1: 適用範囲
  * Clause 2: 引用規格
  * …
  * Clause 10: 改善
  * 附属書

規格の中で **最も実務に即した部分は附属書** であり、そこには、組織が適合のために実施しなければならない管理項目のリストが含まれています。

  * B.6.2."
"Yes, ISO 42001 integrates with other ISO standards for technology and management systems. It follows the same high-level structure as other management system standards, which allows for easier integration and consistency across different management systems within an organization. This structure includes a common framework with clauses and annexes that outline the requirements for implementing a management system. ISO 42001 also references other standards related to AI, such as ISO 5259 series on Data Quality, ISO 23894 on AI Risk Management, and ISO 24029-1 on the assessment of the robustness of neural networks, indicating that it is designed to work in conjunction with these technical specifications.","ISO standards consolidate years of work by experts, are widely accepted by
industry, and are often certifiable and auditable.

In regulated industries, ISO standards also provide technical specifications
and a pathway for companies and products to meet regulatory requirements,
which include medical devices, automobiles, and now AI systems under the EU AI
Act.

### How is ISO 42001 different from the other AI-related standards?

The SC 42 committee has already published dozens of technical documents
focusing on different aspects of AI development. For example, [ISO/IEC TS
4213:2022](https://www.iso.org/standard/79799.html), focusing on the
assessment of machine learning classification performance, can be applied to
quantitatively measure the performance of AI products.

In contrast to product-level standards, ISO 42001 provides an organization-
level approach to AI: as a **management system standard (MSS)** , it covers
principles of good governance and practical ways to manage AI-related risks,
by making sure that adequate preventative and remedial measures are in place.
Establishing a management system and complying with ISO 42001 involves:

  1. A company-wide review of the different use cases, processes, and policies for AI development and use.
  2. Validating or updating these existing processes, policies, systems, tools, and documentation.
  3. Providing evidence that the existing or updated processes are actually being followed at the product or service level.

In order to confirm that the organization-level management system principles
are followed, a sizable amount of product or project-level evidence also needs
to be provided. This includes per-project assessments and documentation on
internal tooling, evaluation, monitoring, and other technical infrastructure
(more on that below).

### What is the structure of ISO 42001?

ISO 42001 joins the family of other management system standards that have been
published over the years. Some of these well-known standards include:

  * ISO 27001: Information Security Management System
  * ISO 13485: Medical Device Quality Management System
  * ISO 37001: Anti-Bribery Management System
  * ISO 14001: Environmental Management System

All MSS follow [the exact same document
structure](https://www.iso.org/management-system-standards.html), which has
been tried and tested to be as flexible as possible and apply to organizations
across industries, with ISO 42001 being no exception:

  * Clause 1: Scope
  * Clause 2: Normative references
  * …
  * Clause 10: Improvement
  * Annexes

Most of the **practical requirements** are included in the **Annexes** , which
contain the list of controls that the organization must implement, such as:

  * B.6.2.4 AI system verification and validation. It requires developers to implement a standardized evaluation protocol for AI models and datasets.
  * B.6.2.8 AI system recording of event logs. It requires developers to have a logging system to record production input data, output predictions, and potential outliers.
  * B.7.4 Quality of data for AI systems. It requires developers to define and measure training and production data quality, as well as consider impact of data bias on performance.

(If you’re interested in a solution to automate the requirements above,
consider using [Citadel Lens](https://www.citadel.co.jp/en/products/lens/) and
[Citadel Radar](https://www.citadel.co.jp/en/products/radar/).)
Some examples of
other standards referenced in ISO 42001 are:

  * [ISO 5259](https://www.iso.org/standard/81860.html) series on Data Quality
  * [ISO 23894](https://www.iso.org/standard/77304.html) AI Risk Management
  * [ISO 24029-1](https://www.iso.org/standard/77609.html) Assessment of the robustness of neural networks

### What is the process for ISO 42001 certification?

To obtain an ISO 42001 certification, an organization must successfully pass
an audit conducted by a certification body. However, the [ISO
42006](https://www.iso.org/standard/44546.html) standard defining the
requirements for such bodies is currently under development, which means that
**as of January 2024, no certifications can be granted yet**.

However, the standard can still be freely used for voluntary assessments, both
internally and by third parties. An **early gap analysis** based on the
published standard can significantly accelerate preparations for an official
certification when it becomes available.

Based on known timelines for other management system standards, the audit can
be expected to take **from several months to up to a year** , depending on the
number of people involved in the AI lifecycle processes; the organization’s
role as an AI provider, developer, or user; and the complexity of the AIMS.

According to the current draft of ISO 42006, the audit process will likely
align with the established sequence of an internal audit (initial assessment
of nonconformities with a window for improvement), followed by a two-stage
external audit (an accredited body reviewing relevant documentation and
evidence), with annual recertification.

## How Citadel AI tools help with ISO 42001 preparation

Citadel AI’s technology helps organizations streamline their AI testing and
governance processes, and automate compliance with AI standards and
guidelines. Our products, Lens and Radar, can:

  1. Automatically fulfill some of the most technically demanding requirements of ISO 42001
  2. Help engineering teams validate their models and datasets against international standards
  3. Provide easy reporting and guidance to get on the certification track as quickly as possible

Citadel AI is trusted by world-leading organizations such as [the British
Standards Institution](https://www.citadel.co.jp/en/news/2023/04/11/bsi-and-
citadel-ai-partner-to-solve-the-ai-challenges-of-tomorrow/). At this critical
period of time where AI standards and regulation are maturing, we believe that
we can help you streamline compliance, improve AI reliability, and navigate
this evolving landscape.

Contact us if you’re interested in learning more.

## Get in Touch

Interested in a product demo or discussing how Citadel AI can improve your AI
quality? Please reach out to us here or [by email](mailto:info@citadel.co.jp).

Full Name

Company Email

Message

I agree to Citadel AI’s [privacy policy](/en/privacy-policy/).

Submit

## Related Articles

[ ![](https://sp-
ao.shortpixel.ai/client/to_auto,q_glossy,ret_img,w_1536,h_806/https://www.citadel.co.jp/en/wp-
content/uploads/sites/6/2024/02/AIA-Header-1536x806.png)
](https://www.citadel.co.jp/en/blog/2024/02/14/eu-ai-act-what-it-means-for-
you/)

  * 14 Feb 2024 

###### [EU AI Act: what it means for
you](https://www.citadel.co.jp/en/blog/2024/02/14/eu-ai-act-what-it-means-for-
you/)

  * [Blog](https://www.citadel.co.jp/en/./blog/)

[ ![](https://sp-
ao.shortpixel.ai/client/to_auto,q_glossy,ret_img,w_1536,h_806/https://www.citadel.co.jp/en/wp-
content/uploads/sites/6/2024/01/ISO-42001-1536x806.png)
](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-
you/)

  * 26 Jan 2024 

###### [ISO 42001 AI Management System: what it means for
you](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-
you/)

  * [Blog](https://www.citadel.co.jp/en/./blog/)

[ !"
"No, ISO 42001 certification does not guarantee that an AI system is completely unbiased and ethical. The certification indicates that an organization has established a management system for responsible AI practices, including addressing AI-related risks and implementing certain controls. However, it does not cover all technical aspects of AI processes, nor does it address the details of specific AI applications, such as ensuring complete absence of bias or ensuring ethical behavior in all scenarios. The standard provides a framework for organizations to manage AI responsibly, but it does not certify the ethical nature or unbiased state of individual AI systems.","ISO standards consolidate years of work by experts, are widely accepted by
industry, and are often certifiable and auditable.

In regulated industries, ISO standards also provide technical specifications
and a pathway for companies and products to meet regulatory requirements,
which include medical devices, automobiles, and now AI systems under the EU AI
Act.

### How is ISO 42001 different from the other AI-related standards?

The SC 42 committee has already published dozens of technical documents
focusing on different aspects of AI development. For example, [ISO/IEC TS
4213:2022](https://www.iso.org/standard/79799.html), focusing on the
assessment of machine learning classification performance, can be applied to
quantitatively measure the performance of AI products.

In contrast to product-level standards, ISO 42001 provides an organization-
level approach to AI: as a **management system standard (MSS)** , it covers
principles of good governance and practical ways to manage AI-related risks,
by making sure that adequate preventative and remedial measures are in place.
Establishing a management system and complying with ISO 42001 involves:

  1. A company-wide review of the different use cases, processes, and policies for AI development and use.
  2. Validating or updating these existing processes, policies, systems, tools, and documentation.
  3. Providing evidence that the existing or updated processes are actually being followed at the product or service level.

In order to confirm that the organization-level management system principles
are followed, a sizable amount of product or project-level evidence also needs
to be provided. This includes per-project assessments and documentation on
internal tooling, evaluation, monitoring, and other technical infrastructure
(more on that below).

### What is the structure of ISO 42001?

ISO 42001 joins the family of other management system standards that have been
published over the years. Some of these well-known standards include:

  * ISO 27001: Information Security Management System
  * ISO 13485: Medical Device Quality Management System
  * ISO 37001: Anti-Bribery Management System
  * ISO 14001: Environmental Management System

All MSS follow [the exact same document
structure](https://www.iso.org/management-system-standards.html), which has
been tried and tested to be as flexible as possible and apply to organizations
across industries, with ISO 42001 being no exception:

  * Clause 1: Scope
  * Clause 2: Normative references
  * …
  * Clause 10: Improvement
  * Annexes

Most of the **practical requirements** are included in the **Annexes** , which
contain the list of controls that the organization must implement, such as:

  * B.6.2.4 AI system verification and validation. It requires developers to implement a standardized evaluation protocol for AI models and datasets.
  * B.6.2.8 AI system recording of event logs. It requires developers to have a logging system to record production input data, output predictions, and potential outliers.
  * B.7.4 Quality of data for AI systems. It requires developers to define and measure training and production data quality, as well as consider impact of data bias on performance.

(If you’re interested in a solution to automate the requirements above,
consider using [Citadel Lens](https://www.citadel.co.jp/en/products/lens/) and
[Citadel Radar](https://www.citadel.co.jp/en/products/radar/).)
It requires developers to implement a standardized evaluation protocol for AI models and datasets.
  * B.6.2.8 AI system recording of event logs. It requires developers to have a logging system to record production input data, output predictions, and potential outliers.
  * B.7.4 Quality of data for AI systems. It requires developers to define and measure training and production data quality, as well as consider impact of data bias on performance.

(If you’re interested in a solution to automate the requirements above,
consider using [Citadel Lens](https://www.citadel.co.jp/en/products/lens/) and
[Citadel Radar](https://www.citadel.co.jp/en/products/radar/).)

###### Organization-level Requirements

These requirements apply to the whole organization and its processes:

  * B.2.2 AI policy
  * B.2.3 Alignment with other organizational policies
  * B.2.4 Review of the AI policy
  * B.3.2 AI roles and responsibilities
  * B.3.3 Reporting of concerns
  * B.5.2 AI system impact assessment process
  * B.6.1.2 Objectives for responsible development of AI system
  * B.6.1.3 Processes for responsible design and development of AI systems
  * B.7.2 Data for development and enhancement of AI system
  * B.7.3 Acquisition of data
  * B.8.3 External reporting
  * B.8.4 Communication of incidents
  * B.8.5 Information for interested parties
  * B.9.2 Processes for responsible use of AI
  * B.9.3 Objectives for responsible use of AI system
  * B.10.2 Allocating responsibilities
  * B.10.3 Suppliers
  * B.10.4 Customers

###### Project-level Requirements

These requirements apply to specific projects within the organization:

  * B.4.2 Resource documentation
  * B.4.3 Data resources
  * B.4.4 Tooling resources
  * B.4.5 System and computing resources
  * B.4.6 Human resources
  * B.5.3 Documentation of AI system impact assessments
  * B.5.4 Assessing AI system impact on individuals and groups of individuals
  * B.5.5 Assessing societal impacts of AI systems
  * B.6.2.2 AI system requirements and specification
  * B.6.2.3 Documentation of AI system design and development
  * B.6.2.4 AI system verification and validation
  * B.6.2.5 AI system deployment
  * B.6.2.6 AI system operation and monitoring
  * B.6.2.7 AI system technical documentation
  * B.6.2.8 AI system recording of event logs
  * B.7.4 Quality of data for AI systems
  * B.7.5 Data provenance
  * B.7.6 Data preparation
  * B.8.2 System documentation and information for users
  * B.9.3 Objectives for responsible use of AI system
  * B.9.4 Intended use of the AI system

### Does ISO 42001 cover all technical aspects of various AI processes?

ISO 42001 sits at the center of the responsible AI standards ecosystem, which
includes other standards that go deeper into specific aspects of AI
management, use, and development. The goal of ISO 42001 is to maintain a
balance between being prescriptive in terms of the requirements for an AIMS,
but flexible in terms of how AIMS is implemented for a particular
organization.

Since ISO 42001 itself **does not address the details of specific AI
applications** , it outsources the more technical details to more narrowly
focused standards and other ""generally accepted frameworks"".  Some examples of
other standards referenced in ISO 42001 are:

  * [ISO 5259](https://www.iso.org/standard/81860.html) series on Data Quality
  * [ISO 23894](https://www.iso.org/standard/77304.html) AI Risk Management
  * [ISO 24029-1](https://www.iso.org/standard/77609.html) Assessment of the robustness of neural networks

### What is the process for ISO 42001 certification?

To obtain an ISO 42001 certification, an organization must successfully pass
an audit conducted by a certification body. However, the [ISO
42006](https://www.iso.org/standard/44546.html) standard defining the
requirements for such bodies is currently under development, which means that
**as of January 2024, no certifications can be granted yet**."
"Yes, ISO 42001 is applicable to AI systems developed or deployed before the standard was published. The standard provides an organization-level approach to managing AI-related risks and ensuring good governance, which involves a company-wide review of AI use cases, processes, and policies, regardless of when the AI systems were developed or deployed. Organizations can validate or update their existing processes and policies to comply with ISO 42001 and provide evidence that these processes are being followed for all AI products or services, including those created before the standard's publication.","ISO standards consolidate years of work by experts, are widely accepted by
industry, and are often certifiable and auditable.

In regulated industries, ISO standards also provide technical specifications
and a pathway for companies and products to meet regulatory requirements,
which include medical devices, automobiles, and now AI systems under the EU AI
Act.

### How is ISO 42001 different from the other AI-related standards?

The SC 42 committee has already published dozens of technical documents
focusing on different aspects of AI development. For example, [ISO/IEC TS
4213:2022](https://www.iso.org/standard/79799.html), focusing on the
assessment of machine learning classification performance, can be applied to
quantitatively measure the performance of AI products.

In contrast to product-level standards, ISO 42001 provides an organization-
level approach to AI: as a **management system standard (MSS)** , it covers
principles of good governance and practical ways to manage AI-related risks,
by making sure that adequate preventative and remedial measures are in place.
Establishing a management system and complying with ISO 42001 involves:

  1. A company-wide review of the different use cases, processes, and policies for AI development and use.
  2. Validating or updating these existing processes, policies, systems, tools, and documentation.
  3. Providing evidence that the existing or updated processes are actually being followed at the product or service level.

In order to confirm that the organization-level management system principles
are followed, a sizable amount of product or project-level evidence also needs
to be provided. This includes per-project assessments and documentation on
internal tooling, evaluation, monitoring, and other technical infrastructure
(more on that below).

### What is the structure of ISO 42001?

ISO 42001 joins the family of other management system standards that have been
published over the years. Some of these well-known standards include:

  * ISO 27001: Information Security Management System
  * ISO 13485: Medical Device Quality Management System
  * ISO 37001: Anti-Bribery Management System
  * ISO 14001: Environmental Management System

All MSS follow [the exact same document
structure](https://www.iso.org/management-system-standards.html), which has
been tried and tested to be as flexible as possible and apply to organizations
across industries, with ISO 42001 being no exception:

  * Clause 1: Scope
  * Clause 2: Normative references
  * …
  * Clause 10: Improvement
  * Annexes

Most of the **practical requirements** are included in the **Annexes** , which
contain the list of controls that the organization must implement, such as:

  * B.6.2.4 AI system verification and validation. It requires developers to implement a standardized evaluation protocol for AI models and datasets.
  * B.6.2.8 AI system recording of event logs. It requires developers to have a logging system to record production input data, output predictions, and potential outliers.
  * B.7.4 Quality of data for AI systems. It requires developers to define and measure training and production data quality, as well as consider impact of data bias on performance.

(If you’re interested in a solution to automate the requirements above,
consider using [Citadel Lens](https://www.citadel.co.jp/en/products/lens/) and
[Citadel Radar](https://www.citadel.co.jp/en/products/radar/).)
It requires developers to implement a standardized evaluation protocol for AI models and datasets.
  * B.6.2.8 AI system recording of event logs. It requires developers to have a logging system to record production input data, output predictions, and potential outliers.
  * B.7.4 Quality of data for AI systems. It requires developers to define and measure training and production data quality, as well as consider impact of data bias on performance.

(If you’re interested in a solution to automate the requirements above,
consider using [Citadel Lens](https://www.citadel.co.jp/en/products/lens/) and
[Citadel Radar](https://www.citadel.co.jp/en/products/radar/).)

###### Organization-level Requirements

These requirements apply to the whole organization and its processes:

  * B.2.2 AI policy
  * B.2.3 Alignment with other organizational policies
  * B.2.4 Review of the AI policy
  * B.3.2 AI roles and responsibilities
  * B.3.3 Reporting of concerns
  * B.5.2 AI system impact assessment process
  * B.6.1.2 Objectives for responsible development of AI system
  * B.6.1.3 Processes for responsible design and development of AI systems
  * B.7.2 Data for development and enhancement of AI system
  * B.7.3 Acquisition of data
  * B.8.3 External reporting
  * B.8.4 Communication of incidents
  * B.8.5 Information for interested parties
  * B.9.2 Processes for responsible use of AI
  * B.9.3 Objectives for responsible use of AI system
  * B.10.2 Allocating responsibilities
  * B.10.3 Suppliers
  * B.10.4 Customers

###### Project-level Requirements

These requirements apply to specific projects within the organization:

  * B.4.2 Resource documentation
  * B.4.3 Data resources
  * B.4.4 Tooling resources
  * B.4.5 System and computing resources
  * B.4.6 Human resources
  * B.5.3 Documentation of AI system impact assessments
  * B.5.4 Assessing AI system impact on individuals and groups of individuals
  * B.5.5 Assessing societal impacts of AI systems
  * B.6.2.2 AI system requirements and specification
  * B.6.2.3 Documentation of AI system design and development
  * B.6.2.4 AI system verification and validation
  * B.6.2.5 AI system deployment
  * B.6.2.6 AI system operation and monitoring
  * B.6.2.7 AI system technical documentation
  * B.6.2.8 AI system recording of event logs
  * B.7.4 Quality of data for AI systems
  * B.7.5 Data provenance
  * B.7.6 Data preparation
  * B.8.2 System documentation and information for users
  * B.9.3 Objectives for responsible use of AI system
  * B.9.4 Intended use of the AI system

### Does ISO 42001 cover all technical aspects of various AI processes?

ISO 42001 sits at the center of the responsible AI standards ecosystem, which
includes other standards that go deeper into specific aspects of AI
management, use, and development. The goal of ISO 42001 is to maintain a
balance between being prescriptive in terms of the requirements for an AIMS,
but flexible in terms of how AIMS is implemented for a particular
organization.

Since ISO 42001 itself **does not address the details of specific AI
applications** , it outsources the more technical details to more narrowly
focused standards and other ""generally accepted frameworks"".  Some examples of
other standards referenced in ISO 42001 are:

  * [ISO 5259](https://www.iso.org/standard/81860.html) series on Data Quality
  * [ISO 23894](https://www.iso.org/standard/77304.html) AI Risk Management
  * [ISO 24029-1](https://www.iso.org/standard/77609.html) Assessment of the robustness of neural networks

### What is the process for ISO 42001 certification?

To obtain an ISO 42001 certification, an organization must successfully pass
an audit conducted by a certification body. However, the [ISO
42006](https://www.iso.org/standard/44546.html) standard defining the
requirements for such bodies is currently under development, which means that
**as of January 2024, no certifications can be granted yet**."
"Implementing ISO 42001 standards does not guarantee that an AI system will perform as intended under all conditions. The standards provide a framework for responsible AI management, including the development and use of AI systems, but they do not cover all technical aspects of AI processes or the details of specific AI applications. The standards aim to ensure that organizations have processes in place for good governance and risk management related to AI, but the performance of AI systems under all conditions also depends on many other factors, including the specific design and implementation of the AI system, the quality of the data used, and the environment in which the AI operates.","It requires developers to implement a standardized evaluation protocol for AI models and datasets.
  * B.6.2.8 AI system recording of event logs. It requires developers to have a logging system to record production input data, output predictions, and potential outliers.
  * B.7.4 Quality of data for AI systems. It requires developers to define and measure training and production data quality, as well as consider impact of data bias on performance.

(If you’re interested in a solution to automate the requirements above,
consider using [Citadel Lens](https://www.citadel.co.jp/en/products/lens/) and
[Citadel Radar](https://www.citadel.co.jp/en/products/radar/).)

###### Organization-level Requirements

These requirements apply to the whole organization and its processes:

  * B.2.2 AI policy
  * B.2.3 Alignment with other organizational policies
  * B.2.4 Review of the AI policy
  * B.3.2 AI roles and responsibilities
  * B.3.3 Reporting of concerns
  * B.5.2 AI system impact assessment process
  * B.6.1.2 Objectives for responsible development of AI system
  * B.6.1.3 Processes for responsible design and development of AI systems
  * B.7.2 Data for development and enhancement of AI system
  * B.7.3 Acquisition of data
  * B.8.3 External reporting
  * B.8.4 Communication of incidents
  * B.8.5 Information for interested parties
  * B.9.2 Processes for responsible use of AI
  * B.9.3 Objectives for responsible use of AI system
  * B.10.2 Allocating responsibilities
  * B.10.3 Suppliers
  * B.10.4 Customers

###### Project-level Requirements

These requirements apply to specific projects within the organization:

  * B.4.2 Resource documentation
  * B.4.3 Data resources
  * B.4.4 Tooling resources
  * B.4.5 System and computing resources
  * B.4.6 Human resources
  * B.5.3 Documentation of AI system impact assessments
  * B.5.4 Assessing AI system impact on individuals and groups of individuals
  * B.5.5 Assessing societal impacts of AI systems
  * B.6.2.2 AI system requirements and specification
  * B.6.2.3 Documentation of AI system design and development
  * B.6.2.4 AI system verification and validation
  * B.6.2.5 AI system deployment
  * B.6.2.6 AI system operation and monitoring
  * B.6.2.7 AI system technical documentation
  * B.6.2.8 AI system recording of event logs
  * B.7.4 Quality of data for AI systems
  * B.7.5 Data provenance
  * B.7.6 Data preparation
  * B.8.2 System documentation and information for users
  * B.9.3 Objectives for responsible use of AI system
  * B.9.4 Intended use of the AI system

### Does ISO 42001 cover all technical aspects of various AI processes?

ISO 42001 sits at the center of the responsible AI standards ecosystem, which
includes other standards that go deeper into specific aspects of AI
management, use, and development. The goal of ISO 42001 is to maintain a
balance between being prescriptive in terms of the requirements for an AIMS,
but flexible in terms of how AIMS is implemented for a particular
organization.

Since ISO 42001 itself **does not address the details of specific AI
applications** , it outsources the more technical details to more narrowly
focused standards and other ""generally accepted frameworks"".  Some examples of
other standards referenced in ISO 42001 are:

  * [ISO 5259](https://www.iso.org/standard/81860.html) series on Data Quality
  * [ISO 23894](https://www.iso.org/standard/77304.html) AI Risk Management
  * [ISO 24029-1](https://www.iso.org/standard/77609.html) Assessment of the robustness of neural networks

### What is the process for ISO 42001 certification?

To obtain an ISO 42001 certification, an organization must successfully pass
an audit conducted by a certification body. However, the [ISO
42006](https://www.iso.org/standard/44546.html) standard defining the
requirements for such bodies is currently under development, which means that
**as of January 2024, no certifications can be granted yet**.
ISO standards consolidate years of work by experts, are widely accepted by
industry, and are often certifiable and auditable.

In regulated industries, ISO standards also provide technical specifications
and a pathway for companies and products to meet regulatory requirements,
which include medical devices, automobiles, and now AI systems under the EU AI
Act.

### How is ISO 42001 different from the other AI-related standards?

The SC 42 committee has already published dozens of technical documents
focusing on different aspects of AI development. For example, [ISO/IEC TS
4213:2022](https://www.iso.org/standard/79799.html), focusing on the
assessment of machine learning classification performance, can be applied to
quantitatively measure the performance of AI products.

In contrast to product-level standards, ISO 42001 provides an organization-
level approach to AI: as a **management system standard (MSS)** , it covers
principles of good governance and practical ways to manage AI-related risks,
by making sure that adequate preventative and remedial measures are in place.
Establishing a management system and complying with ISO 42001 involves:

  1. A company-wide review of the different use cases, processes, and policies for AI development and use.
  2. Validating or updating these existing processes, policies, systems, tools, and documentation.
  3. Providing evidence that the existing or updated processes are actually being followed at the product or service level.

In order to confirm that the organization-level management system principles
are followed, a sizable amount of product or project-level evidence also needs
to be provided. This includes per-project assessments and documentation on
internal tooling, evaluation, monitoring, and other technical infrastructure
(more on that below).

### What is the structure of ISO 42001?

ISO 42001 joins the family of other management system standards that have been
published over the years. Some of these well-known standards include:

  * ISO 27001: Information Security Management System
  * ISO 13485: Medical Device Quality Management System
  * ISO 37001: Anti-Bribery Management System
  * ISO 14001: Environmental Management System

All MSS follow [the exact same document
structure](https://www.iso.org/management-system-standards.html), which has
been tried and tested to be as flexible as possible and apply to organizations
across industries, with ISO 42001 being no exception:

  * Clause 1: Scope
  * Clause 2: Normative references
  * …
  * Clause 10: Improvement
  * Annexes

Most of the **practical requirements** are included in the **Annexes** , which
contain the list of controls that the organization must implement, such as:

  * B.6.2.4 AI system verification and validation. It requires developers to implement a standardized evaluation protocol for AI models and datasets.
  * B.6.2.8 AI system recording of event logs. It requires developers to have a logging system to record production input data, output predictions, and potential outliers.
  * B.7.4 Quality of data for AI systems. It requires developers to define and measure training and production data quality, as well as consider impact of data bias on performance.

(If you’re interested in a solution to automate the requirements above,
consider using [Citadel Lens](https://www.citadel.co.jp/en/products/lens/) and
[Citadel Radar](https://www.citadel.co.jp/en/products/radar/).)"
"ISO 42001 does not impose penalties for compliance failures in AI systems. Penalties for non-compliance with AI system regulations are determined by the regulatory framework outlined in the AI Act, which specifies administrative fines for various infringements. For example, non-compliance with the prohibition of certain artificial intelligence practices can result in fines of up to 35 million EUR or up to 7% of the total worldwide annual turnover for the preceding financial year, whichever is higher. Other infringements related to the obligations of operators or notified bodies can result in fines of up to 15 million EUR or up to 3% of the total worldwide annual turnover for the preceding financial year, whichever is higher. ISO standards like ISO 42001 provide guidelines for quality management systems but do not have the authority to enforce penalties.","Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

1\. In compliance with the terms and conditions laid down in this Regulation,
Member States shall lay down the rules on penalties and other enforcement
measures, which may also include warnings and non-monetary measures,
applicable to infringements of this Regulation by operators, and shall take
all measures necessary to ensure that they are properly and effectively
implemented and taking into account the guidelines issued by the Commission
pursuant to [Article 82b](https://artificialintelligenceact.eu/article/82b/).
The penalties provided for shall be effective, proportionate, and dissuasive.
They shall take into account the interests of SMEs including start-ups and
their economic viability.

2\. The Member States shall without delay notify the Commission and at the
latest by the date of entry into application of those respective rules and of
those respective measures and shall notify them, without delay, of any
subsequent amendment affecting them.

3\. Non-compliance with the prohibition of the artificial intelligence
practices referred to in [Article
5](https://artificialintelligenceact.eu/article/5/) shall be subject to
administrative fines of up to 35 000 000 EUR or, if the offender is a company,
up to 7 % of its total worldwide annual turnover for the preceding financial
year, whichever is higher.

4\. Non-compliance of an AI system with any of the following provisions
related to operators or notified bodies, other than those laid down in
[Article 5](https://artificialintelligenceact.eu/article/5/), shall be subject
to administrative fines of up to 15 000 000 EUR or, if the offender is a
company, up to 3% of its total worldwide annual turnover for the preceding
financial year, whichever is higher:

(b) obligations of providers pursuant to [Article
16](https://artificialintelligenceact.eu/article/16/);

(d) obligations of authorised representatives pursuant to [Article
22](https://artificialintelligenceact.eu/article\\/22/);

(e) obligations of importers pursuant to [Article
23](https://artificialintelligenceact.eu/article/23/);

(f) obligations of distributors pursuant to [Article
24](https://artificialintelligenceact.eu/article/24/);

(g) obligations of deployers pursuant to [Article
26](https://artificialintelligenceact.eu/article/26/), paragraphs 1 to 6a;

(h) requirements and obligations of notified bodies pursuant to Article
[33](https://artificialintelligenceact.eu/article/33/),
[34](https://artificialintelligenceact.eu/article/34/)(1),
[34](https://artificialintelligenceact.eu/article/34/)(3),
[34](https://artificialintelligenceact.eu/article/34/)(4),
[34a](https://artificialintelligenceact.eu/article/34a/);

(i) transparency obligations for providers and users pursuant to [Article
50](https://artificialintelligenceact.eu/article/50/).

5\. The supply of incorrect, incomplete or misleading information to notified
bodies and national competent authorities in reply to a request shall be
subject to administrative fines of up to 7 500 000 EUR or, if the offender is
a company, up to 1 % of its total worldwide annual turnover for the preceding
financial year, whichever is higher.

5a. In case of SMEs, including start-ups, each fine referred to in this
Article shall be up to the percentages or amount referred to paragraphs 3, 4
and 5, whichever of the two is lower.

6\.
Setting up a Quality Management System (e.g. [based on ISO 42001](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-you/))
  * [Article 18](https://artificialintelligenceact.eu/article/18). Keeping documentation for 10 years at the disposal of the national competent authorities.
  * [Article 20](https://artificialintelligenceact.eu/article/20/). Storing the automatically generated logs for at least 6 months.
  * [Article 21](https://artificialintelligenceact.eu/article/21/). Corrective actions and Duty of information when the system is not in conformity.
  * [Article 23](https://artificialintelligenceact.eu/article/23). Providing the necessary information to competent authorities.
  * [Article 51](https://artificialintelligenceact.eu/article/51). Registering the high-risk model in the EU Database.

 **Deployers** of high-risk AI systems have lighter obligations, and will need
to comply with [Article 29](https://artificialintelligenceact.eu/article/29).
This includes exercising due diligence in using the system; monitoring and
logging the system in use; and, in specific conditions, carrying out [a
fundamental rights impact
assessment](https://artificialintelligenceact.eu/article/29a).

However, a Deployer, Importer, or Distributor is considered a Provider of a
high-risk AI system if they:

  * Change the name or a trademark of the AI system
  * Make a substantial modification to the AI system
  * Modify the intended purpose of a non-high-risk AI system already on the market, in such a manner that makes the AI system high-risk

If the initial Provider was overridden in the cases above, the initial
Provider is no longer subject to the usual obligations, but must provide “the
reasonably expected technical access” to the new Provider.

 **Importers** and **Distributors** are subject to the obligations in [Article
26](https://artificialintelligenceact.eu/article/26) and [Article
27](https://artificialintelligenceact.eu/article/27), respectively. They
include verifying that the provider has complied with their obligations, and
that the system has gone through the relevant conformity assessment and bears
the CE mark.

 **Authorized Representatives** are subject to the obligations in [Article
25](https://artificialintelligenceact.eu/article/25), which outline the
cooperation with relevant authorities and submission of relevant documents.

The following diagram can be useful to understand your entity’s (“operator’s”)
position in the AI system supply chain as defined by the AI Act. Note that
this is a simplified version that makes some assumptions; for example, that
the AI system’s output is located or used within the EU. We also recommend
using the interactive [AI Act Compliance
Checker](https://artificialintelligenceact.eu/assessment/eu-ai-act-compliance-
checker/) tool developed by the Future of Life Institute.

![](https://sp-
ao.shortpixel.ai/client/to_auto,q_glossy,ret_img,w_2090,h_3309/https://www.citadel.co.jp/en/wp-
content/uploads/sites/6/2024/02/AIA-flowchart-2.png)

## What are the requirements for different types of AI systems?

The Act defines several categories of AI systems with different obligations.
In principle, these categories are not mutually exclusive, and the same AI
system may fall under a mix of obligations. For example, a foundation model
that’s used in a medical device and directly interacts with end users might
need to fulfill High-Risk, GPAI, and Transparency obligations all at once.

### Prohibited AI

According to [Article 5](https://artificialintelligenceact.eu/article/5),
these use cases are considered unacceptable and will be prohibited, with some
exceptions for law enforcement activities. Examples: real-time biometric
identification, social scoring, manipulative techniques.

### High-Risk AI

These use cases are considered highly harmful if misused, and will need to go
through a third-party conformity assessment before being placed on the market.
According to [Article 6](https://artificialintelligenceact.eu/article/6), an
AI system is considered high-risk under either of the following conditions:

  1. It’s used as a safety component of a product, or is itself a product that’s already regulated by existing EU laws listed in Annex II, and is required to undergo a third-party conformity assessment under those Annex II laws; or,
  2. It falls under Annex III, unless it’s proved to not possess significant risk of harm to health, safety, or fundamental rights

We will now cover the two points above in more detail."
"ISO 42001 requirements do not have a specified date to become mandatory for all AI applications worldwide. The standard provides guidelines for responsible AI use and can be adopted voluntarily by organizations. It is not a regulation that mandates compliance, but rather a framework to help organizations implement responsible AI management systems.","It requires developers to implement a standardized evaluation protocol for AI models and datasets.
  * B.6.2.8 AI system recording of event logs. It requires developers to have a logging system to record production input data, output predictions, and potential outliers.
  * B.7.4 Quality of data for AI systems. It requires developers to define and measure training and production data quality, as well as consider impact of data bias on performance.

(If you’re interested in a solution to automate the requirements above,
consider using [Citadel Lens](https://www.citadel.co.jp/en/products/lens/) and
[Citadel Radar](https://www.citadel.co.jp/en/products/radar/).)

###### Organization-level Requirements

These requirements apply to the whole organization and its processes:

  * B.2.2 AI policy
  * B.2.3 Alignment with other organizational policies
  * B.2.4 Review of the AI policy
  * B.3.2 AI roles and responsibilities
  * B.3.3 Reporting of concerns
  * B.5.2 AI system impact assessment process
  * B.6.1.2 Objectives for responsible development of AI system
  * B.6.1.3 Processes for responsible design and development of AI systems
  * B.7.2 Data for development and enhancement of AI system
  * B.7.3 Acquisition of data
  * B.8.3 External reporting
  * B.8.4 Communication of incidents
  * B.8.5 Information for interested parties
  * B.9.2 Processes for responsible use of AI
  * B.9.3 Objectives for responsible use of AI system
  * B.10.2 Allocating responsibilities
  * B.10.3 Suppliers
  * B.10.4 Customers

###### Project-level Requirements

These requirements apply to specific projects within the organization:

  * B.4.2 Resource documentation
  * B.4.3 Data resources
  * B.4.4 Tooling resources
  * B.4.5 System and computing resources
  * B.4.6 Human resources
  * B.5.3 Documentation of AI system impact assessments
  * B.5.4 Assessing AI system impact on individuals and groups of individuals
  * B.5.5 Assessing societal impacts of AI systems
  * B.6.2.2 AI system requirements and specification
  * B.6.2.3 Documentation of AI system design and development
  * B.6.2.4 AI system verification and validation
  * B.6.2.5 AI system deployment
  * B.6.2.6 AI system operation and monitoring
  * B.6.2.7 AI system technical documentation
  * B.6.2.8 AI system recording of event logs
  * B.7.4 Quality of data for AI systems
  * B.7.5 Data provenance
  * B.7.6 Data preparation
  * B.8.2 System documentation and information for users
  * B.9.3 Objectives for responsible use of AI system
  * B.9.4 Intended use of the AI system

### Does ISO 42001 cover all technical aspects of various AI processes?

ISO 42001 sits at the center of the responsible AI standards ecosystem, which
includes other standards that go deeper into specific aspects of AI
management, use, and development. The goal of ISO 42001 is to maintain a
balance between being prescriptive in terms of the requirements for an AIMS,
but flexible in terms of how AIMS is implemented for a particular
organization.

Since ISO 42001 itself **does not address the details of specific AI
applications** , it outsources the more technical details to more narrowly
focused standards and other ""generally accepted frameworks"".  Some examples of
other standards referenced in ISO 42001 are:

  * [ISO 5259](https://www.iso.org/standard/81860.html) series on Data Quality
  * [ISO 23894](https://www.iso.org/standard/77304.html) AI Risk Management
  * [ISO 24029-1](https://www.iso.org/standard/77609.html) Assessment of the robustness of neural networks

### What is the process for ISO 42001 certification?

To obtain an ISO 42001 certification, an organization must successfully pass
an audit conducted by a certification body. However, the [ISO
42006](https://www.iso.org/standard/44546.html) standard defining the
requirements for such bodies is currently under development, which means that
**as of January 2024, no certifications can be granted yet**.
Some examples of
other standards referenced in ISO 42001 are:

  * [ISO 5259](https://www.iso.org/standard/81860.html) series on Data Quality
  * [ISO 23894](https://www.iso.org/standard/77304.html) AI Risk Management
  * [ISO 24029-1](https://www.iso.org/standard/77609.html) Assessment of the robustness of neural networks

### What is the process for ISO 42001 certification?

To obtain an ISO 42001 certification, an organization must successfully pass
an audit conducted by a certification body. However, the [ISO
42006](https://www.iso.org/standard/44546.html) standard defining the
requirements for such bodies is currently under development, which means that
**as of January 2024, no certifications can be granted yet**.

However, the standard can still be freely used for voluntary assessments, both
internally and by third parties. An **early gap analysis** based on the
published standard can significantly accelerate preparations for an official
certification when it becomes available.

Based on known timelines for other management system standards, the audit can
be expected to take **from several months to up to a year** , depending on the
number of people involved in the AI lifecycle processes; the organization’s
role as an AI provider, developer, or user; and the complexity of the AIMS.

According to the current draft of ISO 42006, the audit process will likely
align with the established sequence of an internal audit (initial assessment
of nonconformities with a window for improvement), followed by a two-stage
external audit (an accredited body reviewing relevant documentation and
evidence), with annual recertification.

## How Citadel AI tools help with ISO 42001 preparation

Citadel AI’s technology helps organizations streamline their AI testing and
governance processes, and automate compliance with AI standards and
guidelines. Our products, Lens and Radar, can:

  1. Automatically fulfill some of the most technically demanding requirements of ISO 42001
  2. Help engineering teams validate their models and datasets against international standards
  3. Provide easy reporting and guidance to get on the certification track as quickly as possible

Citadel AI is trusted by world-leading organizations such as [the British
Standards Institution](https://www.citadel.co.jp/en/news/2023/04/11/bsi-and-
citadel-ai-partner-to-solve-the-ai-challenges-of-tomorrow/). At this critical
period of time where AI standards and regulation are maturing, we believe that
we can help you streamline compliance, improve AI reliability, and navigate
this evolving landscape.

Contact us if you’re interested in learning more.

## Get in Touch

Interested in a product demo or discussing how Citadel AI can improve your AI
quality? Please reach out to us here or [by email](mailto:info@citadel.co.jp).

Full Name

Company Email

Message

I agree to Citadel AI’s [privacy policy](/en/privacy-policy/).

Submit

## Related Articles

[ ![](https://sp-
ao.shortpixel.ai/client/to_auto,q_glossy,ret_img,w_1536,h_806/https://www.citadel.co.jp/en/wp-
content/uploads/sites/6/2024/02/AIA-Header-1536x806.png)
](https://www.citadel.co.jp/en/blog/2024/02/14/eu-ai-act-what-it-means-for-
you/)

  * 14 Feb 2024 

###### [EU AI Act: what it means for
you](https://www.citadel.co.jp/en/blog/2024/02/14/eu-ai-act-what-it-means-for-
you/)

  * [Blog](https://www.citadel.co.jp/en/./blog/)

[ ![](https://sp-
ao.shortpixel.ai/client/to_auto,q_glossy,ret_img,w_1536,h_806/https://www.citadel.co.jp/en/wp-
content/uploads/sites/6/2024/01/ISO-42001-1536x806.png)
](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-
you/)

  * 26 Jan 2024 

###### [ISO 42001 AI Management System: what it means for
you](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-
you/)

  * [Blog](https://www.citadel.co.jp/en/./blog/)

[ !"
