output,source
"## Obligations for AI Software Developers in High-Risk Applications

If your company develops AI software that clients later use in high-risk applications, your role and obligations under the AI Act would be classified as a **Provider**. This means you hold the primary responsibility for ensuring the AI system complies with the regulations.

Here's a breakdown of your key obligations:

* **Risk Management:** Implement robust risk management processes to identify, assess, and mitigate potential risks associated with the AI system.
* **Data Governance:** Ensure the use of high-quality data for training, validation, and testing of the AI system. This includes establishing data governance practices to maintain data integrity and security.
* **Documentation:** Maintain comprehensive documentation as outlined in Annex IV of the AI Act. This documentation should detail the design, development, and intended use of the AI system.
* **Logging:** Implement automatic logging mechanisms to track the operation of the AI system and record relevant data for potential audits or investigations.
* **Transparency:** Provide your clients (Deployers) with clear and understandable information about the capabilities, limitations, and intended use of the AI system.
* **Human Oversight:** Ensure appropriate human oversight measures are in place to monitor the AI system's operation and intervene when necessary.
* **Robustness and Accuracy:** Guarantee the robustness, accuracy, and cybersecurity of the AI system to prevent malfunctions or unauthorized access.
* **Quality Management System:** Establish a Quality Management System, potentially based on ISO 42001, to ensure ongoing compliance with the AI Act requirements.
* **Record Keeping:** Keep all relevant documentation and automatically generated logs for the specified durations (10 years for documentation and 6 months for logs) to be readily available for authorities.
* **Corrective Actions:** Implement corrective actions and inform relevant parties if the AI system is found to be non-compliant with the regulations.
* **Cooperation with Authorities:** Provide necessary information and cooperate with competent authorities during audits or investigations.
* **Registration:** Register the high-risk AI model in the EU database.

**Additional Considerations:**

* **Substantial Modifications:** If your clients modify your AI software in a way that significantly alters its functionality or purpose, they may assume the role of Provider and take on the associated obligations. However, you must still provide them with reasonable technical access to the system.
* **Change in Intended Use:** If your AI software is initially intended for non-high-risk applications but is later used in high-risk scenarios, it may become subject to the AI Act's requirements for high-risk AI systems.

**Compliance Tools:**

* **AI Act Compliance Checker:** Utilize tools like the AI Act Compliance Checker developed by the Future of Life Institute to assess your compliance with the regulations.

By fulfilling these obligations, you can ensure your AI software meets the necessary standards for safe and ethical use in high-risk applications within the EU market. 
","The AI Act takes a “risk-based” approach, scaling the stringency of
requirements based on the inherent risk of an AI system. You can learn more
about how this approach differs from other frameworks and standards in our
[previous blog post on the state of AI
regulation.](https://www.citadel.co.jp/en/blog/2023/03/20/an-overview-of-ai-
standardization-and-regulation-in-2023/)

As explained below, AI systems categorized as __ high-risk will feel the most
impact from this regulation. Developers of such high-risk AI systems must go
through a **third-party conformity assessment** in order to **obtain a CE mark
and access the EU market**.

## Do the requirements differ for developers and users of AI systems?

The most recent version of the AI Act [from Feb
2](https://artificialintelligenceact.eu/wp-content/uploads/2024/01/AI-Act-
FullText.pdf), approved by the Committee of Permanent Representatives,
provides additional information on the
[roles](https://artificialintelligenceact.eu/article/3) and [their
responsibilities](https://artificialintelligenceact.eu/article/28/) across the
AI Value Chain. The law defines six key roles:

  1.  **Provider**. An entity (e.g. company) that develops an AI system and “places them on the market or puts the system into service under its own name or trademark, whether for payment or free of charge”.
  2.  **Deployer**. An entity that uses an AI system under its authority, except in personal non-professional activities. Note that this does not refer to the affected end users.
  3.  **Importer**. An EU entity that places an non-EU AI system on the EU market.
  4.  **Distributor**. An entity in the supply chain, other than the provider or the importer, that makes an AI system available in the EU market.
  5.  **Authorized Representatives**. An EU entity that carries out obligations on behalf of a Provider.
  6.  **Operator.** An entity that falls into any of the five categories above.

> Operators could act in more than one role at the same time and should
> therefore fulfill cumulatively all relevant obligations associated with
> those roles. For example, an operator could act as a distributor and an
> importer at the same time.
>
> [Recital 56a](https://artificialintelligenceact.eu/recital/56a)

 **Providers** bear the majority of responsibilities under the AI Act.
Providers of high-risk AI systems will need to comply with [Article
16](https://artificialintelligenceact.eu/article/16/), and by extension:

  * [Article 9](https://artificialintelligenceact.eu/article/9). Implementing risk management processes.
  * [Article 10](https://artificialintelligenceact.eu/article/10). Data and Data Governance. Using high-quality training, validation and testing data.
  * [Article 11](https://artificialintelligenceact.eu/article/11). Establishing documentation as defined by [Annex IV](https://artificialintelligenceact.eu/annex/4).
  * [Article 12](https://artificialintelligenceact.eu/article/12). Implementing automatic logging.
  * [Article 13](https://artificialintelligenceact.eu/article/13). Ensuring an appropriate level of transparency with Deployers.
  * [Article 14](https://artificialintelligenceact.eu/article/14). Ensuring human oversight measures.
  * [Article 15](https://artificialintelligenceact.eu/article/15). Ensuring robustness, accuracy and cybersecurity.
  * [Article 17](https://artificialintelligenceact.eu/article/17). Setting up a Quality Management System (e.g. [based on ISO 42001](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-you/))
  * [Article 18](https://artificialintelligenceact.eu/article/18). Keeping documentation for 10 years at the disposal of the national competent authorities.
  * [Article 20](https://artificialintelligenceact.eu/article/20/). Storing the automatically generated logs for at least 6 months.
  * [Article 21](https://artificialintelligenceact.eu/article/21/). Corrective actions and Duty of information when the system is not in conformity.
  * [Article 23](https://artificialintelligenceact.eu/article/23). Providing the necessary information to competent authorities.
  * [Article 51](https://artificialintelligenceact.eu/article/51).
Setting up a Quality Management System (e.g. [based on ISO 42001](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-you/))
  * [Article 18](https://artificialintelligenceact.eu/article/18). Keeping documentation for 10 years at the disposal of the national competent authorities.
  * [Article 20](https://artificialintelligenceact.eu/article/20/). Storing the automatically generated logs for at least 6 months.
  * [Article 21](https://artificialintelligenceact.eu/article/21/). Corrective actions and Duty of information when the system is not in conformity.
  * [Article 23](https://artificialintelligenceact.eu/article/23). Providing the necessary information to competent authorities.
  * [Article 51](https://artificialintelligenceact.eu/article/51). Registering the high-risk model in the EU Database.

 **Deployers** of high-risk AI systems have lighter obligations, and will need
to comply with [Article 29](https://artificialintelligenceact.eu/article/29).
This includes exercising due diligence in using the system; monitoring and
logging the system in use; and, in specific conditions, carrying out [a
fundamental rights impact
assessment](https://artificialintelligenceact.eu/article/29a).

However, a Deployer, Importer, or Distributor is considered a Provider of a
high-risk AI system if they:

  * Change the name or a trademark of the AI system
  * Make a substantial modification to the AI system
  * Modify the intended purpose of a non-high-risk AI system already on the market, in such a manner that makes the AI system high-risk

If the initial Provider was overridden in the cases above, the initial
Provider is no longer subject to the usual obligations, but must provide “the
reasonably expected technical access” to the new Provider.

 **Importers** and **Distributors** are subject to the obligations in [Article
26](https://artificialintelligenceact.eu/article/26) and [Article
27](https://artificialintelligenceact.eu/article/27), respectively. They
include verifying that the provider has complied with their obligations, and
that the system has gone through the relevant conformity assessment and bears
the CE mark.

 **Authorized Representatives** are subject to the obligations in [Article
25](https://artificialintelligenceact.eu/article/25), which outline the
cooperation with relevant authorities and submission of relevant documents.

The following diagram can be useful to understand your entity’s (“operator’s”)
position in the AI system supply chain as defined by the AI Act. Note that
this is a simplified version that makes some assumptions; for example, that
the AI system’s output is located or used within the EU. We also recommend
using the interactive [AI Act Compliance
Checker](https://artificialintelligenceact.eu/assessment/eu-ai-act-compliance-
checker/) tool developed by the Future of Life Institute.

![](https://sp-
ao.shortpixel.ai/client/to_auto,q_glossy,ret_img,w_2090,h_3309/https://www.citadel.co.jp/en/wp-
content/uploads/sites/6/2024/02/AIA-flowchart-2.png)

## What are the requirements for different types of AI systems?

The Act defines several categories of AI systems with different obligations.
In principle, these categories are not mutually exclusive, and the same AI
system may fall under a mix of obligations. For example, a foundation model
that’s used in a medical device and directly interacts with end users might
need to fulfill High-Risk, GPAI, and Transparency obligations all at once.

### Prohibited AI

According to [Article 5](https://artificialintelligenceact.eu/article/5),
these use cases are considered unacceptable and will be prohibited, with some
exceptions for law enforcement activities. Examples: real-time biometric
identification, social scoring, manipulative techniques.

### High-Risk AI

These use cases are considered highly harmful if misused, and will need to go
through a third-party conformity assessment before being placed on the market.
According to [Article 6](https://artificialintelligenceact.eu/article/6), an
AI system is considered high-risk under either of the following conditions:

  1. It’s used as a safety component of a product, or is itself a product that’s already regulated by existing EU laws listed in Annex II, and is required to undergo a third-party conformity assessment under those Annex II laws; or,
  2. It falls under Annex III, unless it’s proved to not possess significant risk of harm to health, safety, or fundamental rights

We will now cover the two points above in more detail."
"## AI Act and Social Scoring

The AI Act takes a strong stance against AI systems that could be used for social scoring by governments. Such systems are categorized as having an unacceptable level of risk and are therefore strictly prohibited. 
","The AI Act takes a “risk-based” approach, scaling the stringency of
requirements based on the inherent risk of an AI system. You can learn more
about how this approach differs from other frameworks and standards in our
[previous blog post on the state of AI
regulation.](https://www.citadel.co.jp/en/blog/2023/03/20/an-overview-of-ai-
standardization-and-regulation-in-2023/)

As explained below, AI systems categorized as __ high-risk will feel the most
impact from this regulation. Developers of such high-risk AI systems must go
through a **third-party conformity assessment** in order to **obtain a CE mark
and access the EU market**.

## Do the requirements differ for developers and users of AI systems?

The most recent version of the AI Act [from Feb
2](https://artificialintelligenceact.eu/wp-content/uploads/2024/01/AI-Act-
FullText.pdf), approved by the Committee of Permanent Representatives,
provides additional information on the
[roles](https://artificialintelligenceact.eu/article/3) and [their
responsibilities](https://artificialintelligenceact.eu/article/28/) across the
AI Value Chain. The law defines six key roles:

  1.  **Provider**. An entity (e.g. company) that develops an AI system and “places them on the market or puts the system into service under its own name or trademark, whether for payment or free of charge”.
  2.  **Deployer**. An entity that uses an AI system under its authority, except in personal non-professional activities. Note that this does not refer to the affected end users.
  3.  **Importer**. An EU entity that places an non-EU AI system on the EU market.
  4.  **Distributor**. An entity in the supply chain, other than the provider or the importer, that makes an AI system available in the EU market.
  5.  **Authorized Representatives**. An EU entity that carries out obligations on behalf of a Provider.
  6.  **Operator.** An entity that falls into any of the five categories above.

> Operators could act in more than one role at the same time and should
> therefore fulfill cumulatively all relevant obligations associated with
> those roles. For example, an operator could act as a distributor and an
> importer at the same time.
>
> [Recital 56a](https://artificialintelligenceact.eu/recital/56a)

 **Providers** bear the majority of responsibilities under the AI Act.
Providers of high-risk AI systems will need to comply with [Article
16](https://artificialintelligenceact.eu/article/16/), and by extension:

  * [Article 9](https://artificialintelligenceact.eu/article/9). Implementing risk management processes.
  * [Article 10](https://artificialintelligenceact.eu/article/10). Data and Data Governance. Using high-quality training, validation and testing data.
  * [Article 11](https://artificialintelligenceact.eu/article/11). Establishing documentation as defined by [Annex IV](https://artificialintelligenceact.eu/annex/4).
  * [Article 12](https://artificialintelligenceact.eu/article/12). Implementing automatic logging.
  * [Article 13](https://artificialintelligenceact.eu/article/13). Ensuring an appropriate level of transparency with Deployers.
  * [Article 14](https://artificialintelligenceact.eu/article/14). Ensuring human oversight measures.
  * [Article 15](https://artificialintelligenceact.eu/article/15). Ensuring robustness, accuracy and cybersecurity.
  * [Article 17](https://artificialintelligenceact.eu/article/17). Setting up a Quality Management System (e.g. [based on ISO 42001](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-you/))
  * [Article 18](https://artificialintelligenceact.eu/article/18). Keeping documentation for 10 years at the disposal of the national competent authorities.
  * [Article 20](https://artificialintelligenceact.eu/article/20/). Storing the automatically generated logs for at least 6 months.
  * [Article 21](https://artificialintelligenceact.eu/article/21/). Corrective actions and Duty of information when the system is not in conformity.
  * [Article 23](https://artificialintelligenceact.eu/article/23). Providing the necessary information to competent authorities.
  * [Article 51](https://artificialintelligenceact.eu/article/51).
According to [Article 52c](https://artificialintelligenceact.eu/article/52c),
Providers of GPAI models must:

  * Draw up technical documentation, including training and testing process and evaluation results.
  * Enable other Providers that intend to integrate the GPAI model into their own AI system with sufficient documentation and information.
  * Put in place a policy to respect the EU copyright law.
  * Based on a template provided by the AI Office, publish a sufficiently detailed summary about the content used for training.
  * Develop and adhere to a code of practice

One of the reasons for the recent delays in the AI Act’s development was a
debate on the two-tiered categorization for GPAI models. Currently, models
that were trained using a total computing power of more than [10^25
FLOPs](https://artificialintelligenceact.eu/article/52a) are considered to
carry “ **systemic risk** ”. This places most models on the market, excluding
**GPT-4 and potentially Gemini Ultra** , in the non-systemic risk subcategory.
However, the threshold may be updated in future by the European Commission and
the AI Office as the technology progresses.

Providers of GPAI models with systemic risk are subject [to additional
obligations](https://artificialintelligenceact.eu/article/52d) to:

  * Perform model evaluations, including adversarial testing
  * Assess and mitigate possible systemic risks
  * Document and report issues to the AI office and relevant authorities

* * *

However, there are also major **exemptions** for Providers of AI systems for
research and academic use, as well as for AI models (including GPAI) that are
free and open-source. These models are not subject to any obligations until
they are placed on the market by a Provider as part of an AI system that has
obligations under the AI Act. Military and law enforcement uses are excluded
from the majority of obligations, but have some special considerations that we
will not cover.

You can learn more details about each risk category in the [official
FAQ](https://ec.europa.eu/commission/presscorner/detail/en/QANDA_21_1683)
published by the European Commission.

## What are the consequences of not complying with the EU AI Act?

As defined in [Article 71](https://artificialintelligenceact.eu/article/71/),
administrative fines act as the main penalty. The actual fine amount will be
decided on a case-by-case basis depending on the severity of the infringement
(based on rules to be further developed by individual EU Member states), but
the AI Act does outline maximum penalties for different types of non-
compliance:

  *  _Prohibited practices or non-compliance related to requirements on data:_ up to €35m or 7% of the total worldwide annual turnover
  *  _Non-compliance with any of the other requirements:_ up to €15m or 3% of the total worldwide annual turnover
  *  _Incorrect, incomplete or misleading information to notified bodies and national competent authorities:_ up to €7.5m or 1.5% of the total worldwide annual turnover

For each category of infringement, the threshold would be the lower of the two
amounts for small and medium-sized enterprises (including startups) and the
higher for other companies.

## What is the current legislative status and timeline?

The EU AI Act is not yet law, but it’s on a fast track to be published in the
Official Journal in early-mid 2024, and most of its requirements will
gradually be enforced in the following 24 months.

The[ timeline for
enforcement](https://artificialintelligenceact.eu/article/85) is as follows:

  1. After 6 months: Prohibition of AI applications of unacceptable risk becomes applicable.
  2. After 9 months: [Codes of practice](https://artificialintelligenceact.eu/recital/60t/) for GPAI providers must be prepared.
  3. After 12 months (approximately 2025 Q2~): Provisions on GPAI become applicable.
  4. After 24 months (approximately 2026 Q2~): Obligations for high-risk AI systems under Annex III become applicable. Each EU Member State is required to establish or participate in at least one regulatory sandbox.
  5. After 36 months: Obligations for high-risk AI systems under Annex II become applicable.

In practical terms, this means that the next two years will see a flurry of
activity in the field of AI auditing, both in government and in the private
sector. Relevant offices and systems will need to be set up in EU member
states; standards-making and certification bodies will be under extra pressure
to fill the gap and provide clear assessment frameworks."
"## Regulation of AI Systems that Manipulate Human Behavior

The increasing sophistication of AI systems, particularly those capable of generating synthetic content like deepfakes, presents significant challenges to the integrity of information and trust within society. The potential for misuse, including misinformation, manipulation, fraud, and impersonation, necessitates a regulatory framework to mitigate these risks.

One approach involves requiring providers of such AI systems to implement technical solutions for marking AI-generated content. This could involve embedding machine-readable markers like watermarks, metadata, or cryptographic signatures, enabling the identification of manipulated or synthetic content. The methods employed should be reliable, interoperable, and robust, considering the specific content type and current technological advancements.

However, it's important to maintain proportionality in regulation. AI systems primarily designed for standard editing assistance or those that don't significantly alter input data or its meaning may be exempt from these marking requirements. 
","Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

This Regulation regulates AI systems and models by imposing certain
requirements and obligations for relevant market actors that are placing them
on the market, putting into service or use in the Union, thereby complementing
obligations for providers of intermediary services that embed such systems or
models into their services regulated by Regulation (EU) 2022/2065. To the
extent that such systems or models are embedded into designated very large
online platforms or very large online search engines, they are subject to the
risk management framework provided for in Regulation (EU) 2022/2065.
Consequently, the corresponding obligations of the AI Act should be presumed
to be fulfilled, unless significant systemic risks not covered by Regulation
(EU) 2022/2065 emerge and are identified in such models. Within this
framework, providers of very large online platforms and very large search
engines are obliged to assess potential systemic risks stemming from the
design, functioning and use of their services, including how the design of
algorithmic systems used in the service may contribute to such risks, as well
as systemic risks stemming from potential misuses. Those providers are also
obliged to take appropriate mitigating measures in observance of fundamental
rights.

[ <- Previous ](https://artificialintelligenceact.eu/recital/117/) [ Next ->
](https://artificialintelligenceact.eu/recital/119/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

A variety of AI systems can generate large quantities of synthetic content
that becomes increasingly hard for humans to distinguish from human-generated
and authentic content. The wide availability and increasing capabilities of
those systems have a significant impact on the integrity and trust in the
information ecosystem, raising new risks of misinformation and manipulation at
scale, fraud, impersonation and consumer deception. In the light of those
impacts, the fast technological pace and the need for new methods and
techniques to trace origin of information, it is appropriate to require
providers of those systems to embed technical solutions that enable marking in
a machine readable format and detection that the output has been generated or
manipulated by an AI system and not a human. Such techniques and methods
should be sufficiently reliable, interoperable, effective and robust as far as
this is technically feasible, taking into account available techniques or a
combination of such techniques, such as watermarks, metadata identifications,
cryptographic methods for proving provenance and authenticity of content,
logging methods, fingerprints or other techniques, as may be appropriate. When
implementing this obligation, providers should also take into account the
specificities and the limitations of the different types of content and the
relevant technological and market developments in the field, as reflected in
the generally acknowledged state-of-the-art. Such techniques and methods can
be implemented at the level of the system or at the level of the model,
including general-purpose AI models generating content, thereby facilitating
fulfilment of this obligation by the downstream provider of the AI system. To
remain proportionate, it is appropriate to envisage that this marking
obligation should not cover AI systems performing primarily an assistive
function for standard editing or AI systems not substantially altering the
input data provided by the deployer or the semantics thereof.

[ <- Previous ](https://artificialintelligenceact.eu/recital/132/) [ Next ->
](https://artificialintelligenceact.eu/recital/134/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"There are two main categories of AI applications that are considered high-risk. The first category includes AI systems that are used as safety components in products or are themselves products that are already regulated by existing EU laws. Examples of such products include machinery, toys, medical devices, and vehicles. The second category includes AI systems that are listed in Annex III of the EU AI Act, unless it can be proven that they do not pose a significant risk of harm to health, safety, or fundamental rights. Examples of such AI systems include those used for biometric identification, critical infrastructure management, and education and vocational training. 
","Examples: real-time biometric
identification, social scoring, manipulative techniques.

### High-Risk AI

These use cases are considered highly harmful if misused, and will need to go
through a third-party conformity assessment before being placed on the market.
According to [Article 6](https://artificialintelligenceact.eu/article/6), an
AI system is considered high-risk under either of the following conditions:

  1. It’s used as a safety component of a product, or is itself a product that’s already regulated by existing EU laws listed in Annex II, and is required to undergo a third-party conformity assessment under those Annex II laws; or,
  2. It falls under Annex III, unless it’s proved to not possess significant risk of harm to health, safety, or fundamental rights

We will now cover the two points above in more detail.

* * *

One of the major points of deliberation during the AI Act’s development was
alignment with existing EU harmonisation legislations that regulate existing
product categories. This is covered by Annex II.

 **[Annex II](https://artificialintelligenceact.eu/annex/2), Section A: List
of Union Harmonisation Legislation Based on the New Legislative Framework**

Includes AI systems, or the products for which the AI system is a safety
component, that fall under the following categories. As per [Article 28
2(a)](https://artificialintelligenceact.eu/article/28/), the manufacturer of
such products is considered the Provider of the included high-risk AI system
if it’s placed on the market under the manufacturer’s name or trademark:

  * Machinery
  * Toys
  * Recreational craft and personal watercraft
  * Lifts
  * Equipment and protective systems intended for use in potentially explosive atmospheres
  * Radio equipment
  * Pressure equipment
  * Cableway installations
  * Personal protective equipment
  * Appliances burning gaseous fuels
  * Medical devices
  * In vitro diagnostic medical devices

According to [Article 6(1)](https://artificialintelligenceact.eu/article/6),
if an AI system is covered by the above categories, and already needs to go
through third-party assessment under the relevant legislation, it’s considered
high-risk – with all the relevant obligations applicable to the Provider.

However, the exact interplay between the AI Act and existing regulations, e.g.
the Medical Devices Regulation (MDR), is [not yet
finalized](https://www.emergobyul.com/news/effect-europes-artificial-
intelligence-act-medical-device-industry) and remains a [point of
discussion](https://haiweb.org/wp-content/uploads/2023/03/MDR-
AIAct_OnePager_FINAL.pdf). Obligations for AI systems in Annex II will be
enforced a year later than other high-risk systems to provide sufficient time
to make the requirements compatible.

[ **Annex II**](https://artificialintelligenceact.eu/annex/2) **, Section B:
Other devices that fall under the Union Harmonisation Legislation**

Includes AI systems, or the product for which the AI system is a safety
component, that fall under the following categories:

  * Civil aviation security
  * Two- or three-wheel vehicles and quadricycles
  * Agricultural and forestry vehicles
  * Marine equipment
  * Interoperability of the rail system
  * Motor vehicles and their trailers
  * Civil aviation

According to [Article 2(2)](https://artificialintelligenceact.eu/article/2),
if a high-risk AI system is covered by the above categories, and already needs
to go through third-party assessment under the relevant legislation, it’s
categorized as high-risk. However, unlike for use cases under Annex II Section
A above, **only** **[Article
84](https://artificialintelligenceact.eu/article/84) shall apply.** The
Provider is only obligated to comply with the existing harmonized legislation,
and is exempt from the regular high-risk AI system rules.

* * *

[ **Annex III**](https://artificialintelligenceact.eu/annex/3) provides the
list of other high-risk use cases that are not covered by other existing
harmonization legislation:

  * Biometrics (ones permitted under relevant Union or national law) 
  * Critical infrastructure (e.g. safety component in water, gas, heating or electricity infrastructure)
  * Education and vocational training (e.g. determining admission to training institutions)
  * Employment, workers management and access to self-employment (e.g. monitoring and evaluating performance; targeted job ads)
  * Access to and enjoyment of essential public and private services (e.g.
It is uncertain what share of AI systems will be in the high-risk category, as
both the field of AI and the law are still evolving. The European Commission
estimated in an [impact study](https://digital-
strategy.ec.europa.eu/en/library/impact-assessment-regulation-artificial-
intelligence) that only 5-15% of applications would be subject to stricter
rules. A [study](https://aai.frb.io/assets/files/AI-Act-Risk-Classification-
Study-appliedAI-March-2023.pdf) by appliedAI of 106 enterprise AI systems
found that 18% were high-risk, 42% low-risk, and 40% unclear if high or low
risk. The system with unclear risk classification in this study were mainly
within the areas of critical infrastructure, employment, law enforcement, and
product safety. In a related [survey](https://aai.frb.io/assets/files/AI-Act-
Impact-Survey_Report_Dec12.2022.pdf) of 113 EU AI startups, 33% of the
startups surveyed believed that their AI systems would be classified as high-
risk, compared to the 5-15% assessed by the European Commission.



#### What are the penalties of the EU AI Act?

Based on the European Parliament’s adopted position, using prohibited AI
practices outlined in Article 5 can result in fines of up to €40 million, or
7% of worldwide annual turnover – whichever is higher. Not complying with the
data governance requirements (under Article 10) and the requirements for
transparency and provision of information (under Articles 13) can lead to
fines up to €20 million, or 4% of worldwide turnover if that is higher. Non-
compliance with any other requirements or obligations can result in fines of
up to €10 million or 2% of worldwide turnover – again, whichever is higher.
Fines are tiered depending on the provision violated, with prohibited
practices receiving the highest penalties and other violations receiving lower
maximum fines.



#### What measures does the EU AI Act implement for SMEs specifically?

The EU AI Act aims to provide support for SMEs and startups as specified in
Article 55. Specific measures include granting priority access for SMEs and
EU-based startups to regulatory sandboxes if eligibility criteria are met.
Additionally, tailored awareness raising and digital skills development
activities will be organised to address the needs of smaller organisations.
Moreover, dedicated communication channels will be set up to offer guidance
and respond to queries from SMEs and startups. Participation of SMEs and other
stakeholders in the standards development process will also be encouraged. To
reduce the financial burden of compliance, conformity assessment fees will be
lowered for SMEs and startups based on factors like development stage, size,
and market demand. The Commission will regularly review certification and
compliance costs for SMEs/startups (with input from transparent consultations)
and work with Member States to reduce these costs where possible.



#### Can I voluntarily comply with the EU AI Act even if my system is not in
scope?

Yes! The Commission, the AI Office, and/or Member States will encourage
voluntary codes of conduct for requirements under Title III, Chapter 2 (e.g.
risk management, data governance, and human oversight) for AI systems not
deemed to be high-risk. These codes of conduct will provide technical
solutions for how an AI system can meet the requirements, according to the
system’s intended purpose. Other objectives like environmental sustainability,
accessibility, stakeholder participation, and diversity of development teams
will be considered. Small businesses and startups will be taken into account
when encouraging codes of conduct. See Article 69 on Codes of Conduct for
Voluntary Application of Specific Requirements.



## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"The provided information focuses on the EU AI Act and does not offer insights into the NIST AI RMF or its recommendations for the frequency of AI risk assessment reviews and updates. 
","49 6. Re-analy sis of conditions and risks  
(1) Re-implementing Action Targets 1 -1 to 1 -3 in a timely manner  
Action Target 6 -1: Companies that develop and operate AI systems should, under the leadership 
of top management, conduct re -evaluations, update their understanding, obtain new points of 
views, or take other relevant actions with respect to Action Targets 1 -1 through 1.3, in a timely 
manner. When implementing Action Target 5 -2, they should also consider obtaining opinions not 
only for the current AI management system and the operation of such system, but also conducive 
to review of entire AI governance, incl uding analyses of conditions and risks.  
 
[Practical Example 1]  
Because, as of the present time, a standard perception of AI systems has yet to form in our societies, 
we should re -evaluate the positive and negative impacts that AI systems can have, societies’ 
acceptance of AI system development and operation, and our o wn AI proficiency as described in 
Action Targets 1 -1 through 1 -3, as well as update our understanding and acquire new perspectives in 
a timely manner. We regularly perform analyses of conditions and risks, and report to 
management even during normal times where we do not see any “near misses,” or a 
significant increase in public attention to specific incidents, or changes in the regulatory 
environment . While there is an active ongoing debate over how to properly develop and operate AI 
systems, we place emph asis on avoiding governance fatigue by haphazard agile re -analyses and 
using agile re -analyses to understand major trends. Opportunities to report to management are good 
opportunities for directing our attention to major trends.  
 
[Practical Example 2]  
We r egularly analyze conditions and risks as described in Practical Example 1, but because 
verifications of AI governance and AI management systems have overlapping elements, we include 
the positive and negative impacts of AI systems and social acceptance of A I system 
development and operation in the agenda of the AI Ethics Committee meetings that we hold 
regularly with invited external experts to gain their insights into the major trends associated 
with these points .
7\. The testing of the high-risk AI systems shall be performed, as
appropriate, at any point in time throughout the development process, and, in
any event, prior to the placing on the market or the putting into service.
Testing shall be made against prior defined metrics and probabilistic
thresholds that are appropriate to the intended purpose of the high-risk AI
system.

8\. When implementing the risk management system described in paragraphs 1 to
6, providers shall give consideration to whether in view of its intended
purpose the high-risk AI system is likely to adversely impact persons under
the age of 18 and, as appropriate, other vulnerable groups of people.

9\. For providers of high-risk AI systems that are subject to requirements
regarding internal risk management processes under relevant sectorial Union
law, the aspects described in paragraphs 1 to 8 may be part of or combined
with the risk management procedures established pursuant to that law.

[ <- Previous ](https://artificialintelligenceact.eu/article/8/) [ Next ->
](https://artificialintelligenceact.eu/article/10/)

#### Suitable Recitals

[

Recital 47

](https://artificialintelligenceact.eu/recital/47/) [

Recital 64

](https://artificialintelligenceact.eu/recital/64/) [

Recital 66

](https://artificialintelligenceact.eu/recital/66/) [

Recital 67

](https://artificialintelligenceact.eu/recital/67/) [

Recital 69

](https://artificialintelligenceact.eu/recital/69/) [

Recital 71

](https://artificialintelligenceact.eu/recital/71/) [

Recital 81

](https://artificialintelligenceact.eu/recital/81/) [

Recital 138

](https://artificialintelligenceact.eu/recital/138/) [

Recital 155

](https://artificialintelligenceact.eu/recital/155/) [

Recital 158

](https://artificialintelligenceact.eu/recital/158/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"There are two primary situations where an AI system would be classified as high-risk. The first is when the system is a safety component of a product or is itself a product that is already subject to existing EU regulations and requires a third-party conformity assessment. The second situation is when the AI system falls under a specific list of use cases, unless it can be demonstrated that it does not pose a significant risk to health, safety, or fundamental rights. 
","Examples: real-time biometric
identification, social scoring, manipulative techniques.

### High-Risk AI

These use cases are considered highly harmful if misused, and will need to go
through a third-party conformity assessment before being placed on the market.
According to [Article 6](https://artificialintelligenceact.eu/article/6), an
AI system is considered high-risk under either of the following conditions:

  1. It’s used as a safety component of a product, or is itself a product that’s already regulated by existing EU laws listed in Annex II, and is required to undergo a third-party conformity assessment under those Annex II laws; or,
  2. It falls under Annex III, unless it’s proved to not possess significant risk of harm to health, safety, or fundamental rights

We will now cover the two points above in more detail.

* * *

One of the major points of deliberation during the AI Act’s development was
alignment with existing EU harmonisation legislations that regulate existing
product categories. This is covered by Annex II.

 **[Annex II](https://artificialintelligenceact.eu/annex/2), Section A: List
of Union Harmonisation Legislation Based on the New Legislative Framework**

Includes AI systems, or the products for which the AI system is a safety
component, that fall under the following categories. As per [Article 28
2(a)](https://artificialintelligenceact.eu/article/28/), the manufacturer of
such products is considered the Provider of the included high-risk AI system
if it’s placed on the market under the manufacturer’s name or trademark:

  * Machinery
  * Toys
  * Recreational craft and personal watercraft
  * Lifts
  * Equipment and protective systems intended for use in potentially explosive atmospheres
  * Radio equipment
  * Pressure equipment
  * Cableway installations
  * Personal protective equipment
  * Appliances burning gaseous fuels
  * Medical devices
  * In vitro diagnostic medical devices

According to [Article 6(1)](https://artificialintelligenceact.eu/article/6),
if an AI system is covered by the above categories, and already needs to go
through third-party assessment under the relevant legislation, it’s considered
high-risk – with all the relevant obligations applicable to the Provider.

However, the exact interplay between the AI Act and existing regulations, e.g.
the Medical Devices Regulation (MDR), is [not yet
finalized](https://www.emergobyul.com/news/effect-europes-artificial-
intelligence-act-medical-device-industry) and remains a [point of
discussion](https://haiweb.org/wp-content/uploads/2023/03/MDR-
AIAct_OnePager_FINAL.pdf). Obligations for AI systems in Annex II will be
enforced a year later than other high-risk systems to provide sufficient time
to make the requirements compatible.

[ **Annex II**](https://artificialintelligenceact.eu/annex/2) **, Section B:
Other devices that fall under the Union Harmonisation Legislation**

Includes AI systems, or the product for which the AI system is a safety
component, that fall under the following categories:

  * Civil aviation security
  * Two- or three-wheel vehicles and quadricycles
  * Agricultural and forestry vehicles
  * Marine equipment
  * Interoperability of the rail system
  * Motor vehicles and their trailers
  * Civil aviation

According to [Article 2(2)](https://artificialintelligenceact.eu/article/2),
if a high-risk AI system is covered by the above categories, and already needs
to go through third-party assessment under the relevant legislation, it’s
categorized as high-risk. However, unlike for use cases under Annex II Section
A above, **only** **[Article
84](https://artificialintelligenceact.eu/article/84) shall apply.** The
Provider is only obligated to comply with the existing harmonized legislation,
and is exempt from the regular high-risk AI system rules.

* * *

[ **Annex III**](https://artificialintelligenceact.eu/annex/3) provides the
list of other high-risk use cases that are not covered by other existing
harmonization legislation:

  * Biometrics (ones permitted under relevant Union or national law) 
  * Critical infrastructure (e.g. safety component in water, gas, heating or electricity infrastructure)
  * Education and vocational training (e.g. determining admission to training institutions)
  * Employment, workers management and access to self-employment (e.g. monitoring and evaluating performance; targeted job ads)
  * Access to and enjoyment of essential public and private services (e.g.
eu/annex/12/) [

Annex XIII: Criteria for the designation of general purpose AI models with
systemic risk referred to in Article 51

](https://artificialintelligenceact.eu/annex/13/)

## Search within the Act

Search for:

⚠️ The AI Act Explorer is currently being updated to reflect the texts adopted
by the European Parliament on 13 March 2024. Internal links to other parts of
the Act may not work as expected.

#### article

Part of [Section 1: Classification of AI Systems as High-
Risk](https://artificialintelligenceact.eu/chapter/3-1/)

# Article 6: Classification Rules for High-Risk AI Systems

**Feedback** – We are working to improve this tool. Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

1\. Irrespective of whether an AI system is placed on the market or put into
service independently from the products referred to in points (a) and (b),
that AI system shall be considered high-risk where both of the following
conditions are fulfilled:

(a) the AI system is intended to be used as a safety component of a product,
or the AI system is itself a product, covered by the Union harmonisation
legislation listed in [Annex
I](https://artificialintelligenceact.eu/annex/1/);

(b) the product whose safety component pursuant to point (a) is the AI system,
or the AI system itself as a product, is required to undergo a third party
conformity assessment, with a view to the placing on the market or putting
into service of that product pursuant to the Union harmonisation legislation
listed in [Annex I](https://artificialintelligenceact.eu/annex/1/).

2\. In addition to the high-risk AI systems referred to in paragraph 1, AI
systems referred to in [Annex
III](https://artificialintelligenceact.eu/annex/3/) shall also be considered
high-risk.

2a. By derogation from paragraph 2 AI systems shall not be considered as high
risk if they do not pose a significant risk of harm, to the health, safety or
fundamental rights of natural persons, including by not materially influencing
the outcome of decision making. This shall be the case if one or more of the
following criteria are fulfilled:

(a) the AI system is intended to perform a narrow procedural task;

(b) the AI system is intended to improve the result of a previously completed
human activity;

(c) the AI system is intended to detect decision-making patterns or deviations
from prior decision-making patterns and is not meant to replace or influence
the previously completed human assessment, without proper human review; or

(d) the AI system is intended to perform a preparatory task to an assessment
relevant for the purpose of the use cases listed in [Annex
III](https://artificialintelligenceact.eu/annex/3/). Notwithstanding first
subparagraph of this paragraph, an AI system shall always be considered high-
risk if the AI system performs profiling of natural persons.

2b. A provider who considers that an AI system referred to in [Annex
III](https://artificialintelligenceact.eu/annex/3/) is not high-risk shall
document its assessment before that system is placed on the market or put into
service. Such provider shall be subject to the registration obligation set out
in [Article 49](https://artificialintelligenceact.eu/article/49/)(1a). Upon
request of national competent authorities, the provider shall provide the
documentation of the assessment.

2c. The Commission shall, after consulting the AI Board, and no later than [18
months] after the entry into force of this Regulation, provide guidelines
specifying the practical implementation of this article completed by a
comprehensive list of practical examples of high risk and non-high risk use
cases on AI systems pursuant to [Article
82b](https://artificialintelligenceact.eu/article/82b/).

2d. The Commission is empowered to adopt delegated acts in accordance with
[Article 97](https://artificialintelligenceact.eu/article/97/) to amend the
criteria laid down in points a) to d) of the first subparagraph of paragraph
2a."
"## Security Considerations in the Provided Information

While the text doesn't explicitly mention the NIST AI RMF, it does highlight the importance of cybersecurity for high-risk AI systems.  It emphasizes the need to consider risks like data poisoning and adversarial attacks, which are directly relevant to AI security.  Additionally, it mentions the role of conformity assessments in ensuring that AI systems meet essential cybersecurity requirements. 
","Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

Without prejudice to the requirements related to robustness and accuracy set
out in this Regulation, high-risk AI systems which fall within the scope of
the Regulation 2022/0272, in accordance with Article 8 of the the Regulation
2022/0272 may demonstrate compliance with the cybersecurity requirement of
this Regulation by fulfilling the essential cybersecurity requirements set out
in Article 10 and Annex I of the Regulation 2022/0272.When high-risk AI
systems fulfil the essential requirements of Regulation 2022/0272, they should
be deemed compliant with the cybersecurity requirements set out in this
Regulation in so far as the achievement of those requirements is demonstrated
in the EU declaration of conformity or parts thereof issued under Regulation
2022/0272. For this purpose, the assessment of the cybersecurity risks,
associated to a product with digital elements classified as high-risk AI
system according to this Regulation, carried out under Regulation 2022/0272,
should consider risks to the cyber resilience of an AI system as regards
attempts by unauthorised third parties to alter its use, behaviour or
performance, including AI specific vulnerabilities such as data poisoning or
adversarial attacks, as well as, as relevant, risks to fundamental rights as
required by this Regulation. The conformity assessment procedure provided by
this Regulation should apply in relation to the essential cybersecurity
requirements of a product with digital elements covered by Regulation
2022/0272 and classified as a high-risk AI system under this Regulation.
However, this rule should not result in reducing the necessary level of
assurance for critical products with digital elements covered by Regulation
2022/0272. Therefore, by way of derogation from this rule, high-risk AI
systems that fall within the scope of this Regulation and are also qualified
as important and critical products with digital elements pursuant to
Regulation 2022/0272 and to which the conformity assessment procedure based on
internal control referred to in [Annex
VI](https://artificialintelligenceact.eu/annex/6/) of this Regulation applies,
are subject to the conformity assessment provisions of Regulation 2022/0272
insofar as the essential cybersecurity requirements of Regulation 2022/0272
are concerned. In this case, for all the other aspects covered by this
Regulation the respective provisions on conformity assessment based on
internal control set out in [Annex
VI](https://artificialintelligenceact.eu/annex/6/) of this Regulation should
apply. Building on the knowledge and expertise of ENISA on the cybersecurity
policy and tasks assigned to ENISA under the Regulation 2019/1020 the European
Commission should cooperate with ENISA on issues related to cybersecurity of
AI systems.

[ <- Previous ](https://artificialintelligenceact.eu/recital/76/) [ Next ->
](https://artificialintelligenceact.eu/recital/78/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
It requires developers to implement a standardized evaluation protocol for AI models and datasets.
  * B.6.2.8 AI system recording of event logs. It requires developers to have a logging system to record production input data, output predictions, and potential outliers.
  * B.7.4 Quality of data for AI systems. It requires developers to define and measure training and production data quality, as well as consider impact of data bias on performance.

(If you’re interested in a solution to automate the requirements above,
consider using [Citadel Lens](https://www.citadel.co.jp/en/products/lens/) and
[Citadel Radar](https://www.citadel.co.jp/en/products/radar/).)

###### Organization-level Requirements

These requirements apply to the whole organization and its processes:

  * B.2.2 AI policy
  * B.2.3 Alignment with other organizational policies
  * B.2.4 Review of the AI policy
  * B.3.2 AI roles and responsibilities
  * B.3.3 Reporting of concerns
  * B.5.2 AI system impact assessment process
  * B.6.1.2 Objectives for responsible development of AI system
  * B.6.1.3 Processes for responsible design and development of AI systems
  * B.7.2 Data for development and enhancement of AI system
  * B.7.3 Acquisition of data
  * B.8.3 External reporting
  * B.8.4 Communication of incidents
  * B.8.5 Information for interested parties
  * B.9.2 Processes for responsible use of AI
  * B.9.3 Objectives for responsible use of AI system
  * B.10.2 Allocating responsibilities
  * B.10.3 Suppliers
  * B.10.4 Customers

###### Project-level Requirements

These requirements apply to specific projects within the organization:

  * B.4.2 Resource documentation
  * B.4.3 Data resources
  * B.4.4 Tooling resources
  * B.4.5 System and computing resources
  * B.4.6 Human resources
  * B.5.3 Documentation of AI system impact assessments
  * B.5.4 Assessing AI system impact on individuals and groups of individuals
  * B.5.5 Assessing societal impacts of AI systems
  * B.6.2.2 AI system requirements and specification
  * B.6.2.3 Documentation of AI system design and development
  * B.6.2.4 AI system verification and validation
  * B.6.2.5 AI system deployment
  * B.6.2.6 AI system operation and monitoring
  * B.6.2.7 AI system technical documentation
  * B.6.2.8 AI system recording of event logs
  * B.7.4 Quality of data for AI systems
  * B.7.5 Data provenance
  * B.7.6 Data preparation
  * B.8.2 System documentation and information for users
  * B.9.3 Objectives for responsible use of AI system
  * B.9.4 Intended use of the AI system

### Does ISO 42001 cover all technical aspects of various AI processes?

ISO 42001 sits at the center of the responsible AI standards ecosystem, which
includes other standards that go deeper into specific aspects of AI
management, use, and development. The goal of ISO 42001 is to maintain a
balance between being prescriptive in terms of the requirements for an AIMS,
but flexible in terms of how AIMS is implemented for a particular
organization.

Since ISO 42001 itself **does not address the details of specific AI
applications** , it outsources the more technical details to more narrowly
focused standards and other ""generally accepted frameworks"".  Some examples of
other standards referenced in ISO 42001 are:

  * [ISO 5259](https://www.iso.org/standard/81860.html) series on Data Quality
  * [ISO 23894](https://www.iso.org/standard/77304.html) AI Risk Management
  * [ISO 24029-1](https://www.iso.org/standard/77609.html) Assessment of the robustness of neural networks

### What is the process for ISO 42001 certification?

To obtain an ISO 42001 certification, an organization must successfully pass
an audit conducted by a certification body. However, the [ISO
42006](https://www.iso.org/standard/44546.html) standard defining the
requirements for such bodies is currently under development, which means that
**as of January 2024, no certifications can be granted yet**."
"The provided information focuses on the development and management of AI systems, with a strong emphasis on security and ethical considerations. It doesn't specifically address deployment guidelines. 
","Machine Learning Quality Management Guideline,   National Institute of  
3rd English edition    Advanced Industrial Science and Technology  
 
191 
  
Table 11: Security considerations 
Chapter and 
section  Guideline description  Security response plan Remarks  
1.3.2  Lifetime -long requirements 
for risk assessment  At the time of security risk assessment, the result s of the functional 
safety risk assessment  and the items to be treated with priority are 
included in the targets of assumed attacks. Include items such as privacy and fairness 
that related  to the target business for which 
the system will be used. 
1.3.3  when a machine leaning based  
system is developed by 
sharing works  When referring to terms  and guidelines related to AI and machine 
learning, include compliance with referenced standards and guidelines 
for the entire supply chain in the contract. Machine Learning Quality Management 
Guidelines, ISO/IEC regulations, etc.  
 GDPR, China Cyber Security Law, etc. 
depending on the business to be designed 
and developed.  
 Also refer to laws related to outsourcing, 
such as  the Dispatched Worker Law and the 
Subcontracting Law.  
1.3.3  Security risk of contamination 
of learning  results due to 
intentional inclusion of 
improper  data  ● Collect information on attacks specializing in AI and machine 
learning, such as evasion atta cks, data poisoning attacks, and model 
poisoning attacks exemplified in Chapter 9 of the Machine Learning Quality Management Guidelines, analyze attack scenarios,  and identify 
threats and vulnerabilities. implement measures for  
● Collect information on attack and defense methods and update 
defense measures periodically . The cycle of review of overall defense 
measures should be within one year. 
 Defense measures should be prioritized 
and implemented from the most feasible 
items.
31 Management Agency have created , based on these Guidelines, a format for recording details of the 
implementation of reliability assessments in the area of plant security. In addition, we also learned 
that model cards are proposed with the belief that AI models should have labelling abou t their 
performance just as ingredient labelling on food products help consumers make responsible choices20. 
At the moment, although there is no standard documenting procedure for sharing, performance and 
quality information of trained machine learning mod els, etc . between plural companies , we intend to 
refer to various other efforts instead of considering our own standards from scratch in 
establishing our in -house system . 
 
[Practical Example 2]  
We belong to an organization engaged in AI ethics and quality,  and actively exchange views 
with other affiliated companies on best practices for providing information on AI system’s 
performance and other attributes . AI system users should be provided with sufficient information 
about AI systems, but because not all u sers, whether they are consumers or other users, are 
knowledgeable about details of characteristics and limitations of AI, it is not appropriate to think that 
it is enough merely to give AI system users  information that is difficult for anyone other than e xperts 
to understand or huge amounts of detailed information unilaterally . In order to consider appropriate 
ways of providing information, it is important for companies to not only have direct user experience of 
their own, but also indirect contact with la rge numbers of users through exchanges of views with other 
companies.  
Information that AI system developers should give to AI system operators includes, for example, 
information about the data used to develop their AI systems. For example, this may include  
information on the source of data (which may sometimes be open data), the amount of data and their 
distribution, and overviews for each of the categories covered in the data. It is also important to 
describe the algorithms chosen (or not chosen) during de velopment, provide an overview of the 
generated model, and in particular, to explain the conditions under which tests were performed and 
how much accuracy was attained.  
While these perspectives are not new to companies with rich experience in developing an d operating 
AI systems, we believe that “how we communicate ” is important;  that is, types of information we 
should provide and how in -depth our explanations should be.  Understanding the current state 
of information sharing among plural companies is important in considering the overall design 
of AI governance , and herein lies the meaningfulness of participating in organizations that focus on 
AI ethics and quality.  
 
  
 
20 Google, “The value of a shared understanding of AI models,” https://modelcards.withgoogle.com/about ."
"AI systems used for military, defense, or national security purposes are not covered by the regulations outlined. This exclusion is due to the specialized nature of these sectors and the existing international laws governing them. 
","eu/annex/13/)

## Search within the Act

Search for:

⚠️ The AI Act Explorer is currently being updated to reflect the texts adopted
by the European Parliament on 13 March 2024. Internal links to other parts of
the Act may not work as expected.

#### recital

# Recital 24

**Feedback** – We are working to improve this tool. Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

If and insofar AI systems are placed on the market, put into service, or used
with or without modification of such systems for military, defence or national
security purposes, those should be excluded from the scope of this Regulation
regardless of which type of entity is carrying out those activities, such as
whether it is a public or private entity. As regards military and defence
purposes, such exclusion is justified both by Article 4(2) TEU and by the
specifities of the Member States’ and the common Union defence policy covered
by Chapter 2 of Title V of the Treaty on European Union (TEU) that are subject
to public international law, which is therefore the more appropriate legal
framework for the regulation of AI systems in the context of the use of lethal
force and other AI systems in the context of military and defence activities.
As regards national security purposes, the exclusion is justified both by the
fact that national security remains the sole responsibility of Member States
in accordance with Article 4(2) TEU and by the specific nature and operational
needs of national security activities and specific national rules applicable
to those activities. Nonetheless, if an AI system developed, placed on the
market, put into service or used for military, defence or national security
purposes is used outside those temporarily or permanently for other purposes
(for example, civilian or humanitarian purposes, law enforcement or public
security purposes), such a system would fall within the scope of this
Regulation. In that case, the entity using the system for other than military,
defence or national security purposes should ensure compliance of the system
with this Regulation, unless the system is already compliant with this
Regulation. AI systems placed on the market or put into service for an
excluded (i.e. military, defence or national security) and one or more non
excluded purposes (e.g. civilian purposes, law enforcement, etc.), fall within
the scope of this Regulation and providers of those systems should ensure
compliance with this Regulation. In those cases, the fact that an AI system
may fall within the scope of this Regulation should not affect the possibility
of entities carrying out national security, defence and military activities,
regardless of the type of entity carrying out those activities, to use AI
systems for national security, military and defence purposes, the use of which
is excluded from the scope of this Regulation. An AI system placed on the
market for civilian or law enforcement purposes which is used with or without
modification for military, defence or national security purposes should not
fall within the scope of this Regulation, regardless of the type of entity
carrying out those activities.

[ <- Previous ](https://artificialintelligenceact.eu/recital/23/) [ Next ->
](https://artificialintelligenceact.eu/recital/25/)

#### This Recital relates to

[Article 2: Scope](https://artificialintelligenceact.eu/article/2/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
Depending on the
type of AI system, the use of the system may affect persons other than the
deployer.

[ <- Previous ](https://artificialintelligenceact.eu/recital/12/) [ Next ->
](https://artificialintelligenceact.eu/recital/14/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"The reach of the regulations extends beyond the European Union's borders, encompassing AI systems developed externally but with intended application within the EU. This measure aims to prevent the circumvention of the regulations and ensure comprehensive protection for individuals residing in the EU. 
","Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

In light of their digital nature, certain AI systems should fall within the
scope of this Regulation even when they are neither placed on the market, nor
put into service, nor used in the Union. This is the case for example of an
operator established in the Union that contracts certain services to an
operator established outside the Union in relation to an activity to be
performed by an AI system that would qualify as high-risk. In those
circumstances, the AI system used by the operator outside the Union could
process data lawfully collected in and transferred from the Union, and provide
to the contracting operator in the Union the output of that AI system
resulting from that processing, without that AI system being placed on the
market, put into service or used in the Union. To prevent the circumvention of
this Regulation and to ensure an effective protection of natural persons
located in the Union, this Regulation should also apply to providers and
deployers of AI systems that are established in a third country, to the extent
the output produced by those systems is intended to be used in the Union.
Nonetheless, to take into account existing arrangements and special needs for
future cooperation with foreign partners with whom information and evidence is
exchanged, this Regulation should not apply to public authorities of a third
country and international organisations when acting in the framework of
cooperation or international agreements concluded at national or European
level for law enforcement and judicial cooperation with the Union or with its
Member States, under the condition that this third country or international
organisations provide adequate safeguards with respect to the protection of
fundamental rights and freedoms of individuals. Where relevant, this may also
cover activities of entities entrusted by the third countries to carry out
specific tasks in support of such law enforcement and judicial cooperation.
Such framework for cooperation or agreements have been established bilaterally
between Member States and third countries or between the European Union,
Europol and other EU agencies and third countries and international
organisations. The authorities competent for supervision of the law
enforcement and judicial authorities under the AI Act should assess whether
these frameworks for cooperation or international agreements include adequate
safeguards with respect to the protection of fundamental rights and freedoms
of individuals. Recipient Member States authorities and Union institutions,
offices and bodies making use of such outputs in the Union remain accountable
to ensure their use complies with Union law. When those international
agreements are revised or new ones are concluded in the future, the
contracting parties should undertake the utmost effort to align those
agreements with the requirements of this Regulation.

[ <- Previous ](https://artificialintelligenceact.eu/recital/21/) [ Next ->
](https://artificialintelligenceact.eu/recital/23/)

#### This Recital relates to

[Article 2: Scope](https://artificialintelligenceact.eu/article/2/)

[Article 50: Transparency Obligations for Providers and Users of Certain AI
Systems and GPAI Models](https://artificialintelligenceact.eu/article/50/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
The
EU’s AI regulation is already making waves internationally. In late September
2021, Brazil’s Congress [passed](https://politico.us8.list-
manage.com/track/click?u=e26c1a1c392386a968d02fdbc&id=edf7623d97&e=e4d8507e94)
a bill that creates a legal framework for artificial intelligence. It still
needs to pass the country's Senate.

![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/robot.webp)

## How can organisations apply it?

We have developed a new tool at FLI to help European SMEs and startups better
understand whether they might have any legal obligations under the EU AI Act
or whether they may implement the Act solely to make their business stand out
as more trustworthy. Please note that the Act is still in negotiations, and
our tool is a simplification. There are three positions with different
proposals for the Act currently available and we were selective to make the
tool more user-friendly. This tool can help give an indication about what
obligations your system might face. This tool is still a work in progress.
Please use the tool [here](https://artificialintelligenceact.eu/assessment/eu-
ai-act-compliance-checker/).

![](https://artificialintelligenceact.eu/wp-content/uploads/2021/08/document-
scaled.jpeg)

## Articles on the AI Act

####  [The AI Office is hiring](https://artificialintelligenceact.eu/the-ai-
office-is-hiring/)

Mar 22, 2024

The European Commission is recruiting contract agents who are AI technology
specialists to govern the most cutting-edge AI models.  Deadline to apply is
12:00 CET on 27 March (application form).  Role This is an opportunity to work
in a team within the...

####  [The AI Office: What is it, and how does it
work?](https://artificialintelligenceact.eu/the-ai-office-summary/)

Mar 21, 2024

In this overview, we offer a summary of the key elements of the AI Office
relevant for those interested in AI governance. We’ve highlighted the
responsibilities of the AI Office, its role within the European Commission,
its relationship with the AI Board, its national...

####  [AI Act Implementation: Timelines & Next
steps](https://artificialintelligenceact.eu/ai-act-implementation-next-steps/)

Feb 28, 2024

In this article we provide an outline of the key dates relevant to the
implementation of the AI Act. We also list some secondary legislation that the
Commission might add to supplement the AI Act, and some guidelines it may
publish to support compliance efforts. The...

####  [High-level summary of the AI
Act](https://artificialintelligenceact.eu/high-level-summary/)

Feb 27, 2024

Everything you need to know about the AI Act in ten minutes.

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"While the EU AI Act primarily focuses on high-risk AI systems, it also encourages the voluntary adoption of specific requirements for low-risk applications through codes of conduct. These codes of conduct are designed to offer technical solutions for meeting the requirements while considering factors like environmental sustainability and stakeholder participation.  Smaller businesses and startups are specifically taken into account in the development of these codes. 
","It is uncertain what share of AI systems will be in the high-risk category, as
both the field of AI and the law are still evolving. The European Commission
estimated in an [impact study](https://digital-
strategy.ec.europa.eu/en/library/impact-assessment-regulation-artificial-
intelligence) that only 5-15% of applications would be subject to stricter
rules. A [study](https://aai.frb.io/assets/files/AI-Act-Risk-Classification-
Study-appliedAI-March-2023.pdf) by appliedAI of 106 enterprise AI systems
found that 18% were high-risk, 42% low-risk, and 40% unclear if high or low
risk. The system with unclear risk classification in this study were mainly
within the areas of critical infrastructure, employment, law enforcement, and
product safety. In a related [survey](https://aai.frb.io/assets/files/AI-Act-
Impact-Survey_Report_Dec12.2022.pdf) of 113 EU AI startups, 33% of the
startups surveyed believed that their AI systems would be classified as high-
risk, compared to the 5-15% assessed by the European Commission.



#### What are the penalties of the EU AI Act?

Based on the European Parliament’s adopted position, using prohibited AI
practices outlined in Article 5 can result in fines of up to €40 million, or
7% of worldwide annual turnover – whichever is higher. Not complying with the
data governance requirements (under Article 10) and the requirements for
transparency and provision of information (under Articles 13) can lead to
fines up to €20 million, or 4% of worldwide turnover if that is higher. Non-
compliance with any other requirements or obligations can result in fines of
up to €10 million or 2% of worldwide turnover – again, whichever is higher.
Fines are tiered depending on the provision violated, with prohibited
practices receiving the highest penalties and other violations receiving lower
maximum fines.



#### What measures does the EU AI Act implement for SMEs specifically?

The EU AI Act aims to provide support for SMEs and startups as specified in
Article 55. Specific measures include granting priority access for SMEs and
EU-based startups to regulatory sandboxes if eligibility criteria are met.
Additionally, tailored awareness raising and digital skills development
activities will be organised to address the needs of smaller organisations.
Moreover, dedicated communication channels will be set up to offer guidance
and respond to queries from SMEs and startups. Participation of SMEs and other
stakeholders in the standards development process will also be encouraged. To
reduce the financial burden of compliance, conformity assessment fees will be
lowered for SMEs and startups based on factors like development stage, size,
and market demand. The Commission will regularly review certification and
compliance costs for SMEs/startups (with input from transparent consultations)
and work with Member States to reduce these costs where possible.



#### Can I voluntarily comply with the EU AI Act even if my system is not in
scope?

Yes! The Commission, the AI Office, and/or Member States will encourage
voluntary codes of conduct for requirements under Title III, Chapter 2 (e.g.
risk management, data governance, and human oversight) for AI systems not
deemed to be high-risk. These codes of conduct will provide technical
solutions for how an AI system can meet the requirements, according to the
system’s intended purpose. Other objectives like environmental sustainability,
accessibility, stakeholder participation, and diversity of development teams
will be considered. Small businesses and startups will be taken into account
when encouraging codes of conduct. See Article 69 on Codes of Conduct for
Voluntary Application of Specific Requirements.



## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
It worries that
the law, in its current form, would fail to achieve the objective of
protecting fundamental rights. Specifically, it does not think that the
proposal goes far enough to protect fundamental rights in relation to
biometric applications such as emotion recognition and AI polygraphs. The
current draft of the AI Act advocates for transparency obligations for these
applications, but Access Now recommends stronger measures to reduce all
associated risks, such as bans. Read its concrete suggestions
[here](https://ec.europa.eu/info/law/better-regulation/have-your-
say/initiatives/12527-Artificial-intelligence-ethical-and-legal-
requirements/F2665462_en).

![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/09/AccessNowNewSiteAnnouncement.jpeg)

## Michael Veale & Frederik Zuiderveen Borgesius

Michael Veale, Assistant Professor at University College London in Digital
Rights and Regulation, and Frederik Zuiderveen Borgesius, Professor of ICT and
Private Law at the Dutch Radboud University, provide a thorough analysis of
some of the most sensitive parts of the EU AI Act. One of their article's many
surprising insights is that compliance with the law would almost completely
depend on self-assessment. Self-assessment means that there is no enforcement
to comply with the law. Once standardisation bodies such as CEN and CENELEC
have published their standards, third-party verification with the law will no
longer be required. The full article can be found
[here](https://osf.io/preprints/socarxiv/38p5f).

![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/michael_veale_800x500.png)

## The Future Society

The Future Society, a nonprofit organisation registered in Estonia, which
advocates for the responsible adoption of AI for the benefit of humanity,
submitted its feedback to the European Commission on the EU AI Act. One of its
suggestions is to ensure governance remains responsive to technological
trends. This could be achieved by improving the flow of information between
national and European institutions and systematically compiling and analysing
incident reports from member states. Read the full feedback
[here](https://ec.europa.eu/info/law/better-regulation/have-your-
say/initiatives/12527-Artificial-intelligence-ethical-and-legal-
requirements/F2665611_en).

![](https://artificialintelligenceact.eu/wp-content/uploads/2021/08/FS.png)

## Nathalie A. Smuha and colleagues

Nathalie A. Smuha, Researcher at the Faculty of Law at KU Leuven, Emma Ahmed-
Rengers, PhD Researcher in Law and Computer Science at the University of
Birmingham, and colleagues argue that the EU AI Act does not always accurately
recognise the wrongs and harms associated with different kinds of AI systems
nor allocate responsibility for them adequately. They also state that the
proposal does not provide an effective framework for the enforcement of legal
rights and duties. The proposal neglects to ensure meaningful transparency,
accountability, and rights of public participation. Read the full article
[here](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3899991).

![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/smuha.jpeg)

## The European DIGITAL SME Alliance

The European DIGITAL SME Alliance, a network of ICT small and medium
enterprises (SME) in Europe, welcomes a harmonised AI regulation and focus on
ethical AI in the EU but suggests many improvements to avoid overburdening
SMEs. For instance, it contends that wherever conformity assessments will be
based on standards, SMEs should actively participate in the development of
those standards. Otherwise, standards may be written in a way that is
impractical for SMEs. Many other recommendations can be read
[here](https://ec.europa.eu/info/law/better-regulation/have-your-
say/initiatives/12527-Artificial-intelligence-ethical-and-legal-
requirements/F2665574_en).

![](https://artificialintelligenceact.eu/wp-content/uploads/2021/09/DIGITAL-
SME-Logo.png)

## The cost of the EU AI Act

Center for Data Innovation, a nonprofit focused on data-driven innovation,
published a [report](https://www2.datainnovation.org/2021-aia-costs.pdf)
claiming that the EU AI Act will cost €31 billion over the next five years and
reduce AI investments by almost 20%."
"The provided information primarily focuses on high-risk AI systems and doesn't explicitly mention a ""minimal-risk"" category within the EU AI Act framework. 
","It is uncertain what share of AI systems will be in the high-risk category, as
both the field of AI and the law are still evolving. The European Commission
estimated in an [impact study](https://digital-
strategy.ec.europa.eu/en/library/impact-assessment-regulation-artificial-
intelligence) that only 5-15% of applications would be subject to stricter
rules. A [study](https://aai.frb.io/assets/files/AI-Act-Risk-Classification-
Study-appliedAI-March-2023.pdf) by appliedAI of 106 enterprise AI systems
found that 18% were high-risk, 42% low-risk, and 40% unclear if high or low
risk. The system with unclear risk classification in this study were mainly
within the areas of critical infrastructure, employment, law enforcement, and
product safety. In a related [survey](https://aai.frb.io/assets/files/AI-Act-
Impact-Survey_Report_Dec12.2022.pdf) of 113 EU AI startups, 33% of the
startups surveyed believed that their AI systems would be classified as high-
risk, compared to the 5-15% assessed by the European Commission.



#### What are the penalties of the EU AI Act?

Based on the European Parliament’s adopted position, using prohibited AI
practices outlined in Article 5 can result in fines of up to €40 million, or
7% of worldwide annual turnover – whichever is higher. Not complying with the
data governance requirements (under Article 10) and the requirements for
transparency and provision of information (under Articles 13) can lead to
fines up to €20 million, or 4% of worldwide turnover if that is higher. Non-
compliance with any other requirements or obligations can result in fines of
up to €10 million or 2% of worldwide turnover – again, whichever is higher.
Fines are tiered depending on the provision violated, with prohibited
practices receiving the highest penalties and other violations receiving lower
maximum fines.



#### What measures does the EU AI Act implement for SMEs specifically?

The EU AI Act aims to provide support for SMEs and startups as specified in
Article 55. Specific measures include granting priority access for SMEs and
EU-based startups to regulatory sandboxes if eligibility criteria are met.
Additionally, tailored awareness raising and digital skills development
activities will be organised to address the needs of smaller organisations.
Moreover, dedicated communication channels will be set up to offer guidance
and respond to queries from SMEs and startups. Participation of SMEs and other
stakeholders in the standards development process will also be encouraged. To
reduce the financial burden of compliance, conformity assessment fees will be
lowered for SMEs and startups based on factors like development stage, size,
and market demand. The Commission will regularly review certification and
compliance costs for SMEs/startups (with input from transparent consultations)
and work with Member States to reduce these costs where possible.



#### Can I voluntarily comply with the EU AI Act even if my system is not in
scope?

Yes! The Commission, the AI Office, and/or Member States will encourage
voluntary codes of conduct for requirements under Title III, Chapter 2 (e.g.
risk management, data governance, and human oversight) for AI systems not
deemed to be high-risk. These codes of conduct will provide technical
solutions for how an AI system can meet the requirements, according to the
system’s intended purpose. Other objectives like environmental sustainability,
accessibility, stakeholder participation, and diversity of development teams
will be considered. Small businesses and startups will be taken into account
when encouraging codes of conduct. See Article 69 on Codes of Conduct for
Voluntary Application of Specific Requirements.



## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
Examples: real-time biometric
identification, social scoring, manipulative techniques.

### High-Risk AI

These use cases are considered highly harmful if misused, and will need to go
through a third-party conformity assessment before being placed on the market.
According to [Article 6](https://artificialintelligenceact.eu/article/6), an
AI system is considered high-risk under either of the following conditions:

  1. It’s used as a safety component of a product, or is itself a product that’s already regulated by existing EU laws listed in Annex II, and is required to undergo a third-party conformity assessment under those Annex II laws; or,
  2. It falls under Annex III, unless it’s proved to not possess significant risk of harm to health, safety, or fundamental rights

We will now cover the two points above in more detail.

* * *

One of the major points of deliberation during the AI Act’s development was
alignment with existing EU harmonisation legislations that regulate existing
product categories. This is covered by Annex II.

 **[Annex II](https://artificialintelligenceact.eu/annex/2), Section A: List
of Union Harmonisation Legislation Based on the New Legislative Framework**

Includes AI systems, or the products for which the AI system is a safety
component, that fall under the following categories. As per [Article 28
2(a)](https://artificialintelligenceact.eu/article/28/), the manufacturer of
such products is considered the Provider of the included high-risk AI system
if it’s placed on the market under the manufacturer’s name or trademark:

  * Machinery
  * Toys
  * Recreational craft and personal watercraft
  * Lifts
  * Equipment and protective systems intended for use in potentially explosive atmospheres
  * Radio equipment
  * Pressure equipment
  * Cableway installations
  * Personal protective equipment
  * Appliances burning gaseous fuels
  * Medical devices
  * In vitro diagnostic medical devices

According to [Article 6(1)](https://artificialintelligenceact.eu/article/6),
if an AI system is covered by the above categories, and already needs to go
through third-party assessment under the relevant legislation, it’s considered
high-risk – with all the relevant obligations applicable to the Provider.

However, the exact interplay between the AI Act and existing regulations, e.g.
the Medical Devices Regulation (MDR), is [not yet
finalized](https://www.emergobyul.com/news/effect-europes-artificial-
intelligence-act-medical-device-industry) and remains a [point of
discussion](https://haiweb.org/wp-content/uploads/2023/03/MDR-
AIAct_OnePager_FINAL.pdf). Obligations for AI systems in Annex II will be
enforced a year later than other high-risk systems to provide sufficient time
to make the requirements compatible.

[ **Annex II**](https://artificialintelligenceact.eu/annex/2) **, Section B:
Other devices that fall under the Union Harmonisation Legislation**

Includes AI systems, or the product for which the AI system is a safety
component, that fall under the following categories:

  * Civil aviation security
  * Two- or three-wheel vehicles and quadricycles
  * Agricultural and forestry vehicles
  * Marine equipment
  * Interoperability of the rail system
  * Motor vehicles and their trailers
  * Civil aviation

According to [Article 2(2)](https://artificialintelligenceact.eu/article/2),
if a high-risk AI system is covered by the above categories, and already needs
to go through third-party assessment under the relevant legislation, it’s
categorized as high-risk. However, unlike for use cases under Annex II Section
A above, **only** **[Article
84](https://artificialintelligenceact.eu/article/84) shall apply.** The
Provider is only obligated to comply with the existing harmonized legislation,
and is exempt from the regular high-risk AI system rules.

* * *

[ **Annex III**](https://artificialintelligenceact.eu/annex/3) provides the
list of other high-risk use cases that are not covered by other existing
harmonization legislation:

  * Biometrics (ones permitted under relevant Union or national law) 
  * Critical infrastructure (e.g. safety component in water, gas, heating or electricity infrastructure)
  * Education and vocational training (e.g. determining admission to training institutions)
  * Employment, workers management and access to self-employment (e.g. monitoring and evaluating performance; targeted job ads)
  * Access to and enjoyment of essential public and private services (e.g."
"The approach taken by the legislation focuses on the level of risk associated with an AI system rather than providing a definitive list of high-risk applications. This means that the level of regulatory scrutiny applied to an AI system will depend on its potential to cause harm or negative impacts. 
","The AI Act takes a “risk-based” approach, scaling the stringency of
requirements based on the inherent risk of an AI system. You can learn more
about how this approach differs from other frameworks and standards in our
[previous blog post on the state of AI
regulation.](https://www.citadel.co.jp/en/blog/2023/03/20/an-overview-of-ai-
standardization-and-regulation-in-2023/)

As explained below, AI systems categorized as __ high-risk will feel the most
impact from this regulation. Developers of such high-risk AI systems must go
through a **third-party conformity assessment** in order to **obtain a CE mark
and access the EU market**.

## Do the requirements differ for developers and users of AI systems?

The most recent version of the AI Act [from Feb
2](https://artificialintelligenceact.eu/wp-content/uploads/2024/01/AI-Act-
FullText.pdf), approved by the Committee of Permanent Representatives,
provides additional information on the
[roles](https://artificialintelligenceact.eu/article/3) and [their
responsibilities](https://artificialintelligenceact.eu/article/28/) across the
AI Value Chain. The law defines six key roles:

  1.  **Provider**. An entity (e.g. company) that develops an AI system and “places them on the market or puts the system into service under its own name or trademark, whether for payment or free of charge”.
  2.  **Deployer**. An entity that uses an AI system under its authority, except in personal non-professional activities. Note that this does not refer to the affected end users.
  3.  **Importer**. An EU entity that places an non-EU AI system on the EU market.
  4.  **Distributor**. An entity in the supply chain, other than the provider or the importer, that makes an AI system available in the EU market.
  5.  **Authorized Representatives**. An EU entity that carries out obligations on behalf of a Provider.
  6.  **Operator.** An entity that falls into any of the five categories above.

> Operators could act in more than one role at the same time and should
> therefore fulfill cumulatively all relevant obligations associated with
> those roles. For example, an operator could act as a distributor and an
> importer at the same time.
>
> [Recital 56a](https://artificialintelligenceact.eu/recital/56a)

 **Providers** bear the majority of responsibilities under the AI Act.
Providers of high-risk AI systems will need to comply with [Article
16](https://artificialintelligenceact.eu/article/16/), and by extension:

  * [Article 9](https://artificialintelligenceact.eu/article/9). Implementing risk management processes.
  * [Article 10](https://artificialintelligenceact.eu/article/10). Data and Data Governance. Using high-quality training, validation and testing data.
  * [Article 11](https://artificialintelligenceact.eu/article/11). Establishing documentation as defined by [Annex IV](https://artificialintelligenceact.eu/annex/4).
  * [Article 12](https://artificialintelligenceact.eu/article/12). Implementing automatic logging.
  * [Article 13](https://artificialintelligenceact.eu/article/13). Ensuring an appropriate level of transparency with Deployers.
  * [Article 14](https://artificialintelligenceact.eu/article/14). Ensuring human oversight measures.
  * [Article 15](https://artificialintelligenceact.eu/article/15). Ensuring robustness, accuracy and cybersecurity.
  * [Article 17](https://artificialintelligenceact.eu/article/17). Setting up a Quality Management System (e.g. [based on ISO 42001](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-you/))
  * [Article 18](https://artificialintelligenceact.eu/article/18). Keeping documentation for 10 years at the disposal of the national competent authorities.
  * [Article 20](https://artificialintelligenceact.eu/article/20/). Storing the automatically generated logs for at least 6 months.
  * [Article 21](https://artificialintelligenceact.eu/article/21/). Corrective actions and Duty of information when the system is not in conformity.
  * [Article 23](https://artificialintelligenceact.eu/article/23). Providing the necessary information to competent authorities.
  * [Article 51](https://artificialintelligenceact.eu/article/51).
It is uncertain what share of AI systems will be in the high-risk category, as
both the field of AI and the law are still evolving. The European Commission
estimated in an [impact study](https://digital-
strategy.ec.europa.eu/en/library/impact-assessment-regulation-artificial-
intelligence) that only 5-15% of applications would be subject to stricter
rules. A [study](https://aai.frb.io/assets/files/AI-Act-Risk-Classification-
Study-appliedAI-March-2023.pdf) by appliedAI of 106 enterprise AI systems
found that 18% were high-risk, 42% low-risk, and 40% unclear if high or low
risk. The system with unclear risk classification in this study were mainly
within the areas of critical infrastructure, employment, law enforcement, and
product safety. In a related [survey](https://aai.frb.io/assets/files/AI-Act-
Impact-Survey_Report_Dec12.2022.pdf) of 113 EU AI startups, 33% of the
startups surveyed believed that their AI systems would be classified as high-
risk, compared to the 5-15% assessed by the European Commission.



#### What are the penalties of the EU AI Act?

Based on the European Parliament’s adopted position, using prohibited AI
practices outlined in Article 5 can result in fines of up to €40 million, or
7% of worldwide annual turnover – whichever is higher. Not complying with the
data governance requirements (under Article 10) and the requirements for
transparency and provision of information (under Articles 13) can lead to
fines up to €20 million, or 4% of worldwide turnover if that is higher. Non-
compliance with any other requirements or obligations can result in fines of
up to €10 million or 2% of worldwide turnover – again, whichever is higher.
Fines are tiered depending on the provision violated, with prohibited
practices receiving the highest penalties and other violations receiving lower
maximum fines.



#### What measures does the EU AI Act implement for SMEs specifically?

The EU AI Act aims to provide support for SMEs and startups as specified in
Article 55. Specific measures include granting priority access for SMEs and
EU-based startups to regulatory sandboxes if eligibility criteria are met.
Additionally, tailored awareness raising and digital skills development
activities will be organised to address the needs of smaller organisations.
Moreover, dedicated communication channels will be set up to offer guidance
and respond to queries from SMEs and startups. Participation of SMEs and other
stakeholders in the standards development process will also be encouraged. To
reduce the financial burden of compliance, conformity assessment fees will be
lowered for SMEs and startups based on factors like development stage, size,
and market demand. The Commission will regularly review certification and
compliance costs for SMEs/startups (with input from transparent consultations)
and work with Member States to reduce these costs where possible.



#### Can I voluntarily comply with the EU AI Act even if my system is not in
scope?

Yes! The Commission, the AI Office, and/or Member States will encourage
voluntary codes of conduct for requirements under Title III, Chapter 2 (e.g.
risk management, data governance, and human oversight) for AI systems not
deemed to be high-risk. These codes of conduct will provide technical
solutions for how an AI system can meet the requirements, according to the
system’s intended purpose. Other objectives like environmental sustainability,
accessibility, stakeholder participation, and diversity of development teams
will be considered. Small businesses and startups will be taken into account
when encouraging codes of conduct. See Article 69 on Codes of Conduct for
Voluntary Application of Specific Requirements.



## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"I am unable to provide the specific number of AI incidents that trigger a re-evaluation of an AI system under the EU AI Act. The provided information outlines the process for reporting serious incidents but does not specify a threshold for re-evaluation. 
","3a. For high-risk AI systems which are safety components of devices, or are
themselves devices, covered by Regulation (EU) 2017/745 and Regulation (EU)
2017/746 the notification of serious incidents shall be limited to those
referred to in [Article
3](https://artificialintelligenceact.eu/article/3/)(44)(c) and be made to the
national competent authority chosen for this purpose by the Member States
where that incident occurred.

3a. National competent authorities shall immediately notify the Commission of
any serious incident, whether or not it has taken action on it, in accordance
with Article 20 of Regulation 2019/1020.

[ <- Previous ](https://artificialintelligenceact.eu/article/72/) [ Next ->
](https://artificialintelligenceact.eu/article/74/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
It is uncertain what share of AI systems will be in the high-risk category, as
both the field of AI and the law are still evolving. The European Commission
estimated in an [impact study](https://digital-
strategy.ec.europa.eu/en/library/impact-assessment-regulation-artificial-
intelligence) that only 5-15% of applications would be subject to stricter
rules. A [study](https://aai.frb.io/assets/files/AI-Act-Risk-Classification-
Study-appliedAI-March-2023.pdf) by appliedAI of 106 enterprise AI systems
found that 18% were high-risk, 42% low-risk, and 40% unclear if high or low
risk. The system with unclear risk classification in this study were mainly
within the areas of critical infrastructure, employment, law enforcement, and
product safety. In a related [survey](https://aai.frb.io/assets/files/AI-Act-
Impact-Survey_Report_Dec12.2022.pdf) of 113 EU AI startups, 33% of the
startups surveyed believed that their AI systems would be classified as high-
risk, compared to the 5-15% assessed by the European Commission.



#### What are the penalties of the EU AI Act?

Based on the European Parliament’s adopted position, using prohibited AI
practices outlined in Article 5 can result in fines of up to €40 million, or
7% of worldwide annual turnover – whichever is higher. Not complying with the
data governance requirements (under Article 10) and the requirements for
transparency and provision of information (under Articles 13) can lead to
fines up to €20 million, or 4% of worldwide turnover if that is higher. Non-
compliance with any other requirements or obligations can result in fines of
up to €10 million or 2% of worldwide turnover – again, whichever is higher.
Fines are tiered depending on the provision violated, with prohibited
practices receiving the highest penalties and other violations receiving lower
maximum fines.



#### What measures does the EU AI Act implement for SMEs specifically?

The EU AI Act aims to provide support for SMEs and startups as specified in
Article 55. Specific measures include granting priority access for SMEs and
EU-based startups to regulatory sandboxes if eligibility criteria are met.
Additionally, tailored awareness raising and digital skills development
activities will be organised to address the needs of smaller organisations.
Moreover, dedicated communication channels will be set up to offer guidance
and respond to queries from SMEs and startups. Participation of SMEs and other
stakeholders in the standards development process will also be encouraged. To
reduce the financial burden of compliance, conformity assessment fees will be
lowered for SMEs and startups based on factors like development stage, size,
and market demand. The Commission will regularly review certification and
compliance costs for SMEs/startups (with input from transparent consultations)
and work with Member States to reduce these costs where possible.



#### Can I voluntarily comply with the EU AI Act even if my system is not in
scope?

Yes! The Commission, the AI Office, and/or Member States will encourage
voluntary codes of conduct for requirements under Title III, Chapter 2 (e.g.
risk management, data governance, and human oversight) for AI systems not
deemed to be high-risk. These codes of conduct will provide technical
solutions for how an AI system can meet the requirements, according to the
system’s intended purpose. Other objectives like environmental sustainability,
accessibility, stakeholder participation, and diversity of development teams
will be considered. Small businesses and startups will be taken into account
when encouraging codes of conduct. See Article 69 on Codes of Conduct for
Voluntary Application of Specific Requirements.



## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"The provided information primarily focuses on the categorization and obligations of high-risk AI systems under the EU AI Act, with particular emphasis on General Purpose AI (GPAI) models. While it highlights the importance of bias detection and correction, it does not specify quantitative thresholds for data bias that would trigger action. 
","4\. Datasets shall take into account, to the extent required by the intended
purpose, the characteristics or elements that are particular to the specific
geographical, contextual, behavioural or functional setting within which the
high-risk AI system is intended to be used.

5\. To the extent that it is strictly necessary for the purposes of ensuring
bias detection and correction in relation to the high-risk AI systems in
accordance with the second paragraph, point f and fa, the providers of such
systems may exceptionally process special categories of personal data referred
to in Article 9(1) of Regulation (EU) 2016/679, Article 10 of Directive (EU)
2016/680 and Article 10(1) of Regulation (EU) 2018/1725, subject to
appropriate safeguards for the fundamental rights and freedoms of natural
persons. In addition to provisions set out in the Regulation (EU) 2016/679,
Directive (EU) 2016/680 and Regulation (EU) 2018/1725, all the following
conditions shall apply in order for such processing to occur:

(a) the bias detection and correction cannot be effectively fulfilled by
processing other data, including synthetic or anonymised data;

(b) the special categories of personal data processed for the purpose of this
paragraph are subject to technical limitations on the re-use of the personal
data and state of the art security and privacy-preserving measures, including
pseudonymisation;

(c) the special categories of personal data processed for the purpose of this
paragraph are subject to measures to ensure that the personal data processed
are secured, protected, subject to suitable safeguards, including strict
controls and documentation of the access, to avoid misuse and ensure only
authorised persons have access to those personal data with appropriate
confidentiality obligations;

(d) the special categories of personal data processed for the purpose of this
paragraph are not to be transmitted, transferred or otherwise accessed by
other parties;

(e) the special categories of personal data processed for the purpose of this
paragraph are deleted once the bias has been corrected or the personal data
has reached the end of its retention period, whatever comes first;

(f) the records of processing activities pursuant to Regulation (EU) 2016/679,
Directive (EU) 2016/680 and Regulation (EU) 2018/1725 includes justification
why the processing of special categories of personal data was strictly
necessary to detect and correct biases and this objective could not be
achieved by processing other data.

6\. For the development of high-risk AI systems not using techniques involving
the training of models, paragraphs 2 to 5 shall apply only to the testing data
sets.

[ <- Previous ](https://artificialintelligenceact.eu/article/9/) [ Next ->
](https://artificialintelligenceact.eu/article/11/)

#### Suitable Recitals

[

Recital 66

](https://artificialintelligenceact.eu/recital/66/) [

Recital 67

](https://artificialintelligenceact.eu/recital/67/) [

Recital 69

](https://artificialintelligenceact.eu/recital/69/) [

Recital 71

](https://artificialintelligenceact.eu/recital/71/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
According to [Article 52c](https://artificialintelligenceact.eu/article/52c),
Providers of GPAI models must:

  * Draw up technical documentation, including training and testing process and evaluation results.
  * Enable other Providers that intend to integrate the GPAI model into their own AI system with sufficient documentation and information.
  * Put in place a policy to respect the EU copyright law.
  * Based on a template provided by the AI Office, publish a sufficiently detailed summary about the content used for training.
  * Develop and adhere to a code of practice

One of the reasons for the recent delays in the AI Act’s development was a
debate on the two-tiered categorization for GPAI models. Currently, models
that were trained using a total computing power of more than [10^25
FLOPs](https://artificialintelligenceact.eu/article/52a) are considered to
carry “ **systemic risk** ”. This places most models on the market, excluding
**GPT-4 and potentially Gemini Ultra** , in the non-systemic risk subcategory.
However, the threshold may be updated in future by the European Commission and
the AI Office as the technology progresses.

Providers of GPAI models with systemic risk are subject [to additional
obligations](https://artificialintelligenceact.eu/article/52d) to:

  * Perform model evaluations, including adversarial testing
  * Assess and mitigate possible systemic risks
  * Document and report issues to the AI office and relevant authorities

* * *

However, there are also major **exemptions** for Providers of AI systems for
research and academic use, as well as for AI models (including GPAI) that are
free and open-source. These models are not subject to any obligations until
they are placed on the market by a Provider as part of an AI system that has
obligations under the AI Act. Military and law enforcement uses are excluded
from the majority of obligations, but have some special considerations that we
will not cover.

You can learn more details about each risk category in the [official
FAQ](https://ec.europa.eu/commission/presscorner/detail/en/QANDA_21_1683)
published by the European Commission.

## What are the consequences of not complying with the EU AI Act?

As defined in [Article 71](https://artificialintelligenceact.eu/article/71/),
administrative fines act as the main penalty. The actual fine amount will be
decided on a case-by-case basis depending on the severity of the infringement
(based on rules to be further developed by individual EU Member states), but
the AI Act does outline maximum penalties for different types of non-
compliance:

  *  _Prohibited practices or non-compliance related to requirements on data:_ up to €35m or 7% of the total worldwide annual turnover
  *  _Non-compliance with any of the other requirements:_ up to €15m or 3% of the total worldwide annual turnover
  *  _Incorrect, incomplete or misleading information to notified bodies and national competent authorities:_ up to €7.5m or 1.5% of the total worldwide annual turnover

For each category of infringement, the threshold would be the lower of the two
amounts for small and medium-sized enterprises (including startups) and the
higher for other companies.

## What is the current legislative status and timeline?

The EU AI Act is not yet law, but it’s on a fast track to be published in the
Official Journal in early-mid 2024, and most of its requirements will
gradually be enforced in the following 24 months.

The[ timeline for
enforcement](https://artificialintelligenceact.eu/article/85) is as follows:

  1. After 6 months: Prohibition of AI applications of unacceptable risk becomes applicable.
  2. After 9 months: [Codes of practice](https://artificialintelligenceact.eu/recital/60t/) for GPAI providers must be prepared.
  3. After 12 months (approximately 2025 Q2~): Provisions on GPAI become applicable.
  4. After 24 months (approximately 2026 Q2~): Obligations for high-risk AI systems under Annex III become applicable. Each EU Member State is required to establish or participate in at least one regulatory sandbox.
  5. After 36 months: Obligations for high-risk AI systems under Annex II become applicable.

In practical terms, this means that the next two years will see a flurry of
activity in the field of AI auditing, both in government and in the private
sector. Relevant offices and systems will need to be set up in EU member
states; standards-making and certification bodies will be under extra pressure
to fill the gap and provide clear assessment frameworks."
"Unfortunately, I cannot answer your question based on the provided context. While the text mentions NIST, it does not provide the full name or meaning of ""RMF"" in the context of NIST AI RMF. 
","40 
 外部への説明という目的のため、 米国国立標準技術研究所 （ NIST ） の文書23を参考にし、
可能な限り正確かつ相手に理解できる 記録とし、説明の限界を意識できるよう努めてい
る。 
 
【実践例２】  
 当社は AIシステムを開発する小規模企業である。 技術担当役員 は全てのプロジェクト
を把握しており、自らプログラミングしたり論文を読解したりするなど AIに大変詳し
く、 AI倫理の問題についても強い関心を持っている。そのため当社では、部門間の専門
性のギャップが問題 になることはないと考えている。他方で、 プロジェクトに関わる人
たちの専門性が高いために、 いちいち確認しなくても 、行動目標ができているであろう
と思い込みがち である。そのため、 プロジェクトの進捗報告のレポートに 乖離評価 チェ
ックリストを添付し、 技術担当役員が必要に応じて聞き取りできるように工夫 している。  
また、当社は専門性が 高い集団であることから、世間の認識 とのずれが生じやすい傾
向があると分析している。 そのため、 運用状況を確認しつつ、 行動目標３－３－２にした
がって 日常的な情報収集や意見交換から得られた状況を定期的に共有する ことで、社会
的受容に意識を向けるようにしている。  
 
  
 
23 P J. Phillips, Amanda C. Hahn, Peter C. Fontana, David A. Broniatowski, Mark A. Przybocki, “Four 
Principles of Explainable Artificial Intelligence (Draft), NIST Interagency/Internal Report (NISTIR) - 8312 -
draft” (August 19, 2020)
Machine Learning Quality Management Guideline   National Institute of  
3rd English edition   Advanced Industrial Science and Technology  
 
227 
 via Randomized Smoothing. The 36th International Confer ence on Machine Learning 
(ICML 2019) , 2019.  
[81] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, 
Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. RobustBench: A 
Standardized Adversarial Ro bustness Benchmark. 2021. 
http://arxiv.org/abs/2010.09670, https://robustbench.github.io  
[82] George Deckert , NASA Hazard Analysis Process. Johnson Space Center, National 
Aeronautics and Space Administration, 2010. 
https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/201000406 78.pdf . 
[83] Ann- Kathrin Dombrowski, Maximilian Alber, Christopher J. Anders, Marcel Ackermann, 
Klaus -Robert Mu ̈ ller, and Pan Kessel. Explanations can be manipulated and geometry is 
to blame. In Proceedings of the Annual Conference on Neural Information Processing 
Systems 2019 (NeurIPS’19) , pp. 13567 –13578, 2019. 
https://arxiv.org/pdf/1906.07983.pdf  
[84] Yizhen Dong, Peixin Zhang, Jingyi Wang, Shuang Liu, Jun Sun, Jianye Hao, Xinyu Wang, Li Wang, Jin Song Dong, and Dai Ting. There is Limited Correlation between Coverage and Robustness for Deep Neural Networks. arXiv:1911.05904 [cs.LG]. 
https://arxiv.org/abs/1911.05904
 
[85] Cynthia Dwork, Differential Privacy, In Proc. ICALP’06 , pp.1 -12, 2006.  
[86] Cynthia Dwork and Aaron Roth, The Algorithmic Foundations of Differential Privacy, Foundations and Trends in Theoretical Computer Science , 9(3 -4), pp.211 -407, 2014.  
[87] S. Elbaum and D. S. Rosenblum. Known Unknowns – Testing in the Presence of 
Uncertainty, In P roceedings of the 22nd ACM SIGSOFT International Symposium on 
Foundations of Software Engineering  (FSE 20 14), pp. 833 –836, 2014.  
[88] Arisa Ema (ed.). Small Featured Articles “From AI principles to implementation: 
introduction of international activities” . Artificial Intelligence 36(2), The Japanese 
Society for Artificial Intelligence, March 2021.. In Japanese  
[89] Ulfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova, RAPPOR: Randomized 
Aggregatable Privacy -Preserving Ordinal Response, arXiv:1407.6981v2, 2014.  
[90] Itay P . Fainmesser, Andrea Galeotti, and Ruslan Momot, Digital Privacy, HEC Paris Research Paper, 2021.  
[91] E. R. Faria, J. Gama, and A. C. Carvalho. Novelty detection algorithm for data streams 
multi class problems, In Procedings of the 28th annual ACM symposium on applied 
computing , pp. 795 –800, 2013.  
[92] Y. Feng, Q. Shi, X. Gao, J. Wan, C. Fang, and Z. Chen, DeepGini: Prioritizing Massive Tests 
to Enhance the Robustness of Deep Neural Networks, Proc. 29th ISSTA, pp. 177 -188, 
and arXiv:1903.00661v2, 2020.  
[93] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model Inversion Attacks that"
"## Ensuring Trustworthy AI Systems

The primary objective is to establish a framework that fosters the development and deployment of AI systems that are reliable, robust, and function consistently throughout their entire lifecycle. This framework emphasizes the importance of accuracy, security, and resilience against potential errors or vulnerabilities. 
","Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

1\. High-risk AI systems shall be designed and developed in such a way that
they achieve an appropriate level of accuracy, robustness, and cybersecurity,
and perform consistently in those respects throughout their lifecycle.

1a. To address the technical aspects of how to measure the appropriate levels
of accuracy and robustness set out in paragraph 1 of this Article and any
other relevant performance metrics, the Commission shall, in cooperation with
relevant stakeholder and organisations such as metrology and benchmarking
authorities, encourage as appropriate, the development of benchmarks and
measurement methodologies.

2\. The levels of accuracy and the relevant accuracy metrics of high-risk AI
systems shall be declared in the accompanying instructions of use.

3\. High-risk AI systems shall be as resilient as possible regarding errors,
faults or inconsistencies that may occur within the system or the environment
in which the system operates, in particular due to their interaction with
natural persons or other systems. Technical and organisational measures shall
be taken towards this regard. The robustness of high-risk AI systems may be
achieved through technical redundancy solutions, which may include backup or
fail-safe plans. High-risk AI systems that continue to learn after being
placed on the market or put into service shall be developed in such a way to
eliminate or reduce as far as possible the risk of possibly biased outputs
influencing input for future operations (‘feedback loops’) are duly addressed
with appropriate mitigation measures.

4\. High-risk AI systems shall be resilient as regards to attempts by
unauthorised third parties to alter their use, outputs or performance by
exploiting the system vulnerabilities. The technical solutions aimed at
ensuring the cybersecurity of high-risk AI systems shall be appropriate to the
relevant circumstances and the risks. The technical solutions to address AI
specific vulnerabilities shall include, where appropriate, measures to
prevent, detect, respond to, resolve and control for attacks trying to
manipulate the training dataset (‘data poisoning’), or pre-trained components
used in training (‘model poisoning’) , inputs designed to cause the model to
make a mistake (‘adversarial examples’ or ‘model evasion’), confidentiality
attacks or model flaws.

[ <- Previous ](https://artificialintelligenceact.eu/article/14/) [ Next ->
](https://artificialintelligenceact.eu/article/16/)

#### Suitable Recitals

[

Recital 66

](https://artificialintelligenceact.eu/recital/66/) [

Recital 74

](https://artificialintelligenceact.eu/recital/74/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
This process should be aimed at identifying and mitigating the
relevant risks of artificial intelligence systems on health, safety and
fundamental rights. The risk management system should be regularly reviewed
and updated to ensure its continuing effectiveness, as well as justification
and documentation of any significant decisions and actions taken subject to
this Regulation. This process should ensure that the provider identifies risks
or adverse impacts and implements mitigation measures for the known and
reasonably foreseeable risks of artificial intelligence systems to the health,
safety and fundamental rights in light of its intended purpose and reasonably
foreseeable misuse, including the possible risks arising from the interaction
between the AI system and the environment within which it operates. The risk
management system should adopt the most appropriate risk management measures
in the light of the state of the art in AI. When identifyingthe most
appropriate risk management measures, the provider should document and explain
the choices made and, when relevant, involve experts and external
stakeholders. In identifying reasonably foreseeable misuse of high risk AI
systems the provider should cover uses of the AI systems which, while not
directly covered by the intended purpose and provided for in the instruction
for use may nevertheless be reasonably expected to result from readily
predictable human behaviour in the context of the specific characteristics and
use of the particular AI system. Any known or foreseeable circumstances,
related to the use of the high-risk AI system in accordance with its intended
purpose or under conditions of reasonably foreseeable misuse, which may lead
to risks to the health and safety or fundamental rights should be included in
the instructions for use provided by the provider. This is to ensure that the
deployer is aware and takes them into account when using the high-risk AI
system. Identifying and implementing risk mitigation measures for foreseeable
misuse under this Regulation should not require specific additional training
measures for the high-risk AI system by the provider to address them. The
providers however are encouraged to consider such additional training measures
to mitigate reasonable foreseeable misuses as necessary and appropriate.

[ <- Previous ](https://artificialintelligenceact.eu/recital/64/) [ Next ->
](https://artificialintelligenceact.eu/recital/66/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"The provided information outlines several risk management strategies but does not explicitly mention a tiered approach to managing AI risks. 
","3 
measures, organizations commit to devote attention to the following risks as appropriate:  
 
> Chemical, biological, radio logical, and nuclear risks, such as the ways in which advanced AI systems 
can lower barriers to entry, including for non -state actors, for weapons development, design 
acquisition, or use.  
 > Offensive cyber capabilities, such as the ways in which systems c an enable vulnerability discovery, 
exploitation, or operational use, bearing in mind that such capabilities could also have useful defensive applications and might be appropriate to include in a system.  
 > Risks to health and/or Safety, including the effec ts of system interaction and tool use, including for 
example the capacity to control physical systems and interfere with critical infrastructure.  
 >  Risks from models of making copies of themselves or “ self-replicating ” or training other models.  
 >  Socie tal risks, as well as risks to individuals and communities such as the ways in which advanced 
AI systems or models can give rise to  harmful bias and discrimination or lead to violation of 
applicable legal frameworks, including on privacy and data protectio n. 
 >  Threats to democratic values and human rights, including the facilitation of disinformation or 
harming privacy.  
 > Risk that a particular event could lead to a chain reaction with considerable negative effects that 
could affect up to an entire city,  an entire domain activity or an entire community.  
 Organizations commit to work in collaboration with relevant actors across sectors, to assess and adopt mitigation measures to address these risks, in particular systemic risks.  
 Organizations making these commitments should also endeavor to advance research and investment on the security, safety, bias and disinformation, fairness, explainability and interpretability, and transparency of advanced AI systems and on increasing robust ness and 
trustworthiness of advanced AI systems against misuse.  
  
2 Identify and mitigate vulnerabilities, and, where appropriate, incidents and patterns of 
misuse, after deployment including placement on the market.  
 Organizations should use, as and when appropriate commensurate to the level of risk, AI systems as intended and monitor for vulnerabilities, incidents, emerging risks and misuse after deployment,
* Risk that a particular event could lead to a chain reaction with considerable negative effects that could affect up to an entire city, an entire domain activity or an entire community.

Organizations commit to work in collaboration with relevant actors across
sectors, to assess and adopt mitigation measures to address these risks, in
particular systemic risks.

Organizations making these commitments should also endeavor to advance
research and investment on the security, safety, bias and disinformation,
fairness, explainability and interpretability, and transparency of advanced AI
systems and on increasing robustness and trustworthiness of advanced AI
systems against misuse.

2 Identify and mitigate vulnerabilities, and, where appropriate, incidents and
patterns of misuse, after deployment including placement on the market.

Organizations should use, as and when appropriate commensurate to the level of
risk, AI systems as intended and monitor for vulnerabilities, incidents,
emerging risks and misuse after deployment, and take appropriate action to
address these. Organizations are encouraged to consider, for example,
facilitating third-party and user discovery and reporting of issues and
vulnerabilities after deployment such as through bounty systems, contests, or
prizes to incentivize the responsible disclosure of weaknesses. Organizations
are further encouraged to maintain appropriate documentation of reported
incidents and to mitigate the identified risks and vulnerabilities, in
collaboration with other stakeholders. Mechanisms to report vulnerabilities,
where appropriate, should be accessible to a diverse set of stakeholders.

3 Publicly report advanced AI systemsâ capabilities, limitations and domains
of appropriate and inappropriate use, to support ensuring sufficient
transparency, thereby contributing to increase accountability.

This should include publishing transparency reports containing meaningful
information for all new significant releases of advanced AI systems.  
These reports, instruction for use and relevant technical documentation, as
appropriate as, should be kept up-to-date and should include, for example;

  * Details of the evaluations conducted for potential safety, security, and societal risks, as well as risks to human rights,
  * Capacities of a model/system and significant limitations in performance that have implications for the domains of appropriate use,
  * Discussion and assessment of the modelâs or systemâs effects and risks to safety and society such as harmful bias, discrimination, threats to protection of privacy or personal data, and effects on fairness, and 
  * The results of red-teaming conducted to evaluate the modelâs/systemâs fitness for moving beyond the development stage.

Organizations should make the information in the transparency reports
sufficiently clear and understandable to enable deployers and users as
appropriate and relevant to interpret the model/systemâs output and to
enable users to use it appropriately; and that transparency reporting should
be supported and informed by robust documentation processes such as technical
documentation and instructions for use.

4 Work towards responsible information sharing and reporting of incidents
among organizations developing advanced AI systems including with industry,
governments, civil society, and academia

This includes responsibly sharing information, as appropriate, including, but
not limited to evaluation reports, information on security and safety risks,
dangerous intended or unintended capabilities, and attempts by AI actors to
circumvent safeguards across the AI lifecycle.

Organizations should establish or join mechanisms to develop, advance, and
adopt, where appropriate, shared standards, tools, mechanisms, and best
practices for ensuring the safety, security, and trustworthiness of advanced
AI systems.

This should also include ensuring appropriate and relevant documentation and
transparency across the AI lifecycle in particular for advanced AI systems
that cause significant risks to safety and society.

Organizations should collaborate with other organizations across the AI
lifecycle to share and report relevant information to the public with a view
to advancing safety, security and trustworthiness of advanced AI systems.
Organizations should also collaborate and share the aforementioned information
with relevant public authorities, as appropriate.

Such reporting should safeguard intellectual property rights.

5 Develop, implement and disclose AI governance and risk management policies,
grounded in a risk-based approach â including privacy policies, and
mitigation measures.

Organizations should put in place appropriate organizational mechanisms to
develop, disclose and implement risk management and governance policies,
including for example accountability and governance processes to identify,
assess, prevent, and address risks, where feasible throughout the AI
lifecycle.

This includes disclosing where appropriate privacy policies, including for
personal data, user prompts and advanced AI system outputs. Organizations are
expected to establish and disclose their AI governance policies and
organizational mechanisms to implement these policies in accordance with a
risk-based approach. This should include accountability and governance
processes to evaluate and mitigate risks, where feasible throughout the AI
lifecycle.

The risk management policies should be developed in accordance with a risk-
based approach and apply a risk management framework across the AI lifecycle
as appropriate and relevant, to address the range of risks associated with AI
systems, and policies should also be regularly updated."
"The provided information pertains to the EU AI Act and does not contain details about the NIST AI RMF or its applicability to private sector companies contracting with the federal government. Therefore, I am unable to answer your query. 
","This obligation shall not apply to third parties
making accessible to the public tools, services, processes, or AI components
other than general-purpose AI models under a free and open licence. The AI
Office may develop and recommend voluntary model contractual terms between
providers of high-risk AI systems and third parties that supply tools,
services, components or processes that are used or integrated in high-risk AI
systems. When developing voluntary model contractual terms, the AI Office
shall take into account possible contractual requirements applicable in
specific sectors or business cases. The model contractual terms shall be
published and be available free of charge in an easily usable electronic
format.

2b. Paragraphs 2 and 2a are without prejudice to the need to respect and
protect intellectual property rights and confidential business information or
trade secrets in accordance with Union and national law.

[ <- Previous ](https://artificialintelligenceact.eu/article/24/) [ Next ->
](https://artificialintelligenceact.eu/article/26/)

#### Suitable Recitals

[

Recital 87

](https://artificialintelligenceact.eu/recital/87/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

3\. National public authorities or bodies which supervise or enforce the
respect of obligations under Union law protecting fundamental rights,
including the right to non-discrimination, in relation to the use of high-risk
AI systems referred to in [Annex
III](https://artificialintelligenceact.eu/annex/3/) shall have the power to
request and access any documentation created or maintained under this
Regulation in accesible language and format when access to that documentation
is necessary for effectively fulfilling their mandate within the limits of
their jurisdiction. The relevant public authority or body shall inform the
market surveillance authority of the Member State concerned of any such
request.

4\. By three months after the entering into force of this Regulation, each
Member State shall identify the public authorities or bodies referred to in
paragraph 3 and make a list publicly available. Member States shall notify the
list to the Commission and all other Member States and keep the list up to
date.

5\. Where the documentation referred to in paragraph 3 is insufficient to
ascertain whether a breach of obligations under Union law intended to protect
fundamental rights has occurred, the public authority or body referred to in
paragraph 3 may make a reasoned request to the market surveillance authority,
to organise testing of the high-risk AI system through technical means. The
market surveillance authority shall organise the testing with the close
involvement of the requesting public authority or body within reasonable time
following the request.

6\. Any information and documentation obtained by the national public
authorities or bodies referred to in paragraph 3 pursuant to the provisions of
this Article shall be treated in compliance with the confidentiality
obligations set out in [Article
78](https://artificialintelligenceact.eu/article/78/).

[ <- Previous ](https://artificialintelligenceact.eu/article/76/) [ Next ->
](https://artificialintelligenceact.eu/article/79/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"The provided information outlines several potential risks associated with advanced AI systems and proposes mitigation strategies. It emphasizes the importance of collaboration among various stakeholders to assess and address these risks, particularly those that are systemic in nature. Additionally, it highlights the need for ongoing research and investment in areas such as security, safety, bias mitigation, and transparency to enhance the robustness and trustworthiness of AI systems. 
","3 
measures, organizations commit to devote attention to the following risks as appropriate:  
 
> Chemical, biological, radio logical, and nuclear risks, such as the ways in which advanced AI systems 
can lower barriers to entry, including for non -state actors, for weapons development, design 
acquisition, or use.  
 > Offensive cyber capabilities, such as the ways in which systems c an enable vulnerability discovery, 
exploitation, or operational use, bearing in mind that such capabilities could also have useful defensive applications and might be appropriate to include in a system.  
 > Risks to health and/or Safety, including the effec ts of system interaction and tool use, including for 
example the capacity to control physical systems and interfere with critical infrastructure.  
 >  Risks from models of making copies of themselves or “ self-replicating ” or training other models.  
 >  Socie tal risks, as well as risks to individuals and communities such as the ways in which advanced 
AI systems or models can give rise to  harmful bias and discrimination or lead to violation of 
applicable legal frameworks, including on privacy and data protectio n. 
 >  Threats to democratic values and human rights, including the facilitation of disinformation or 
harming privacy.  
 > Risk that a particular event could lead to a chain reaction with considerable negative effects that 
could affect up to an entire city,  an entire domain activity or an entire community.  
 Organizations commit to work in collaboration with relevant actors across sectors, to assess and adopt mitigation measures to address these risks, in particular systemic risks.  
 Organizations making these commitments should also endeavor to advance research and investment on the security, safety, bias and disinformation, fairness, explainability and interpretability, and transparency of advanced AI systems and on increasing robust ness and 
trustworthiness of advanced AI systems against misuse.  
  
2 Identify and mitigate vulnerabilities, and, where appropriate, incidents and patterns of 
misuse, after deployment including placement on the market.  
 Organizations should use, as and when appropriate commensurate to the level of risk, AI systems as intended and monitor for vulnerabilities, incidents, emerging risks and misuse after deployment,
#### recital

# Recital 111

**Feedback** – We are working to improve this tool. Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

It is appropriate to establish a methodology for the classification of
general-purpose AI models as general-purpose AI model with systemic risks.
Since systemic risks result from particularly high capabilities, a general-
purpose AI models should be considered to present systemic risks if it has
high-impact capabilities, evaluated on the basis of appropriate technical
tools and methodologies, or significant impact on the internal market due to
its reach. High-impact capabilities in general purpose AI models means
capabilities that match or exceed the capabilities recorded in the most
advanced general-purpose AI models. The full range of capabilities in a model
could be better understood after its release on the market or when users
interact with the model. According to the state of the art at the time of
entry into force of this Regulation, the cumulative amount of compute used for
the training of the general-purpose AI model measured in floating point
operations (FLOPs) is one of the relevant approximations for model
capabilities. The amount of compute used for training cumulates the compute
used across the activities and methods that are intended to enhance the
capabilities of the model prior to deployment, such as pre-training, synthetic
data generation and fine-tuning. Therefore, an initial threshold of FLOPs
should be set, which, if met by a general-purpose AI model, leads to a
presumption that the model is a general-purpose AI model with systemic risks.
This threshold should be adjusted over time to reflect technological and
industrial changes, such as algorithmic improvements or increased hardware
efficiency, and should be supplemented with benchmarks and indicators for
model capability. To inform this, the AI Office should engage with the
scientific community, industry, civil society and other experts. Thresholds,
as well as tools and benchmarks for the assessment of high-impact
capabilities, should be strong predictors of generality, its capabilities and
associated systemic risk of general-purpose AI models, and could take into
taking into account the way the model will be placed on the market or the
number of users it may affect. To complement this system, there should be a
possibility for the Commission to take individual decisions designating a
general-purpose AI model as a general-purpose AI model with systemic risk if
it is found that such model has capabilities or impact equivalent to those
captured by the set threshold. This decision should be taken on the basis of
an overall assessment of the criteria set out in [Annex
XIII](https://artificialintelligenceact.eu/annex/13/), such as quality or size
of the training data set, number of business and end users, its input and
output modalities, its degree of autonomy and scalability, or the tools it has
access to. Upon a reasoned request of a provider whose model has been
designated as a general-purpose AI model with systemic risk, the Commission
should take the request into account and may decide to reassess whether the
general-purpose AI model can still be considered to present systemic risks.

[ <- Previous ](https://artificialintelligenceact.eu/recital/110/) [ Next ->
](https://artificialintelligenceact.eu/recital/112/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"I am unable to answer your question as the provided text focuses on the EU AI Act and does not mention the NIST AI RMF or its risk categories. 
","It is uncertain what share of AI systems will be in the high-risk category, as
both the field of AI and the law are still evolving. The European Commission
estimated in an [impact study](https://digital-
strategy.ec.europa.eu/en/library/impact-assessment-regulation-artificial-
intelligence) that only 5-15% of applications would be subject to stricter
rules. A [study](https://aai.frb.io/assets/files/AI-Act-Risk-Classification-
Study-appliedAI-March-2023.pdf) by appliedAI of 106 enterprise AI systems
found that 18% were high-risk, 42% low-risk, and 40% unclear if high or low
risk. The system with unclear risk classification in this study were mainly
within the areas of critical infrastructure, employment, law enforcement, and
product safety. In a related [survey](https://aai.frb.io/assets/files/AI-Act-
Impact-Survey_Report_Dec12.2022.pdf) of 113 EU AI startups, 33% of the
startups surveyed believed that their AI systems would be classified as high-
risk, compared to the 5-15% assessed by the European Commission.



#### What are the penalties of the EU AI Act?

Based on the European Parliament’s adopted position, using prohibited AI
practices outlined in Article 5 can result in fines of up to €40 million, or
7% of worldwide annual turnover – whichever is higher. Not complying with the
data governance requirements (under Article 10) and the requirements for
transparency and provision of information (under Articles 13) can lead to
fines up to €20 million, or 4% of worldwide turnover if that is higher. Non-
compliance with any other requirements or obligations can result in fines of
up to €10 million or 2% of worldwide turnover – again, whichever is higher.
Fines are tiered depending on the provision violated, with prohibited
practices receiving the highest penalties and other violations receiving lower
maximum fines.



#### What measures does the EU AI Act implement for SMEs specifically?

The EU AI Act aims to provide support for SMEs and startups as specified in
Article 55. Specific measures include granting priority access for SMEs and
EU-based startups to regulatory sandboxes if eligibility criteria are met.
Additionally, tailored awareness raising and digital skills development
activities will be organised to address the needs of smaller organisations.
Moreover, dedicated communication channels will be set up to offer guidance
and respond to queries from SMEs and startups. Participation of SMEs and other
stakeholders in the standards development process will also be encouraged. To
reduce the financial burden of compliance, conformity assessment fees will be
lowered for SMEs and startups based on factors like development stage, size,
and market demand. The Commission will regularly review certification and
compliance costs for SMEs/startups (with input from transparent consultations)
and work with Member States to reduce these costs where possible.



#### Can I voluntarily comply with the EU AI Act even if my system is not in
scope?

Yes! The Commission, the AI Office, and/or Member States will encourage
voluntary codes of conduct for requirements under Title III, Chapter 2 (e.g.
risk management, data governance, and human oversight) for AI systems not
deemed to be high-risk. These codes of conduct will provide technical
solutions for how an AI system can meet the requirements, according to the
system’s intended purpose. Other objectives like environmental sustainability,
accessibility, stakeholder participation, and diversity of development teams
will be considered. Small businesses and startups will be taken into account
when encouraging codes of conduct. See Article 69 on Codes of Conduct for
Voluntary Application of Specific Requirements.



## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10.
Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

1\. High-risk AI systems shall be designed and developed in such a way that
they achieve an appropriate level of accuracy, robustness, and cybersecurity,
and perform consistently in those respects throughout their lifecycle.

1a. To address the technical aspects of how to measure the appropriate levels
of accuracy and robustness set out in paragraph 1 of this Article and any
other relevant performance metrics, the Commission shall, in cooperation with
relevant stakeholder and organisations such as metrology and benchmarking
authorities, encourage as appropriate, the development of benchmarks and
measurement methodologies.

2\. The levels of accuracy and the relevant accuracy metrics of high-risk AI
systems shall be declared in the accompanying instructions of use.

3\. High-risk AI systems shall be as resilient as possible regarding errors,
faults or inconsistencies that may occur within the system or the environment
in which the system operates, in particular due to their interaction with
natural persons or other systems. Technical and organisational measures shall
be taken towards this regard. The robustness of high-risk AI systems may be
achieved through technical redundancy solutions, which may include backup or
fail-safe plans. High-risk AI systems that continue to learn after being
placed on the market or put into service shall be developed in such a way to
eliminate or reduce as far as possible the risk of possibly biased outputs
influencing input for future operations (‘feedback loops’) are duly addressed
with appropriate mitigation measures.

4\. High-risk AI systems shall be resilient as regards to attempts by
unauthorised third parties to alter their use, outputs or performance by
exploiting the system vulnerabilities. The technical solutions aimed at
ensuring the cybersecurity of high-risk AI systems shall be appropriate to the
relevant circumstances and the risks. The technical solutions to address AI
specific vulnerabilities shall include, where appropriate, measures to
prevent, detect, respond to, resolve and control for attacks trying to
manipulate the training dataset (‘data poisoning’), or pre-trained components
used in training (‘model poisoning’) , inputs designed to cause the model to
make a mistake (‘adversarial examples’ or ‘model evasion’), confidentiality
attacks or model flaws.

[ <- Previous ](https://artificialintelligenceact.eu/article/14/) [ Next ->
](https://artificialintelligenceact.eu/article/16/)

#### Suitable Recitals

[

Recital 66

](https://artificialintelligenceact.eu/recital/66/) [

Recital 74

](https://artificialintelligenceact.eu/recital/74/)

## Receive EU AI Act updates in your inbox every two weeks

Subscribe to receive biweekly up-to-date developments and analyses of the
proposed EU AI Act. With over 15,000 subscribers, this newsletter is the go-to
resource for EU policymakers on the AI Act.

[See the Newsletter](https://artificialintelligenceact.substack.com/)

[![](https://artificialintelligenceact.eu/wp-
content/uploads/2021/08/FLI-.webp)](https://futureoflife.org/)

© Future of Life Institute, 2024

This website is maintained by the Future of Life Institute (FLI). Our [EU
transparency register](https://transparency-register.europa.eu/searchregister-
or-update/organisation-detail_en?id=787064543128-10) number is
787064543128-10."
"I am unable to answer your query using the provided context. The text focuses on fairness in machine learning and data preparation, and does not contain information about the NIST AI RMF or its compliance effectiveness metrics. 
","Machine Learning Quality Management Guideline   National Institute of  
3rd English edition   Advanced Industrial Science and Technology  
 
131 
  
Table 5: Summary of work scope and actions  by AIFL  
 When the training dataset 
can be modified  When the design model 
and learning algorithm 
can be modified  When only post -
processing of trained models is possible  
AIFL 2  Define and record fairness 
metrics goals  for the 
training dataset, with multiple pre -processing 
techniques, if necessary,  
to improve and ensure they are met as much as possible . Add fairness metrics for 
model output to the learning objectives and balance them with other metrics such as AI perfor mance,  and to 
achieve the target using in-processing techniques.  N/A  
(Consider in advance  
about the measures that 
can be taken to adjust the trained model. If the essential conditions are unlikely to be met, the work scope needs to be reviewed.)  
AIFL 1  Define and measure 
fairness metrics goals  for 
the training dataset.  If 
there are deviations, pre-processing techniques are used to improve and recorded with the result.  Fairness metrics for model 
output are added to the learning goals, and if the results of  training with AI 
performance priority deviate from the goals, improve it by  in-
processing techniques and recorded with the result.  Define fairness metrics for 
model output and measure them during testing.  If there is a 
deviation from the target, try to improve it by adjusting the trained model, or any other adjustment in the operation.  
AIFL 0  Define, measure and 
record fairness metrics for training datasets.  Define fairness metrics for 
model outputs and record their measurement during 
training.  Define fairness metrics for 
model outputs and record 
the measurements during 
testing.  
 
The following sections describe the details of the techniques, mainly in line with the internal 
quality set forth in Section 1.7 and Chapter  6. 
8.5.2.2.  A-1: Sufficiency of p roblem domain analysis ( preliminary preparation)  
By following the guide presented in the preliminary  preparation ( Section 8.5.1 ), the following 
aspects can be investigated.  
– Dependency analysis between attributes  
– Clarification of requirements for training data set distribution  
– Metrics to be checked (fairness metrics)
Machine Learning Quality Management Guideline   National Institute of  
3rd English edition   Advanced Industrial Science and Technology  
 
132 
 In addition, considering the nature of the overall system to be developed, the requirements of 
the following perspectives should be sufficiently confirmed.  
– Consistency with legal and social requirements  
Since the development team may not have sufficient knowledge of legal and social requirements, it is advisable to consult experts in the field.  
 
The required actions for each  target AIFL  level are as follows  in the preliminary preparation 
phase.  
・ AIFL 1  
 Define the fairness requirements and record them, including the history.  
 Based on this requirement, requirements for data preparation such as fairness 
metrics for training datasets shall be  defined.  
 Based on this requirement, fairness metrics for model outputs shall be defined.  
・ AIFL 2 
 In addition to those listed in AIFL  1, the following actions shall be  taken 
 To model dependencies and causal relationships between data attributes.  
 The results of modeling should be reflected in the fairness metrics of the training dataset . 
 
The fairness metrics  for the training dataset are basically from the same perspective as those 
for the model output. For example, if the fairness requirement is n o difference in pass  or fail 
judgments based on gender  for the output, the training dataset should also be free of bias from 
that perspective.  Examples of metrics are described in Section 8.5.2.4.2.1  
8.5.2.3.  B-1 to 3: Coverage/ uniformity/validity of datasets (data preparation) 
In the data preparation stage, pre-processing  techniques are taken to ensure required fairness, 
mainly from the internal quality perspective of coverage , uniformity , and validity of datasets . 
What is commonly referred to as Data Fairness is achieved at this stage, and there are two types 
to address it; one for data collection process and the other for the data collected."
"I am unable to provide the percentage of federal AI projects that have fully adopted the NIST AI RMF. The context provided discusses the EU AI Act and does not contain information about the NIST AI RMF or federal AI projects. 
","40 
 外部への説明という目的のため、 米国国立標準技術研究所 （ NIST ） の文書23を参考にし、
可能な限り正確かつ相手に理解できる 記録とし、説明の限界を意識できるよう努めてい
る。 
 
【実践例２】  
 当社は AIシステムを開発する小規模企業である。 技術担当役員 は全てのプロジェクト
を把握しており、自らプログラミングしたり論文を読解したりするなど AIに大変詳し
く、 AI倫理の問題についても強い関心を持っている。そのため当社では、部門間の専門
性のギャップが問題 になることはないと考えている。他方で、 プロジェクトに関わる人
たちの専門性が高いために、 いちいち確認しなくても 、行動目標ができているであろう
と思い込みがち である。そのため、 プロジェクトの進捗報告のレポートに 乖離評価 チェ
ックリストを添付し、 技術担当役員が必要に応じて聞き取りできるように工夫 している。  
また、当社は専門性が 高い集団であることから、世間の認識 とのずれが生じやすい傾
向があると分析している。 そのため、 運用状況を確認しつつ、 行動目標３－３－２にした
がって 日常的な情報収集や意見交換から得られた状況を定期的に共有する ことで、社会
的受容に意識を向けるようにしている。  
 
  
 
23 P J. Phillips, Amanda C. Hahn, Peter C. Fontana, David A. Broniatowski, Mark A. Przybocki, “Four 
Principles of Explainable Artificial Intelligence (Draft), NIST Interagency/Internal Report (NISTIR) - 8312 -
draft” (August 19, 2020)
**11 March 2024**

  * Updated the Compliance Checker to reflect the 'Final draft' of the AI Act.

**Data Privacy:** We are required to store your inputs from this form,
including your email address, in order to email your results to you. If you do
not use the email function, none of your data will be stored. We do not store
any other data that could be used to identify you or your device. If you wish
to remain anonymous, please use an email address that does not reveal your
identity. We do not share any of your data with any other parties. If you
would like your data to be deleted from our servers, please contact
[taylor@futureoflife.org](mailto:taylor@futureoflife.org)

## Frequently Asked Questions

#### Who developed this tool, and why?

This tool is developed by the team at the Future of Life Institute. We are in
no way affiliated with the European Union. We have developed this tool
voluntarily in order to aid the effective implementation of the AI Act,
because we believe the AI Act supports our mission to ensure that AI
technology remains beneficial for life, and avoids extreme large-scale risks.

#### Can I integrate this tool in my own website?

Our tool is built in a way that makes it difficult to support implementation
on other sites. We do not have an API available. Therefore, for almost all
cases, we suggest that you make this tool available to users of your website
by linking to this webpage – you are welcome to use these [mockup
image](https://workdrive.zohopublic.eu/external/0dde01ca37ea2f3e20d3af51fb016f0360b2570aa2f0fe555fc5321e3281e1d2?layout=list)
to feature the tool on your site.

#### When does the EU AI Act come into effect?

The European Commission is now supporting the Council of the European Union
and the European Parliament in concluding inter-institutional negotiations
(trilogue) – the last phase of negotiations before the EU AI Act is passed.
This is expected to be finished by the end of 2023 or early 2024. Once the
regulation is officially passed, there will be an implementation period of two
to three years depending on how the negotiations between the EU institutions
unfold. This means that the AI Act will likely be enforced in 2026 or later.
In addition, during the implementation period, the European standards bodies
are expected to develop standards for the AI Act.

#### What are the categories of risk defined by the EU AI Act?

The Act’s regulatory framework defines four levels of risk for AI systems:
unacceptable, high, limited, and minimal or no risk. Systems posing
unacceptable risks, such as threatening people’s safety, livelihood, and
rights – from social scoring by governments to toys using voice assistance –
will be prohibited. High-risk systems, such as those used in critical
infrastructure or law enforcement, will face strict requirements, including
around risk assessment, data quality, documentation, transparency, human
oversight, and accuracy. Systems posing limited risks, like chatbots, must
adhere to transparency obligations so users know they are not interacting with
humans. Minimal risk systems like games and spam filters can be freely used.

![](https://futureoflife.org/wp-
content/uploads/2023/08/pyramid_7F5843E5-9386-8052-931F5C4E98C6E5F2_75757.jpg)

#### What share of AI systems will fall into the high-risk category?

It is uncertain what share of AI systems will be in the high-risk category, as
both the field of AI and the law are still evolving. The European Commission
estimated in an [impact study](https://digital-
strategy.ec.europa.eu/en/library/impact-assessment-regulation-artificial-
intelligence) that only 5-15% of applications would be subject to stricter
rules. A [study](https://aai.frb.io/assets/files/AI-Act-Risk-Classification-
Study-appliedAI-March-2023.pdf) by appliedAI of 106 enterprise AI systems
found that 18% were high-risk, 42% low-risk, and 40% unclear if high or low
risk. The system with unclear risk classification in this study were mainly
within the areas of critical infrastructure, employment, law enforcement, and
product safety."
"Unfortunately, I don't have access to the specific publication date of ISO 42001. However, the provided information does mention that as of January 2024, certifications for ISO 42001 were not yet available due to the standard for certification bodies still being under development. This suggests that the standard itself was likely published sometime before January 2024. 
","Some examples of
other standards referenced in ISO 42001 are:

  * [ISO 5259](https://www.iso.org/standard/81860.html) series on Data Quality
  * [ISO 23894](https://www.iso.org/standard/77304.html) AI Risk Management
  * [ISO 24029-1](https://www.iso.org/standard/77609.html) Assessment of the robustness of neural networks

### What is the process for ISO 42001 certification?

To obtain an ISO 42001 certification, an organization must successfully pass
an audit conducted by a certification body. However, the [ISO
42006](https://www.iso.org/standard/44546.html) standard defining the
requirements for such bodies is currently under development, which means that
**as of January 2024, no certifications can be granted yet**.

However, the standard can still be freely used for voluntary assessments, both
internally and by third parties. An **early gap analysis** based on the
published standard can significantly accelerate preparations for an official
certification when it becomes available.

Based on known timelines for other management system standards, the audit can
be expected to take **from several months to up to a year** , depending on the
number of people involved in the AI lifecycle processes; the organization’s
role as an AI provider, developer, or user; and the complexity of the AIMS.

According to the current draft of ISO 42006, the audit process will likely
align with the established sequence of an internal audit (initial assessment
of nonconformities with a window for improvement), followed by a two-stage
external audit (an accredited body reviewing relevant documentation and
evidence), with annual recertification.

## How Citadel AI tools help with ISO 42001 preparation

Citadel AI’s technology helps organizations streamline their AI testing and
governance processes, and automate compliance with AI standards and
guidelines. Our products, Lens and Radar, can:

  1. Automatically fulfill some of the most technically demanding requirements of ISO 42001
  2. Help engineering teams validate their models and datasets against international standards
  3. Provide easy reporting and guidance to get on the certification track as quickly as possible

Citadel AI is trusted by world-leading organizations such as [the British
Standards Institution](https://www.citadel.co.jp/en/news/2023/04/11/bsi-and-
citadel-ai-partner-to-solve-the-ai-challenges-of-tomorrow/). At this critical
period of time where AI standards and regulation are maturing, we believe that
we can help you streamline compliance, improve AI reliability, and navigate
this evolving landscape.

Contact us if you’re interested in learning more.

## Get in Touch

Interested in a product demo or discussing how Citadel AI can improve your AI
quality? Please reach out to us here or [by email](mailto:info@citadel.co.jp).

Full Name

Company Email

Message

I agree to Citadel AI’s [privacy policy](/en/privacy-policy/).

Submit

## Related Articles

[ ![](https://sp-
ao.shortpixel.ai/client/to_auto,q_glossy,ret_img,w_1536,h_806/https://www.citadel.co.jp/en/wp-
content/uploads/sites/6/2024/02/AIA-Header-1536x806.png)
](https://www.citadel.co.jp/en/blog/2024/02/14/eu-ai-act-what-it-means-for-
you/)

  * 14 Feb 2024 

###### [EU AI Act: what it means for
you](https://www.citadel.co.jp/en/blog/2024/02/14/eu-ai-act-what-it-means-for-
you/)

  * [Blog](https://www.citadel.co.jp/en/./blog/)

[ ![](https://sp-
ao.shortpixel.ai/client/to_auto,q_glossy,ret_img,w_1536,h_806/https://www.citadel.co.jp/en/wp-
content/uploads/sites/6/2024/01/ISO-42001-1536x806.png)
](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-
you/)

  * 26 Jan 2024 

###### [ISO 42001 AI Management System: what it means for
you](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-
you/)

  * [Blog](https://www.citadel.co.jp/en/./blog/)

[ !
そうした製品レベルの規格に比べ、ISO
42001は、AIに対する組織レベルのアプローチに焦点を当てており、適切な予防策や改善策を講じることで、AI関連のリスクを管理する実践的な方法を、マネジメントシステム規格として提示しています。マネジメントシステムを構築し、ISO
42001に適合するためには、以下対応が必要となります。

  1. AIの開発・利用時における、組織内のさまざまなユースケース、プロセスならびにポリシーに関わる全社レベルでの再調査
  2. 現在組織内で利用されている実践方法や、ポリシー、システム、ツール、ドキュメンテーションの評価ならびに更新
  3. 製品化やサービス提供段階における、既存のあるいは見直したプロセスが実際に実践されていることのエビデンスの提示

ただし、組織レベルのマネジメントシステム原則が、実際に現場で適用されていることを確認するには、製品レベルあるいはプロジェクトレベルの具体的かつ相当量のエビデンスを提示することも求められます。このため、組織内部で利用するツール、モニタリング、その他の技術的な仕組みの構築に関わるプロジェクトレベルでの評価や、ドキュメンテーションが必要となります（詳細は後述）。

### ISO 42001はどのような構成になっているのか？

ISO42001は、これまで発行されてきたセクター別のマネジメントシステム(MSS)群の中に新たに加わることとなります。よく知られているMSS規格としては以下のようなものがあります。

  * ISO 27001: 情報セキュリティマネジメントシステム
  * ISO 13485: 医療機器品質マネジメントシステム
  * ISO 37001: 贈収賄防止マネジメントシステム
  * ISO 14001: 環境マネジメントシステム

全てのマネジメントシステム(MSS)は[極めて似た構成](https://www.iso.org/management-system-
standards.html)となっており、さまざまな企業や組織に対して可能な限り柔軟に適用・検証することができるようになっています。ISO
42001もその例外ではありません。

  * Clause 1: 適用範囲
  * Clause 2: 引用規格
  * …
  * Clause 10: 改善
  * 附属書

規格の中で **最も実務に即した部分は附属書** であり、そこには、組織が適合のために実施しなければならない管理項目のリストが含まれています。

  * B.6.2."
"There are several established management system standards that have been released over the years, and the structure of this particular standard is designed to be compatible with them. This includes widely used standards such as ISO 27001 for information security management, ISO 13485 for medical device quality management, ISO 37001 for anti-bribery management systems, and ISO 14001 for environmental management systems. 
","ISO standards consolidate years of work by experts, are widely accepted by
industry, and are often certifiable and auditable.

In regulated industries, ISO standards also provide technical specifications
and a pathway for companies and products to meet regulatory requirements,
which include medical devices, automobiles, and now AI systems under the EU AI
Act.

### How is ISO 42001 different from the other AI-related standards?

The SC 42 committee has already published dozens of technical documents
focusing on different aspects of AI development. For example, [ISO/IEC TS
4213:2022](https://www.iso.org/standard/79799.html), focusing on the
assessment of machine learning classification performance, can be applied to
quantitatively measure the performance of AI products.

In contrast to product-level standards, ISO 42001 provides an organization-
level approach to AI: as a **management system standard (MSS)** , it covers
principles of good governance and practical ways to manage AI-related risks,
by making sure that adequate preventative and remedial measures are in place.
Establishing a management system and complying with ISO 42001 involves:

  1. A company-wide review of the different use cases, processes, and policies for AI development and use.
  2. Validating or updating these existing processes, policies, systems, tools, and documentation.
  3. Providing evidence that the existing or updated processes are actually being followed at the product or service level.

In order to confirm that the organization-level management system principles
are followed, a sizable amount of product or project-level evidence also needs
to be provided. This includes per-project assessments and documentation on
internal tooling, evaluation, monitoring, and other technical infrastructure
(more on that below).

### What is the structure of ISO 42001?

ISO 42001 joins the family of other management system standards that have been
published over the years. Some of these well-known standards include:

  * ISO 27001: Information Security Management System
  * ISO 13485: Medical Device Quality Management System
  * ISO 37001: Anti-Bribery Management System
  * ISO 14001: Environmental Management System

All MSS follow [the exact same document
structure](https://www.iso.org/management-system-standards.html), which has
been tried and tested to be as flexible as possible and apply to organizations
across industries, with ISO 42001 being no exception:

  * Clause 1: Scope
  * Clause 2: Normative references
  * …
  * Clause 10: Improvement
  * Annexes

Most of the **practical requirements** are included in the **Annexes** , which
contain the list of controls that the organization must implement, such as:

  * B.6.2.4 AI system verification and validation. It requires developers to implement a standardized evaluation protocol for AI models and datasets.
  * B.6.2.8 AI system recording of event logs. It requires developers to have a logging system to record production input data, output predictions, and potential outliers.
  * B.7.4 Quality of data for AI systems. It requires developers to define and measure training and production data quality, as well as consider impact of data bias on performance.

(If you’re interested in a solution to automate the requirements above,
consider using [Citadel Lens](https://www.citadel.co.jp/en/products/lens/) and
[Citadel Radar](https://www.citadel.co.jp/en/products/radar/).)
Some examples of
other standards referenced in ISO 42001 are:

  * [ISO 5259](https://www.iso.org/standard/81860.html) series on Data Quality
  * [ISO 23894](https://www.iso.org/standard/77304.html) AI Risk Management
  * [ISO 24029-1](https://www.iso.org/standard/77609.html) Assessment of the robustness of neural networks

### What is the process for ISO 42001 certification?

To obtain an ISO 42001 certification, an organization must successfully pass
an audit conducted by a certification body. However, the [ISO
42006](https://www.iso.org/standard/44546.html) standard defining the
requirements for such bodies is currently under development, which means that
**as of January 2024, no certifications can be granted yet**.

However, the standard can still be freely used for voluntary assessments, both
internally and by third parties. An **early gap analysis** based on the
published standard can significantly accelerate preparations for an official
certification when it becomes available.

Based on known timelines for other management system standards, the audit can
be expected to take **from several months to up to a year** , depending on the
number of people involved in the AI lifecycle processes; the organization’s
role as an AI provider, developer, or user; and the complexity of the AIMS.

According to the current draft of ISO 42006, the audit process will likely
align with the established sequence of an internal audit (initial assessment
of nonconformities with a window for improvement), followed by a two-stage
external audit (an accredited body reviewing relevant documentation and
evidence), with annual recertification.

## How Citadel AI tools help with ISO 42001 preparation

Citadel AI’s technology helps organizations streamline their AI testing and
governance processes, and automate compliance with AI standards and
guidelines. Our products, Lens and Radar, can:

  1. Automatically fulfill some of the most technically demanding requirements of ISO 42001
  2. Help engineering teams validate their models and datasets against international standards
  3. Provide easy reporting and guidance to get on the certification track as quickly as possible

Citadel AI is trusted by world-leading organizations such as [the British
Standards Institution](https://www.citadel.co.jp/en/news/2023/04/11/bsi-and-
citadel-ai-partner-to-solve-the-ai-challenges-of-tomorrow/). At this critical
period of time where AI standards and regulation are maturing, we believe that
we can help you streamline compliance, improve AI reliability, and navigate
this evolving landscape.

Contact us if you’re interested in learning more.

## Get in Touch

Interested in a product demo or discussing how Citadel AI can improve your AI
quality? Please reach out to us here or [by email](mailto:info@citadel.co.jp).

Full Name

Company Email

Message

I agree to Citadel AI’s [privacy policy](/en/privacy-policy/).

Submit

## Related Articles

[ ![](https://sp-
ao.shortpixel.ai/client/to_auto,q_glossy,ret_img,w_1536,h_806/https://www.citadel.co.jp/en/wp-
content/uploads/sites/6/2024/02/AIA-Header-1536x806.png)
](https://www.citadel.co.jp/en/blog/2024/02/14/eu-ai-act-what-it-means-for-
you/)

  * 14 Feb 2024 

###### [EU AI Act: what it means for
you](https://www.citadel.co.jp/en/blog/2024/02/14/eu-ai-act-what-it-means-for-
you/)

  * [Blog](https://www.citadel.co.jp/en/./blog/)

[ ![](https://sp-
ao.shortpixel.ai/client/to_auto,q_glossy,ret_img,w_1536,h_806/https://www.citadel.co.jp/en/wp-
content/uploads/sites/6/2024/01/ISO-42001-1536x806.png)
](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-
you/)

  * 26 Jan 2024 

###### [ISO 42001 AI Management System: what it means for
you](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-
you/)

  * [Blog](https://www.citadel.co.jp/en/./blog/)

[ !"
"While achieving certification demonstrates a commitment to responsible AI practices and risk mitigation, it doesn't guarantee the complete absence of bias or ethical concerns within an AI system. The dynamic nature of AI development and the complexities of bias necessitate ongoing evaluation and improvement efforts. 
","ISO standards consolidate years of work by experts, are widely accepted by
industry, and are often certifiable and auditable.

In regulated industries, ISO standards also provide technical specifications
and a pathway for companies and products to meet regulatory requirements,
which include medical devices, automobiles, and now AI systems under the EU AI
Act.

### How is ISO 42001 different from the other AI-related standards?

The SC 42 committee has already published dozens of technical documents
focusing on different aspects of AI development. For example, [ISO/IEC TS
4213:2022](https://www.iso.org/standard/79799.html), focusing on the
assessment of machine learning classification performance, can be applied to
quantitatively measure the performance of AI products.

In contrast to product-level standards, ISO 42001 provides an organization-
level approach to AI: as a **management system standard (MSS)** , it covers
principles of good governance and practical ways to manage AI-related risks,
by making sure that adequate preventative and remedial measures are in place.
Establishing a management system and complying with ISO 42001 involves:

  1. A company-wide review of the different use cases, processes, and policies for AI development and use.
  2. Validating or updating these existing processes, policies, systems, tools, and documentation.
  3. Providing evidence that the existing or updated processes are actually being followed at the product or service level.

In order to confirm that the organization-level management system principles
are followed, a sizable amount of product or project-level evidence also needs
to be provided. This includes per-project assessments and documentation on
internal tooling, evaluation, monitoring, and other technical infrastructure
(more on that below).

### What is the structure of ISO 42001?

ISO 42001 joins the family of other management system standards that have been
published over the years. Some of these well-known standards include:

  * ISO 27001: Information Security Management System
  * ISO 13485: Medical Device Quality Management System
  * ISO 37001: Anti-Bribery Management System
  * ISO 14001: Environmental Management System

All MSS follow [the exact same document
structure](https://www.iso.org/management-system-standards.html), which has
been tried and tested to be as flexible as possible and apply to organizations
across industries, with ISO 42001 being no exception:

  * Clause 1: Scope
  * Clause 2: Normative references
  * …
  * Clause 10: Improvement
  * Annexes

Most of the **practical requirements** are included in the **Annexes** , which
contain the list of controls that the organization must implement, such as:

  * B.6.2.4 AI system verification and validation. It requires developers to implement a standardized evaluation protocol for AI models and datasets.
  * B.6.2.8 AI system recording of event logs. It requires developers to have a logging system to record production input data, output predictions, and potential outliers.
  * B.7.4 Quality of data for AI systems. It requires developers to define and measure training and production data quality, as well as consider impact of data bias on performance.

(If you’re interested in a solution to automate the requirements above,
consider using [Citadel Lens](https://www.citadel.co.jp/en/products/lens/) and
[Citadel Radar](https://www.citadel.co.jp/en/products/radar/).)
It requires developers to implement a standardized evaluation protocol for AI models and datasets.
  * B.6.2.8 AI system recording of event logs. It requires developers to have a logging system to record production input data, output predictions, and potential outliers.
  * B.7.4 Quality of data for AI systems. It requires developers to define and measure training and production data quality, as well as consider impact of data bias on performance.

(If you’re interested in a solution to automate the requirements above,
consider using [Citadel Lens](https://www.citadel.co.jp/en/products/lens/) and
[Citadel Radar](https://www.citadel.co.jp/en/products/radar/).)

###### Organization-level Requirements

These requirements apply to the whole organization and its processes:

  * B.2.2 AI policy
  * B.2.3 Alignment with other organizational policies
  * B.2.4 Review of the AI policy
  * B.3.2 AI roles and responsibilities
  * B.3.3 Reporting of concerns
  * B.5.2 AI system impact assessment process
  * B.6.1.2 Objectives for responsible development of AI system
  * B.6.1.3 Processes for responsible design and development of AI systems
  * B.7.2 Data for development and enhancement of AI system
  * B.7.3 Acquisition of data
  * B.8.3 External reporting
  * B.8.4 Communication of incidents
  * B.8.5 Information for interested parties
  * B.9.2 Processes for responsible use of AI
  * B.9.3 Objectives for responsible use of AI system
  * B.10.2 Allocating responsibilities
  * B.10.3 Suppliers
  * B.10.4 Customers

###### Project-level Requirements

These requirements apply to specific projects within the organization:

  * B.4.2 Resource documentation
  * B.4.3 Data resources
  * B.4.4 Tooling resources
  * B.4.5 System and computing resources
  * B.4.6 Human resources
  * B.5.3 Documentation of AI system impact assessments
  * B.5.4 Assessing AI system impact on individuals and groups of individuals
  * B.5.5 Assessing societal impacts of AI systems
  * B.6.2.2 AI system requirements and specification
  * B.6.2.3 Documentation of AI system design and development
  * B.6.2.4 AI system verification and validation
  * B.6.2.5 AI system deployment
  * B.6.2.6 AI system operation and monitoring
  * B.6.2.7 AI system technical documentation
  * B.6.2.8 AI system recording of event logs
  * B.7.4 Quality of data for AI systems
  * B.7.5 Data provenance
  * B.7.6 Data preparation
  * B.8.2 System documentation and information for users
  * B.9.3 Objectives for responsible use of AI system
  * B.9.4 Intended use of the AI system

### Does ISO 42001 cover all technical aspects of various AI processes?

ISO 42001 sits at the center of the responsible AI standards ecosystem, which
includes other standards that go deeper into specific aspects of AI
management, use, and development. The goal of ISO 42001 is to maintain a
balance between being prescriptive in terms of the requirements for an AIMS,
but flexible in terms of how AIMS is implemented for a particular
organization.

Since ISO 42001 itself **does not address the details of specific AI
applications** , it outsources the more technical details to more narrowly
focused standards and other ""generally accepted frameworks"".  Some examples of
other standards referenced in ISO 42001 are:

  * [ISO 5259](https://www.iso.org/standard/81860.html) series on Data Quality
  * [ISO 23894](https://www.iso.org/standard/77304.html) AI Risk Management
  * [ISO 24029-1](https://www.iso.org/standard/77609.html) Assessment of the robustness of neural networks

### What is the process for ISO 42001 certification?

To obtain an ISO 42001 certification, an organization must successfully pass
an audit conducted by a certification body. However, the [ISO
42006](https://www.iso.org/standard/44546.html) standard defining the
requirements for such bodies is currently under development, which means that
**as of January 2024, no certifications can be granted yet**."
"While the provided information doesn't explicitly address the applicability of ISO 42001 to previously developed or deployed AI systems, it does emphasize the standard's focus on establishing a management system for AI governance and risk management. This suggests that organizations could apply the principles and requirements of ISO 42001 to existing AI systems to enhance their responsible AI practices and ensure alignment with the standard's guidelines. 
","ISO standards consolidate years of work by experts, are widely accepted by
industry, and are often certifiable and auditable.

In regulated industries, ISO standards also provide technical specifications
and a pathway for companies and products to meet regulatory requirements,
which include medical devices, automobiles, and now AI systems under the EU AI
Act.

### How is ISO 42001 different from the other AI-related standards?

The SC 42 committee has already published dozens of technical documents
focusing on different aspects of AI development. For example, [ISO/IEC TS
4213:2022](https://www.iso.org/standard/79799.html), focusing on the
assessment of machine learning classification performance, can be applied to
quantitatively measure the performance of AI products.

In contrast to product-level standards, ISO 42001 provides an organization-
level approach to AI: as a **management system standard (MSS)** , it covers
principles of good governance and practical ways to manage AI-related risks,
by making sure that adequate preventative and remedial measures are in place.
Establishing a management system and complying with ISO 42001 involves:

  1. A company-wide review of the different use cases, processes, and policies for AI development and use.
  2. Validating or updating these existing processes, policies, systems, tools, and documentation.
  3. Providing evidence that the existing or updated processes are actually being followed at the product or service level.

In order to confirm that the organization-level management system principles
are followed, a sizable amount of product or project-level evidence also needs
to be provided. This includes per-project assessments and documentation on
internal tooling, evaluation, monitoring, and other technical infrastructure
(more on that below).

### What is the structure of ISO 42001?

ISO 42001 joins the family of other management system standards that have been
published over the years. Some of these well-known standards include:

  * ISO 27001: Information Security Management System
  * ISO 13485: Medical Device Quality Management System
  * ISO 37001: Anti-Bribery Management System
  * ISO 14001: Environmental Management System

All MSS follow [the exact same document
structure](https://www.iso.org/management-system-standards.html), which has
been tried and tested to be as flexible as possible and apply to organizations
across industries, with ISO 42001 being no exception:

  * Clause 1: Scope
  * Clause 2: Normative references
  * …
  * Clause 10: Improvement
  * Annexes

Most of the **practical requirements** are included in the **Annexes** , which
contain the list of controls that the organization must implement, such as:

  * B.6.2.4 AI system verification and validation. It requires developers to implement a standardized evaluation protocol for AI models and datasets.
  * B.6.2.8 AI system recording of event logs. It requires developers to have a logging system to record production input data, output predictions, and potential outliers.
  * B.7.4 Quality of data for AI systems. It requires developers to define and measure training and production data quality, as well as consider impact of data bias on performance.

(If you’re interested in a solution to automate the requirements above,
consider using [Citadel Lens](https://www.citadel.co.jp/en/products/lens/) and
[Citadel Radar](https://www.citadel.co.jp/en/products/radar/).)
It requires developers to implement a standardized evaluation protocol for AI models and datasets.
  * B.6.2.8 AI system recording of event logs. It requires developers to have a logging system to record production input data, output predictions, and potential outliers.
  * B.7.4 Quality of data for AI systems. It requires developers to define and measure training and production data quality, as well as consider impact of data bias on performance.

(If you’re interested in a solution to automate the requirements above,
consider using [Citadel Lens](https://www.citadel.co.jp/en/products/lens/) and
[Citadel Radar](https://www.citadel.co.jp/en/products/radar/).)

###### Organization-level Requirements

These requirements apply to the whole organization and its processes:

  * B.2.2 AI policy
  * B.2.3 Alignment with other organizational policies
  * B.2.4 Review of the AI policy
  * B.3.2 AI roles and responsibilities
  * B.3.3 Reporting of concerns
  * B.5.2 AI system impact assessment process
  * B.6.1.2 Objectives for responsible development of AI system
  * B.6.1.3 Processes for responsible design and development of AI systems
  * B.7.2 Data for development and enhancement of AI system
  * B.7.3 Acquisition of data
  * B.8.3 External reporting
  * B.8.4 Communication of incidents
  * B.8.5 Information for interested parties
  * B.9.2 Processes for responsible use of AI
  * B.9.3 Objectives for responsible use of AI system
  * B.10.2 Allocating responsibilities
  * B.10.3 Suppliers
  * B.10.4 Customers

###### Project-level Requirements

These requirements apply to specific projects within the organization:

  * B.4.2 Resource documentation
  * B.4.3 Data resources
  * B.4.4 Tooling resources
  * B.4.5 System and computing resources
  * B.4.6 Human resources
  * B.5.3 Documentation of AI system impact assessments
  * B.5.4 Assessing AI system impact on individuals and groups of individuals
  * B.5.5 Assessing societal impacts of AI systems
  * B.6.2.2 AI system requirements and specification
  * B.6.2.3 Documentation of AI system design and development
  * B.6.2.4 AI system verification and validation
  * B.6.2.5 AI system deployment
  * B.6.2.6 AI system operation and monitoring
  * B.6.2.7 AI system technical documentation
  * B.6.2.8 AI system recording of event logs
  * B.7.4 Quality of data for AI systems
  * B.7.5 Data provenance
  * B.7.6 Data preparation
  * B.8.2 System documentation and information for users
  * B.9.3 Objectives for responsible use of AI system
  * B.9.4 Intended use of the AI system

### Does ISO 42001 cover all technical aspects of various AI processes?

ISO 42001 sits at the center of the responsible AI standards ecosystem, which
includes other standards that go deeper into specific aspects of AI
management, use, and development. The goal of ISO 42001 is to maintain a
balance between being prescriptive in terms of the requirements for an AIMS,
but flexible in terms of how AIMS is implemented for a particular
organization.

Since ISO 42001 itself **does not address the details of specific AI
applications** , it outsources the more technical details to more narrowly
focused standards and other ""generally accepted frameworks"".  Some examples of
other standards referenced in ISO 42001 are:

  * [ISO 5259](https://www.iso.org/standard/81860.html) series on Data Quality
  * [ISO 23894](https://www.iso.org/standard/77304.html) AI Risk Management
  * [ISO 24029-1](https://www.iso.org/standard/77609.html) Assessment of the robustness of neural networks

### What is the process for ISO 42001 certification?

To obtain an ISO 42001 certification, an organization must successfully pass
an audit conducted by a certification body. However, the [ISO
42006](https://www.iso.org/standard/44546.html) standard defining the
requirements for such bodies is currently under development, which means that
**as of January 2024, no certifications can be granted yet**."
"While adhering to ISO 42001 standards promotes responsible AI development and deployment, it doesn't inherently guarantee an AI system's performance under all circumstances. The standard focuses on establishing a robust management framework to mitigate risks and ensure ethical considerations are addressed throughout the AI lifecycle. However, the dynamic nature of AI systems and the complexities of real-world environments mean unforeseen challenges and performance variations can still arise. 
","It requires developers to implement a standardized evaluation protocol for AI models and datasets.
  * B.6.2.8 AI system recording of event logs. It requires developers to have a logging system to record production input data, output predictions, and potential outliers.
  * B.7.4 Quality of data for AI systems. It requires developers to define and measure training and production data quality, as well as consider impact of data bias on performance.

(If you’re interested in a solution to automate the requirements above,
consider using [Citadel Lens](https://www.citadel.co.jp/en/products/lens/) and
[Citadel Radar](https://www.citadel.co.jp/en/products/radar/).)

###### Organization-level Requirements

These requirements apply to the whole organization and its processes:

  * B.2.2 AI policy
  * B.2.3 Alignment with other organizational policies
  * B.2.4 Review of the AI policy
  * B.3.2 AI roles and responsibilities
  * B.3.3 Reporting of concerns
  * B.5.2 AI system impact assessment process
  * B.6.1.2 Objectives for responsible development of AI system
  * B.6.1.3 Processes for responsible design and development of AI systems
  * B.7.2 Data for development and enhancement of AI system
  * B.7.3 Acquisition of data
  * B.8.3 External reporting
  * B.8.4 Communication of incidents
  * B.8.5 Information for interested parties
  * B.9.2 Processes for responsible use of AI
  * B.9.3 Objectives for responsible use of AI system
  * B.10.2 Allocating responsibilities
  * B.10.3 Suppliers
  * B.10.4 Customers

###### Project-level Requirements

These requirements apply to specific projects within the organization:

  * B.4.2 Resource documentation
  * B.4.3 Data resources
  * B.4.4 Tooling resources
  * B.4.5 System and computing resources
  * B.4.6 Human resources
  * B.5.3 Documentation of AI system impact assessments
  * B.5.4 Assessing AI system impact on individuals and groups of individuals
  * B.5.5 Assessing societal impacts of AI systems
  * B.6.2.2 AI system requirements and specification
  * B.6.2.3 Documentation of AI system design and development
  * B.6.2.4 AI system verification and validation
  * B.6.2.5 AI system deployment
  * B.6.2.6 AI system operation and monitoring
  * B.6.2.7 AI system technical documentation
  * B.6.2.8 AI system recording of event logs
  * B.7.4 Quality of data for AI systems
  * B.7.5 Data provenance
  * B.7.6 Data preparation
  * B.8.2 System documentation and information for users
  * B.9.3 Objectives for responsible use of AI system
  * B.9.4 Intended use of the AI system

### Does ISO 42001 cover all technical aspects of various AI processes?

ISO 42001 sits at the center of the responsible AI standards ecosystem, which
includes other standards that go deeper into specific aspects of AI
management, use, and development. The goal of ISO 42001 is to maintain a
balance between being prescriptive in terms of the requirements for an AIMS,
but flexible in terms of how AIMS is implemented for a particular
organization.

Since ISO 42001 itself **does not address the details of specific AI
applications** , it outsources the more technical details to more narrowly
focused standards and other ""generally accepted frameworks"".  Some examples of
other standards referenced in ISO 42001 are:

  * [ISO 5259](https://www.iso.org/standard/81860.html) series on Data Quality
  * [ISO 23894](https://www.iso.org/standard/77304.html) AI Risk Management
  * [ISO 24029-1](https://www.iso.org/standard/77609.html) Assessment of the robustness of neural networks

### What is the process for ISO 42001 certification?

To obtain an ISO 42001 certification, an organization must successfully pass
an audit conducted by a certification body. However, the [ISO
42006](https://www.iso.org/standard/44546.html) standard defining the
requirements for such bodies is currently under development, which means that
**as of January 2024, no certifications can be granted yet**.
ISO standards consolidate years of work by experts, are widely accepted by
industry, and are often certifiable and auditable.

In regulated industries, ISO standards also provide technical specifications
and a pathway for companies and products to meet regulatory requirements,
which include medical devices, automobiles, and now AI systems under the EU AI
Act.

### How is ISO 42001 different from the other AI-related standards?

The SC 42 committee has already published dozens of technical documents
focusing on different aspects of AI development. For example, [ISO/IEC TS
4213:2022](https://www.iso.org/standard/79799.html), focusing on the
assessment of machine learning classification performance, can be applied to
quantitatively measure the performance of AI products.

In contrast to product-level standards, ISO 42001 provides an organization-
level approach to AI: as a **management system standard (MSS)** , it covers
principles of good governance and practical ways to manage AI-related risks,
by making sure that adequate preventative and remedial measures are in place.
Establishing a management system and complying with ISO 42001 involves:

  1. A company-wide review of the different use cases, processes, and policies for AI development and use.
  2. Validating or updating these existing processes, policies, systems, tools, and documentation.
  3. Providing evidence that the existing or updated processes are actually being followed at the product or service level.

In order to confirm that the organization-level management system principles
are followed, a sizable amount of product or project-level evidence also needs
to be provided. This includes per-project assessments and documentation on
internal tooling, evaluation, monitoring, and other technical infrastructure
(more on that below).

### What is the structure of ISO 42001?

ISO 42001 joins the family of other management system standards that have been
published over the years. Some of these well-known standards include:

  * ISO 27001: Information Security Management System
  * ISO 13485: Medical Device Quality Management System
  * ISO 37001: Anti-Bribery Management System
  * ISO 14001: Environmental Management System

All MSS follow [the exact same document
structure](https://www.iso.org/management-system-standards.html), which has
been tried and tested to be as flexible as possible and apply to organizations
across industries, with ISO 42001 being no exception:

  * Clause 1: Scope
  * Clause 2: Normative references
  * …
  * Clause 10: Improvement
  * Annexes

Most of the **practical requirements** are included in the **Annexes** , which
contain the list of controls that the organization must implement, such as:

  * B.6.2.4 AI system verification and validation. It requires developers to implement a standardized evaluation protocol for AI models and datasets.
  * B.6.2.8 AI system recording of event logs. It requires developers to have a logging system to record production input data, output predictions, and potential outliers.
  * B.7.4 Quality of data for AI systems. It requires developers to define and measure training and production data quality, as well as consider impact of data bias on performance.

(If you’re interested in a solution to automate the requirements above,
consider using [Citadel Lens](https://www.citadel.co.jp/en/products/lens/) and
[Citadel Radar](https://www.citadel.co.jp/en/products/radar/).)"
"## ISO 42001 and AI System Compliance Failures: Understanding the Nuances

While ISO 42001 provides a valuable framework for establishing a Quality Management System (QMS) to ensure AI system compliance, it's important to understand that it doesn't directly impose specific penalties for compliance failures. 

Instead, ISO 42001 focuses on guiding organizations in building robust QMS processes to proactively mitigate risks and maintain compliance with relevant regulations, such as the AI Act.  The onus of defining and enforcing penalties for non-compliance with the AI Act lies with the regulatory authorities, not the ISO standard itself. 
","Please send feedback to
Risto Uuk at [risto@futureoflife.org](mailto:risto@futureoflife.org)

1\. In compliance with the terms and conditions laid down in this Regulation,
Member States shall lay down the rules on penalties and other enforcement
measures, which may also include warnings and non-monetary measures,
applicable to infringements of this Regulation by operators, and shall take
all measures necessary to ensure that they are properly and effectively
implemented and taking into account the guidelines issued by the Commission
pursuant to [Article 82b](https://artificialintelligenceact.eu/article/82b/).
The penalties provided for shall be effective, proportionate, and dissuasive.
They shall take into account the interests of SMEs including start-ups and
their economic viability.

2\. The Member States shall without delay notify the Commission and at the
latest by the date of entry into application of those respective rules and of
those respective measures and shall notify them, without delay, of any
subsequent amendment affecting them.

3\. Non-compliance with the prohibition of the artificial intelligence
practices referred to in [Article
5](https://artificialintelligenceact.eu/article/5/) shall be subject to
administrative fines of up to 35 000 000 EUR or, if the offender is a company,
up to 7 % of its total worldwide annual turnover for the preceding financial
year, whichever is higher.

4\. Non-compliance of an AI system with any of the following provisions
related to operators or notified bodies, other than those laid down in
[Article 5](https://artificialintelligenceact.eu/article/5/), shall be subject
to administrative fines of up to 15 000 000 EUR or, if the offender is a
company, up to 3% of its total worldwide annual turnover for the preceding
financial year, whichever is higher:

(b) obligations of providers pursuant to [Article
16](https://artificialintelligenceact.eu/article/16/);

(d) obligations of authorised representatives pursuant to [Article
22](https://artificialintelligenceact.eu/article\\/22/);

(e) obligations of importers pursuant to [Article
23](https://artificialintelligenceact.eu/article/23/);

(f) obligations of distributors pursuant to [Article
24](https://artificialintelligenceact.eu/article/24/);

(g) obligations of deployers pursuant to [Article
26](https://artificialintelligenceact.eu/article/26/), paragraphs 1 to 6a;

(h) requirements and obligations of notified bodies pursuant to Article
[33](https://artificialintelligenceact.eu/article/33/),
[34](https://artificialintelligenceact.eu/article/34/)(1),
[34](https://artificialintelligenceact.eu/article/34/)(3),
[34](https://artificialintelligenceact.eu/article/34/)(4),
[34a](https://artificialintelligenceact.eu/article/34a/);

(i) transparency obligations for providers and users pursuant to [Article
50](https://artificialintelligenceact.eu/article/50/).

5\. The supply of incorrect, incomplete or misleading information to notified
bodies and national competent authorities in reply to a request shall be
subject to administrative fines of up to 7 500 000 EUR or, if the offender is
a company, up to 1 % of its total worldwide annual turnover for the preceding
financial year, whichever is higher.

5a. In case of SMEs, including start-ups, each fine referred to in this
Article shall be up to the percentages or amount referred to paragraphs 3, 4
and 5, whichever of the two is lower.

6\.
Setting up a Quality Management System (e.g. [based on ISO 42001](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-you/))
  * [Article 18](https://artificialintelligenceact.eu/article/18). Keeping documentation for 10 years at the disposal of the national competent authorities.
  * [Article 20](https://artificialintelligenceact.eu/article/20/). Storing the automatically generated logs for at least 6 months.
  * [Article 21](https://artificialintelligenceact.eu/article/21/). Corrective actions and Duty of information when the system is not in conformity.
  * [Article 23](https://artificialintelligenceact.eu/article/23). Providing the necessary information to competent authorities.
  * [Article 51](https://artificialintelligenceact.eu/article/51). Registering the high-risk model in the EU Database.

 **Deployers** of high-risk AI systems have lighter obligations, and will need
to comply with [Article 29](https://artificialintelligenceact.eu/article/29).
This includes exercising due diligence in using the system; monitoring and
logging the system in use; and, in specific conditions, carrying out [a
fundamental rights impact
assessment](https://artificialintelligenceact.eu/article/29a).

However, a Deployer, Importer, or Distributor is considered a Provider of a
high-risk AI system if they:

  * Change the name or a trademark of the AI system
  * Make a substantial modification to the AI system
  * Modify the intended purpose of a non-high-risk AI system already on the market, in such a manner that makes the AI system high-risk

If the initial Provider was overridden in the cases above, the initial
Provider is no longer subject to the usual obligations, but must provide “the
reasonably expected technical access” to the new Provider.

 **Importers** and **Distributors** are subject to the obligations in [Article
26](https://artificialintelligenceact.eu/article/26) and [Article
27](https://artificialintelligenceact.eu/article/27), respectively. They
include verifying that the provider has complied with their obligations, and
that the system has gone through the relevant conformity assessment and bears
the CE mark.

 **Authorized Representatives** are subject to the obligations in [Article
25](https://artificialintelligenceact.eu/article/25), which outline the
cooperation with relevant authorities and submission of relevant documents.

The following diagram can be useful to understand your entity’s (“operator’s”)
position in the AI system supply chain as defined by the AI Act. Note that
this is a simplified version that makes some assumptions; for example, that
the AI system’s output is located or used within the EU. We also recommend
using the interactive [AI Act Compliance
Checker](https://artificialintelligenceact.eu/assessment/eu-ai-act-compliance-
checker/) tool developed by the Future of Life Institute.

![](https://sp-
ao.shortpixel.ai/client/to_auto,q_glossy,ret_img,w_2090,h_3309/https://www.citadel.co.jp/en/wp-
content/uploads/sites/6/2024/02/AIA-flowchart-2.png)

## What are the requirements for different types of AI systems?

The Act defines several categories of AI systems with different obligations.
In principle, these categories are not mutually exclusive, and the same AI
system may fall under a mix of obligations. For example, a foundation model
that’s used in a medical device and directly interacts with end users might
need to fulfill High-Risk, GPAI, and Transparency obligations all at once.

### Prohibited AI

According to [Article 5](https://artificialintelligenceact.eu/article/5),
these use cases are considered unacceptable and will be prohibited, with some
exceptions for law enforcement activities. Examples: real-time biometric
identification, social scoring, manipulative techniques.

### High-Risk AI

These use cases are considered highly harmful if misused, and will need to go
through a third-party conformity assessment before being placed on the market.
According to [Article 6](https://artificialintelligenceact.eu/article/6), an
AI system is considered high-risk under either of the following conditions:

  1. It’s used as a safety component of a product, or is itself a product that’s already regulated by existing EU laws listed in Annex II, and is required to undergo a third-party conformity assessment under those Annex II laws; or,
  2. It falls under Annex III, unless it’s proved to not possess significant risk of harm to health, safety, or fundamental rights

We will now cover the two points above in more detail."
"There's no mention of a specific date when ISO 42001 requirements will become mandatory for all AI applications globally. The provided information suggests that it's a voluntary standard, meaning organizations can choose to adopt it to demonstrate their commitment to responsible AI practices. 
","It requires developers to implement a standardized evaluation protocol for AI models and datasets.
  * B.6.2.8 AI system recording of event logs. It requires developers to have a logging system to record production input data, output predictions, and potential outliers.
  * B.7.4 Quality of data for AI systems. It requires developers to define and measure training and production data quality, as well as consider impact of data bias on performance.

(If you’re interested in a solution to automate the requirements above,
consider using [Citadel Lens](https://www.citadel.co.jp/en/products/lens/) and
[Citadel Radar](https://www.citadel.co.jp/en/products/radar/).)

###### Organization-level Requirements

These requirements apply to the whole organization and its processes:

  * B.2.2 AI policy
  * B.2.3 Alignment with other organizational policies
  * B.2.4 Review of the AI policy
  * B.3.2 AI roles and responsibilities
  * B.3.3 Reporting of concerns
  * B.5.2 AI system impact assessment process
  * B.6.1.2 Objectives for responsible development of AI system
  * B.6.1.3 Processes for responsible design and development of AI systems
  * B.7.2 Data for development and enhancement of AI system
  * B.7.3 Acquisition of data
  * B.8.3 External reporting
  * B.8.4 Communication of incidents
  * B.8.5 Information for interested parties
  * B.9.2 Processes for responsible use of AI
  * B.9.3 Objectives for responsible use of AI system
  * B.10.2 Allocating responsibilities
  * B.10.3 Suppliers
  * B.10.4 Customers

###### Project-level Requirements

These requirements apply to specific projects within the organization:

  * B.4.2 Resource documentation
  * B.4.3 Data resources
  * B.4.4 Tooling resources
  * B.4.5 System and computing resources
  * B.4.6 Human resources
  * B.5.3 Documentation of AI system impact assessments
  * B.5.4 Assessing AI system impact on individuals and groups of individuals
  * B.5.5 Assessing societal impacts of AI systems
  * B.6.2.2 AI system requirements and specification
  * B.6.2.3 Documentation of AI system design and development
  * B.6.2.4 AI system verification and validation
  * B.6.2.5 AI system deployment
  * B.6.2.6 AI system operation and monitoring
  * B.6.2.7 AI system technical documentation
  * B.6.2.8 AI system recording of event logs
  * B.7.4 Quality of data for AI systems
  * B.7.5 Data provenance
  * B.7.6 Data preparation
  * B.8.2 System documentation and information for users
  * B.9.3 Objectives for responsible use of AI system
  * B.9.4 Intended use of the AI system

### Does ISO 42001 cover all technical aspects of various AI processes?

ISO 42001 sits at the center of the responsible AI standards ecosystem, which
includes other standards that go deeper into specific aspects of AI
management, use, and development. The goal of ISO 42001 is to maintain a
balance between being prescriptive in terms of the requirements for an AIMS,
but flexible in terms of how AIMS is implemented for a particular
organization.

Since ISO 42001 itself **does not address the details of specific AI
applications** , it outsources the more technical details to more narrowly
focused standards and other ""generally accepted frameworks"".  Some examples of
other standards referenced in ISO 42001 are:

  * [ISO 5259](https://www.iso.org/standard/81860.html) series on Data Quality
  * [ISO 23894](https://www.iso.org/standard/77304.html) AI Risk Management
  * [ISO 24029-1](https://www.iso.org/standard/77609.html) Assessment of the robustness of neural networks

### What is the process for ISO 42001 certification?

To obtain an ISO 42001 certification, an organization must successfully pass
an audit conducted by a certification body. However, the [ISO
42006](https://www.iso.org/standard/44546.html) standard defining the
requirements for such bodies is currently under development, which means that
**as of January 2024, no certifications can be granted yet**.
Some examples of
other standards referenced in ISO 42001 are:

  * [ISO 5259](https://www.iso.org/standard/81860.html) series on Data Quality
  * [ISO 23894](https://www.iso.org/standard/77304.html) AI Risk Management
  * [ISO 24029-1](https://www.iso.org/standard/77609.html) Assessment of the robustness of neural networks

### What is the process for ISO 42001 certification?

To obtain an ISO 42001 certification, an organization must successfully pass
an audit conducted by a certification body. However, the [ISO
42006](https://www.iso.org/standard/44546.html) standard defining the
requirements for such bodies is currently under development, which means that
**as of January 2024, no certifications can be granted yet**.

However, the standard can still be freely used for voluntary assessments, both
internally and by third parties. An **early gap analysis** based on the
published standard can significantly accelerate preparations for an official
certification when it becomes available.

Based on known timelines for other management system standards, the audit can
be expected to take **from several months to up to a year** , depending on the
number of people involved in the AI lifecycle processes; the organization’s
role as an AI provider, developer, or user; and the complexity of the AIMS.

According to the current draft of ISO 42006, the audit process will likely
align with the established sequence of an internal audit (initial assessment
of nonconformities with a window for improvement), followed by a two-stage
external audit (an accredited body reviewing relevant documentation and
evidence), with annual recertification.

## How Citadel AI tools help with ISO 42001 preparation

Citadel AI’s technology helps organizations streamline their AI testing and
governance processes, and automate compliance with AI standards and
guidelines. Our products, Lens and Radar, can:

  1. Automatically fulfill some of the most technically demanding requirements of ISO 42001
  2. Help engineering teams validate their models and datasets against international standards
  3. Provide easy reporting and guidance to get on the certification track as quickly as possible

Citadel AI is trusted by world-leading organizations such as [the British
Standards Institution](https://www.citadel.co.jp/en/news/2023/04/11/bsi-and-
citadel-ai-partner-to-solve-the-ai-challenges-of-tomorrow/). At this critical
period of time where AI standards and regulation are maturing, we believe that
we can help you streamline compliance, improve AI reliability, and navigate
this evolving landscape.

Contact us if you’re interested in learning more.

## Get in Touch

Interested in a product demo or discussing how Citadel AI can improve your AI
quality? Please reach out to us here or [by email](mailto:info@citadel.co.jp).

Full Name

Company Email

Message

I agree to Citadel AI’s [privacy policy](/en/privacy-policy/).

Submit

## Related Articles

[ ![](https://sp-
ao.shortpixel.ai/client/to_auto,q_glossy,ret_img,w_1536,h_806/https://www.citadel.co.jp/en/wp-
content/uploads/sites/6/2024/02/AIA-Header-1536x806.png)
](https://www.citadel.co.jp/en/blog/2024/02/14/eu-ai-act-what-it-means-for-
you/)

  * 14 Feb 2024 

###### [EU AI Act: what it means for
you](https://www.citadel.co.jp/en/blog/2024/02/14/eu-ai-act-what-it-means-for-
you/)

  * [Blog](https://www.citadel.co.jp/en/./blog/)

[ ![](https://sp-
ao.shortpixel.ai/client/to_auto,q_glossy,ret_img,w_1536,h_806/https://www.citadel.co.jp/en/wp-
content/uploads/sites/6/2024/01/ISO-42001-1536x806.png)
](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-
you/)

  * 26 Jan 2024 

###### [ISO 42001 AI Management System: what it means for
you](https://www.citadel.co.jp/en/blog/2024/01/26/iso-42001-what-it-means-for-
you/)

  * [Blog](https://www.citadel.co.jp/en/./blog/)

[ !"
